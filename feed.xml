<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://fbraza.github.io/BrazLog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://fbraza.github.io/BrazLog/" rel="alternate" type="text/html" /><updated>2021-07-08T09:08:40-05:00</updated><id>https://fbraza.github.io/BrazLog/feed.xml</id><title type="html">Lost in Datation</title><subtitle>Python 路 Scala 路 BigData 路 System Design 路 Data Engineering</subtitle><entry><title type="html">Guide for optimizing performance with Spark</title><link href="https://fbraza.github.io/BrazLog/spark/2021/07/08/spark-optimization.html" rel="alternate" type="text/html" title="Guide for optimizing performance with Spark" /><published>2021-07-08T00:00:00-05:00</published><updated>2021-07-08T00:00:00-05:00</updated><id>https://fbraza.github.io/BrazLog/spark/2021/07/08/spark-optimization</id><author><name></name></author><category term="Spark" /><summary type="html">Apache Spark is an analytics engine designed to be particularly efficient to process Big Data. You can use Spark on an on-premise or a cloud-deployed Hadoop cluster or through the Databricks platform. In any of these setups, using Spark efficiently is critical if you want to control and reduce costs. For that you should be able to diagnose and resolve some common performance issues. These usually fall into the five following categories: spill, skew, shuffle, storage and serialization. Here we are going to go over each of them to understand what they are, see how to identify them and mitigate their impact on your workflow.</summary></entry></feed>