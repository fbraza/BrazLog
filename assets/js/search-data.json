{
  
    
        "post0": {
            "title": "Guide for optimizing performance with Spark",
            "content": "Apache Spark is an analytics engine designed to be particularly efficient to process Big Data. You can use Spark on an on-premise or a cloud-deployed Hadoop cluster or through the Databricks platform. In any of these setups, using Spark efficiently is critical if you want to control and reduce costs. For that you should be able to diagnose and resolve some common performance issues. These usually fall into the five following categories: spill, skew, shuffle, storage and serialization. Here we are going to go over each of them to understand what they are, see how to identify them and mitigate their impact on your workflow. . Skew . Definition and root causes . In Spark, data is distributed across a cluster as partitions that are processed by different worker nodes. Usually partitions are 128 MB sized and evenly distributed. In reality however, some of your partitions can have significantly more records than others. Typically you face data skewness when using join or groupBy operations using a key that is not evenly distributed. This is not a Spark specific problem but keep in mind that the distribution of the data dramatically impacts on the performance of distributed systems. Let’s imagine that in your workflow your data ends up being partitioned as shown below: . . As partition one (P1) is around four times bigger than the others, it takes four time as much time and requires four time as much RAM to process P1. Consequently, the entire Spark job is slower. More specifically, the stage, including these tasks, takes as much time as the P1 processing task. Finally, when P1 does not fit in memory, Spark raises out-of-memory (OOM) errors or undergo some spill on disk, another issue described later. . Monitoring skew . To monitor if your data is skewed, on the Spark UI go on the Stages tab and read the timeline. If the tasks execution time is not evenly distributed and some task takes a dramatic amount of time compared to others, you data is skewed. . Mitigating skew issues . Remember skew is a data problem. Several approaches exist to solve and mitigate it. Here three of them that you should consider when using Spark: . Use the well-known “salting-key” strategy which briefly consists on concatenating the key with a random number. This randomizes the data and redistribute it more evenly. . | Use query hints to annotate and help the optimizer engine to improve logical execution plans. . | Use the Adaptive Query Execution framework that shipped with Apache Spark 3.0 by enabling its features: . // Enable AQE and the adaptive skew join spark.conf.set(&quot;spark.sql.adaptive.enabled&quot;, true) spark.conf.set(&quot;spark.sql.adaptive.skewedJoin.enabled&quot;, true) . | . Spill . Definition and root causes . When the partitions are to big and cannot fit in memory, Spark moves the data on disk and gets it back later in memory. This phenomenon, called spill, is made possible thank to the ExternalAppendOnlyMap collection class. This prevents OOM errors when partitioned data is too big to fit in memory. Potentially expensive disk I/O operations arise as a trade-off from this operation. There are different scenarios where spill happens: . Ingesting too large partitions. . | Aggregating tables on a skewed column. . | Using join(), crossjoin() or the explode() operations may create very large partitions. . | Using the union() operation. This operation takes two DataFrames and combine them into one and always use the same number of partitions that it started with. As depicted below we start with two DataFrame (DF1 &amp; DF2) with a certain number of partitions and end up with the same number of concatenated partitions that are bigger. . . | Setting an inappropriate value (too big usually) to the spark.sql.files.maxPartitionBytes parameter (set to 128 MB by default). Our advice is to keep it at default and only alter it after some testing. . | . Monitoring spill . To assess whether spill happened during your jobs, the easiest way is to go to the Stages tab from the Spark U and read the summary metrics table for all completed tasks. There, spill is represented by two values: . Spill (Memory): this is the size of the data as it existed in memory | Spill (Disk): this is the size of the data as it existed in disk | . Please note two things: first, the Spill value in disks will be always lower than Spill value in memory due to compression. Second, if no spill occurred you won’t find these values in the summary metrics table. . To know whether spill is occurring during the execution of your jobs, and not wait its end, use the Spark.Listener class in your code. A SparkListener object captures events from the Spark scheduler over the course of a Spark application execution (it is used to output the logs and metrics in the Spark UI). You can implement your own custom SpillListener to track spill. A very nice example can be found in the spark Github repository: . // from https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/TestUtils.scala /** * A `SparkListener` that detects whether spills have occurred in Spark jobs. */ private class SpillListener extends SparkListener { private val stageIdToTaskMetrics = new mutable.HashMap[Int, ArrayBuffer[TaskMetrics]] private val spilledStageIds = new mutable.HashSet[Int] def numSpilledStages: Int = synchronized { spilledStageIds.size } override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized { stageIdToTaskMetrics.getOrElseUpdate( taskEnd.stageId, new ArrayBuffer[TaskMetrics]) += taskEnd.taskMetrics } override def onStageCompleted(stageComplete: SparkListenerStageCompleted): Unit = synchronized { val stageId = stageComplete.stageInfo.stageId val metrics = stageIdToTaskMetrics.remove(stageId).toSeq.flatten val spilled = metrics.map(_.memoryBytesSpilled).sum &gt; 0 if (spilled) { spilledStageIds += stageId } } } . Mitigating spill issues . A quick answer would be to add more memory to your cluster’s workers. If not possible, decrease the size of each partition by increasing the number of partitions generated during data processing. In Spark you can: . configure the default number of partitions. . spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, number_of_partitions) . | repartition the data with the repartition() method (be careful this is an expensive operation). . | configure the size of each partition with the spark.sql.files.maxPartitionBytes Spark setting. . | solve any issues related to skewed data first. . | . Shuffle . Definition and root causes . Shuffle occurs when Spark needs to regroup data from different partitions to compute a final result. It is a side effect observed with wide transformations. These include for example the groupBy(), distinct() or join() operations. Let’s explain shuffle by going through a quick and simple example involving a groupBy() combined with a count() operation. . . First the data is read from source (i.e., HDFS, cloud storage, previous stage) (1). At stage 1, Spark performs a “mapping” operation to identify which record belongs to which group (2). Then data is prepared for the next partitions and written on disk in shuffle files (3). For the next stage, data is read from the shuffle files and transferred through the network to the next executors (4) where a “reduce” operation is performed. Our final result is computed (5) and data written on disk (6). . Shuffle is a potentially very expensive operation that involves a lot of disk and network I/O which impact on Spark performance. Additionally, keep in mind that whatever type of wide transformations you are executing, the mapping and the reduce operations are performed in-memory and remain susceptible to some spill on disk which will add to the overhead of disk I/O. . Monitoring shuffle . The Spark UI is useful to get some statistics about the shuffled data in the Stages tab. In the summary metrics table, have a look to the following entries: . Shuffle Read Size / Records: this the total of shuffle bytes read locally or from remote executors. | Shuffle Remote Reads: this is the total of shuffle bytes read only from remote executors. | Shuffle Read Blocked Time: this the time spent awaiting for shuffle data to be read from remote executors. | . While these numbers are interesting, there are no conventional threshold. It depends on your data and the way you process it. Reducing the amount of shuffle is something you should target but keep in mind that shuffle is a necessary evil. With the Spark UI, you can identify the most expensive tasks and try to reduce shuffle in these cases. Check also if data is not skewed (watch the timeline) or if spill occurred before focusing on shuffle. Finally knowing how to mitigate it properly should permit you to keep these metrics below acceptable thresholds. . Mitigating shuffle issues . There are different approaches against shuffle: . to reduce the network I/O, design your cluster by favoring fewer but larger workers. This limits the number of machines where the data is shuffled across. . | to reduce the amount of data being shuffled, filter out columns or unnecessary records for your processing and analysis. . | to optimize join queries, when the size of one DataFrame is small (below 10 MB), Spark uses Broadcast joins (see the spark.sql.autoBroadcastJoinThreshold, in the documentation). But be careful with it and be sure to have enough memory on the driver as smallest partitions are processed there. . | to optimize joins, use bucketing (spark.sql.sources.bucketing.enabled is set to true by default). Bucketing is an optimization technique to pull down data into distinct manageable parts named “buckets”. Use the bucketBy() method to bucket your data based on specific columns. Notably bucketing your data by sorted keys permits to avoid expensive shuffle operations. . | to reduce the shuffle behavior you can edit several spark configuration properties. You should not blindly play with these but rather test your changes before going into production. We advise the reader to focus on the following properties to limit shuffle and expensive I/O activity: . Configuration Description and recommendation . spark.driver.memory | The default value is 1GB. This is the amount of memory allocated to the Spark driver to receive data from executors. You can change while submitting a spark job with the spark-submit command. Increase the value if you expect the driver to process more a great amount of data notably in the context of a broadcast join. | . spark.shuffle.file.buffer | The default value is 32 KB. If your workload increased, set it to larger values (1 MB). The more you have the more Spark will buffer your data before writing mapping result on disk. | . spark.file.transferTo | Set to true by default. Set it to false if you want Spark to use the file buffer before writing on disk. This decrease the I/O activity. | . spark.io.compression.lz4.block.Size | The default value is 32 KB. By increasing it you can decrease the size f the shuffle file (don’t go over 1 MB). By default Spark uses lz4 compression but you can change the compression codec by altering the property spark.io.compression.codec | . | . Storage . Definition and root causesStages . When talking about the impact of storage on performance, we talk about the overhead I/O cost of data ingestion. The most common example relate to: . reading tiny files . The “tiny files problem” has been pinpointed and described since the existence of distributed system like Hadoop. Things are similar with Spark. Before executing any query on your data, Spark will assess how many tasks are required to read the input data and determine on which worker it should schedule these tasks. Moreover, some files contain metadata (i.e., ORC, Parquet…) to be read and parsed. Then with a huge number of small files, you increase the workload on the Spark Scheduler, number of read / close file operations and metadata to parse. Collectively these operations greatly impact on Spark performance. . | scanning repositories . Directory scanning adds overhead to the tiny files problem. But it also exists for terabytes files especially in the context of highly partitioned datasets on disks. For each partition you have one directory. If we consider some data partitioned by year, month day and hour we will have 8640 directories to scan! If you let your data scale for 10 years you will end with 86400 directories. Keep in mind that the Spark driver scan the repository one at the time. . | dealing with dataset schemas . inferring schema with Spark for csv and json files also impairs performance in Spark. Indeed it requires to do a full scan of the data to assess all types. In contrast, Spark only reads one file when dealing with the Parquet format. This is under the assumption that all Parquet files under the same partition have the same schema. But be careful if you wish to support Parquet schema evolution. For each new schema evolution, you have a new partition with new files. If you alter the schema a lot you progressively fall into the scanning issue as described before. By default, schema evolution is disabled in Spark 2 &amp; 3 but if you need it use the spark.sql.parquet.mergeSchema property. . | . Monitoring storage . On the Spark UI you have access to some interesting metrics to monitor file scanning and count: . In the Stages tab when looking at the stage details, have a look to the Input Size / Records metrics which gives you an idea about the total amount of data that is ingested versus the number of records. | In the SQL tab, select your job and stage to have access to more details as the total number of files read, scan time total and filesystem read time (sampled) total (min, med, max). | . Mitigating storage issues . Measures to mitigate these issues are pretty simple: . avoid using tiny files if possible or merge them into bigger files before performing any operations on your data. | keep in mind that the reading / scanning problem cannot be solved by adding more resources to your workers. Everything is handled by the driver. | partition your data according to your needs. Avoid over-partitioning if not necessary although this will depend on the data problem you are tackling. | . Serialization . Definition and root causes . Serialization improves performance on distributed applications by converting code objects and data into a stream of bytes and vice-versa. For distributed systems like Spark, the majority of the compute time is spent on data serialization. . When writing and executing code, the Spark driver serializes the code, send it to the executors that deserialized the code to execute it. By default, Spark uses Java serialization with the ObjectOutputStream framework that works with any types and classes that implement java.io.Serializable. The Kryo serialization permits to serialize data faster but is not compatible with every classes and types. You also need to do extra-work before to register the classes you want to be serialized. . At the beginning of Spark, users were mostly dealing with resilient distributed datasets (RDDs) by writing empiric code which describes how you want to do things. For RDDs, Spark uses Java serialization to serialize individual Scala and Java objects. This process is expensive especially because the Java objects model is highly memory consumptive. This is even more expensive with Pyspark where code and data are serialized / deserialized twice: first to Java/Scala and then to Python. . About PySpark: all data that come to and from a Python executor has to be passed through a socket and a Java Virtual Machine (JVM) worker. Briefly, with PySpark, the SparkContext uses Py4J to launch a JVM to create a JavaSparkContext. That is the communication between Py4J and the JVM that orchestrate the data flow. It is worth noting that Py4J calls have pretty high latency. That is why all operations on RDDs takes much more time on PySpark than on Spark. . The project Tungsten in 2004 and the design of the DataFrame API were critical steps towards performances improvement of the Spark engine. The first altered and improved the Java objects model allowing Spark to manage data expressed as DataFrame much more efficiently. The API permits us to write more declarative code that will be processed as instructions for the transformation chain. Both minimize the amount of work required by the Spark JVMs. And if you work with PySpark note that in this context nothing will be done in Python then excluding the double serialization needed with RDDs. . All these theoretical details are not easy to grasp but shed light on a very important aspect of Spark: each time you will get away from the DataFrame API in your code, you will lose all these optimizations and encounter some performance hits. This is notably the case when you: . process manually RDDs (writing map() and / or lambda functions). | use user-defined functions (UDFs) that are useful and easy-to-use to extend Spark SQL functionalities. However they are “black-box” and prevent several Spark optimization processes including the way Spark deals with Java objects. For example using UDFs in PySpark will bring you back to the double serialization issue. | . Mitigating storage issues . Here the rules are simple: . USE THE DATAFRAME API. Dig into the API to know exactly the possibilities offered to you. | Use UDFs only when strictly necessary. By design Scala will be faster than Python when using UDFs. With Python however you can still give a try to the pandas UDFS (also called vectorized UDFs) that use apache Arrow to give a performance boost to PySpark in this context. | Give a try to the Kryo serialization framework despite its limitations. You may benefit from its performance boost for your current data problem. It is actually the default framework used by Spark in Apache Hudi. | . Conclusion . Using Spark efficiently requires a good knowledge of its inner parts and an ability to identify technical and performance issues. Here we framed and articulated the five main problems that you may encounter and have discussed a number of techniques to bypass them and optimize your Spark applications. Noteworthy, do not take this article as a step-wise guide that will solve all your problems for all your situations. Instead, it should give you an idea of what is happening and what are the solutions you can implement based on your project. This lets a lot of rooms for experimentations and testing that are necessary to find the right and optimal balance for your Spark applications. .",
            "url": "https://fbraza.github.io/BrazLog/spark/python/scala/2021/07/08/spark-optimization.html",
            "relUrl": "/spark/python/scala/2021/07/08/spark-optimization.html",
            "date": " • Jul 8, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Modern Python part 2 - write unit tests & enforce Git commit conventions",
            "content": "Good software engineering practices always bring a lot of long-term benefits. For example, writing unit tests permits you to maintain large codebases and ensures that a specific piece of your code behaves as expected. Writing consistent Git commits also enhance the collaboration between the project stakeholders. Well-crafted Git commit messages open the door to automatic versioning and generated change log files. Consequently, a lot of attempts are currently ongoing and applied to normalize the messages written in our Git commits. . In the first part of this serie, we setup, our project by installing different Python versions with pyenv, setting a local version of Python with pyenv, encapsulating it into a virtual environment with poetry. Here we show more precisely how to unit test your Python application and how to enforce and validate your Git commit messages. The source code associated with this article is published on GitHub. . This article is the second one from a series of three in which we share our best practices. . Part 1: project initialization with pyenv and poetry | Part 2: unit testing and commit enforcement | Part 3: CI pipeline with GitHub Actions and publication on PiPy | . Testing our code . The project is a simple python function that summarizes data present in a pandas DataFrame. The function outputs the number of rows and columns and the frequency of each data types present in the pandas DataFrame: . - Data Summary Values Number of rows 230 Number of columns 9 float64 3 int64 4 object 2 . Go to your project root directory and activate your virtual environment: . bash{outputLines: 2-100}{promptUser: adaltas}{promptHost: local} poetry shell . We add a couple of dependencies using poetry: bash{outputLines: 2-100}{promptUser: adaltas}{promptHost: local} poetry add -D pynvim numpy pandas Using version ^0.4.3 for pynvim Using version ^1.20.2 for numpy Using version ^1.2.3 for pandas Updating dependencies Resolving dependencies... (1.4s) Writing lock file Package operations: 8 installs, 0 updates, 0 removals • Installing six (1.15.0) • Installing greenlet (1.0.0) • Installing msgpack (1.0.2) • Installing numpy (1.20.2) • Installing python-dateutil (2.8.1) • Installing pytz (2021.1) • Installing pandas (1.2.3) • Installing pynvim (0.4.3) . The -D flag indicates that the dependency only apply to development environments. . I personally use NeoVim for coding that is why I need the pynvim package to support NeoVim python plugins. . Based on the expected output defined above, our program is made of three steps: . Getting the shape of the pandas DataFrame. | Getting the pandas dtypes frequency. | Concatenating the two results into a unified DataFrame that we will use to output the final result. | Once the final DataFrame is obtained we output the result as depicted above. In this regard our code scaffold could look as the following: . import pandas as pd def data_summary(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function defined to return a DataFrame containing details about the number of rows and columns and the column dtype frequency of the passed pandas DataFrame &quot;&quot;&quot; def _shape(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function defined to return a dataframe with details about the number of row and columns &quot;&quot;&quot; return None def _dtypes_freq(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function defined to return a dataframe with details about the pandas dtypes frequency &quot;&quot;&quot; return None return None def display_summary(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function define to print out the result of the data summary &quot;&quot;&quot; result_df = True message = &#39;- Data summary -&#39; print(message, result_df, sep=&#39; n&#39;) . Let’s now start writing our unit tests. We are going to use the unittest tool available with the Python standard library. You may remember in the previous article that pytest was defined as a developer dependency for testing. It is not an issue with pytest because it natively runs tests written with the unittest library. . Unit tests are single methods that unittest expects you to write inside Python classes. Choose a descriptive name for your test classes and methods. The name of your test methods should start with test_. Additionally, unittest uses a series of special assertion methods inherited from the unittest.TestCase class. In practice, a test should precisely cover one feature, be autonomous without requiring external cues, and should recreate the conditions of their success. . To recreate the necessary environment, setup code must be written. If this code happens to be redundant, implements a setUp() method, that will be executed before every single test. This is pretty convenient to re-use and re-organize your code. Depending on your use case you may have to perform systematic operations after the tests ran. For that, you may use the tearDown() method. . First you can read below the unit test we implemented for the data_summary() function: . import unittest import pandas as pd from summarize_dataframe.summarize_df import data_summary class TestDataSummary(unittest.TestCase): def setUp(self): # initialize dataframe to test df_data = [[1, &#39;a&#39;], [2, &#39;b&#39;], [3, &#39;c&#39;]] df_cols = [&#39;numbers&#39;, &#39;letters&#39;] self.df = pd.DataFrame(data=df_data, columns=df_cols) # initialize expected dataframe exp_col = [&#39;Values&#39;] exp_idx = [&#39;Number of rows&#39;, &#39;Number of columns&#39;, &#39;int64&#39;, &#39;object&#39;] exp_data = [[3], [2], [1], [1]] self.exp_df = pd.DataFrame(data=exp_data, columns=exp_col, index=exp_idx) def test_data_summary(self): expected_df = self.exp_df result_df = data_summary(self.df) self.assertTrue(expected_df.equals(result_df)) if __name__ == &#39;__main__&#39;: unittest.main() . The setUp() method initializes two distinct pandas DataFrame. self.exp_df is the resulting DataFrame we expect to get after calling the data_summary() function and self.df is the one used to test our functions. At the moment, tests are expected to fail. The logic has not been implemented. To test with poetry use the command: . bash{outputLines: 2-12}{promptUser: adaltas} poetry run pytest -v ============================================== test session starts ============================== platform linux – Python 3.8.7, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 – /home/fbraza/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8/bin/python cachedir: .pytest_cache rootdir: /home/fbraza/Documents/python_project/summarize_dataframe collected 1 item tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary FAILED [100%] =============================================== FAILURES ========================================= ____________TestDataSummary.test_data_summary __________ . self = . def test_data_summary(self): expected_df = self.exp_df result_df = data_summary(self.df) &gt; self.assertTrue(expected_df.equals(result_df)) E AssertionError: False is not true . tests/test_summarize_dataframe.py:26: AssertionError ============================================== short test summary info ============================= FAILED tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary - AssertionError: False is not true ============================================== 1 failed in 0.32s =================================== . Using the `-v` flag returns a more verbose output for your test results. You can see that your tests are labeled according to the classes and functions names you gave (i.e., `&lt;test_module.py&gt;::&lt;class&gt;::&lt;test_method&gt;`). The code is updated to conform with the unit tests: python import pandas as pd def data_summary(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to output details about the number of rows and columns and the column dtype frequency of the passed pandas DataFrame &quot;&quot;&quot; def _shape(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to return a dataframe with details about the number of row and columns &quot;&quot;&quot; row, col = df.shape return pd.DataFrame(data=[[row], [col]], columns=[&#39;Values&#39;], index=[&#39;Number of rows&#39;, &#39;Number of columns&#39;]) def _dtypes_freq(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to return a dataframe with details about the pandas dtypes frequency &quot;&quot;&quot; counter, types = {}, df.dtypes for dtype in types: tmp = str(dtype) if tmp in counter.keys(): counter[tmp] += 1 else: counter[tmp] = 1 values = [[value] for value in counter.values()] return pd.DataFrame(data=values, columns=[&#39;Values&#39;], index=list(counter.keys())) result_df = pd.concat([_shape(df), _dtypes_freq(df)]) return result_df def display_summary(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function define to print out the result of the data summary &quot;&quot;&quot; result_df = True message = &#39;- Data summary -&#39; print(message, result_df, sep=&#39; n&#39;) . Run our test again: . bash{outputLines: 2-100}{promptUser: adaltas}{promptHost: local} poetry run pytest -v =============================================== test session starts =============================================================== platform linux – Python 3.8.7, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 – /home/fbraza/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8/bin/python cachedir: .pytest_cache rootdir: /home/fbraza/Documents/python_project/summarize_dataframe collected 1 item . tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary PASSED [100%] =============================================== 1 passed in 0.28s ================================================================= . One last thing here. In our tests, we did not test the actual output. Our module is designed to output a string representation of our DataFrame summary. There are solutions to achieve this goal with `unittest`. However we are going to use `pytest` for this test. Surprising isn&#39;t it? As said before `pytest` interpolates very well with `unittest` and we are going to illustrate it now. Here the code for this test: python import unittest import pytest import pandas as pd from summarize_dataframe.summarize_df import data_summary, display_summary class TestDataSummary(unittest.TestCase): def setUp(self): # initialize dataframe to test df_data = [[1, &#39;a&#39;], [2, &#39;b&#39;], [3, &#39;c&#39;]] df_cols = [&#39;numbers&#39;, &#39;letters&#39;] self.df = pd.DataFrame(data=df_data, columns=df_cols) # initialize expected dataframe exp_col = [&#39;Values&#39;] exp_idx = [&#39;Number of rows&#39;, &#39;Number of columns&#39;, &#39;int64&#39;, &#39;object&#39;] exp_data = [[3], [2], [1], [1]] self.exp_df = pd.DataFrame(data=exp_data, columns=exp_col, index=exp_idx) @pytest.fixture(autouse=True) def _pass_fixture(self, capsys): self.capsys = capsys def test_data_summary(self): expected_df = self.exp_df result_df = data_summary(self.df) self.assertTrue(expected_df.equals(result_df)) def test_display(self): print(&#39;- Data summary -&#39;, self.exp_df, sep=&#39; n&#39;) expected_stdout = self.capsys.readouterr() display_summary(self.df) result_stdout = self.capsys.readouterr() self.assertEqual(expected_stdout, result_stdout) if __name__ == &#39;__main__&#39;: unittest.main() . Notice the decorator @pytest.fixture(autouse=True) and the function it encapsulates (_pass_fixture). In the unit test terminology, this method is called a fixture. Fixtures are functions (or methods if you use an OOP approach), which will run before each test to which it is applied. Fixtures are used to feed some data to the tests. They fill the same objective as the setUp() method we used before. Here we are using a predefined fixture called capsys to capture the standard output (stdout) and reuse it in our test. We can then modify our code display_summary() accordingly: . import pandas as pd def data_summary(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to output details about the number of rows and columns and the column dtype frequency of the passed pandas DataFrame &quot;&quot;&quot; def _shape(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to return a dataframe with details about the number of row and columns &quot;&quot;&quot; row, col = df.shape return pd.DataFrame(data=[[row], [col]], columns=[&#39;Values&#39;], index=[&#39;Number of rows&#39;, &#39;Number of columns&#39;]) def _dtypes_freq(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to return a dataframe with details about the pandas dtypes frequency &quot;&quot;&quot; counter, types = {}, df.dtypes for dtype in types: tmp = str(dtype) if tmp in counter.keys(): counter[tmp] += 1 else: counter[tmp] = 1 values = [[value] for value in counter.values()] return pd.DataFrame(data=values, columns=[&#39;Values&#39;], index=list(counter.keys())) result_df = pd.concat([_shape(df), _dtypes_freq(df)]) return result_df def display_summary(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function define to print out the result of the data summary &quot;&quot;&quot; result_df = data_summary(df) message = &#39;- Data summary -&#39; print(message, result_df, sep=&#39; n&#39;) . Then run the tests again: . bash{outputLines: 2-100}{promptUser: adaltas}{promptHost: local} poetry run pytest -v =============================================== test session starts =============================================================== platform linux – Python 3.8.7, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 – /home/fbraza/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8/bin/python cachedir: .pytest_cache rootdir: /home/fbraza/Documents/python_project/summarize_dataframe collected 2 items tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary PASSED [ 50%] tests/test_summarize_dataframe.py::TestDataSummary::test_display PASSED [100%] . =============================================== 2 passed in 0.29s ================================================================= . The tests now succeed. It is time to commit and share our work, for example by publishing it to [GitHub](https://github.com/). Before that, let&#39;s take a close look at how to properly communicate about our work with Git commit messages while respecting and enforcing a common standard. ## Enforce Git commit messages rules in your Python project Writing optimal Git commit messages is not an easy task. Messages need to be clear, readable, and understandable in the long term. **[The Conventional Commits specification](https://www.conventionalcommits.org/en/v1.0.0/)** proposes a set of rules for creating explicit commit histories. ### Using `commitizen` In our series about [JavaScript monorepos](/en/2021/02/02/js-monorepos-commits-changelog/), we saw how to integrate these conventions to enforce good practices regarding commit messages. Applied to Python, we are going to use a package called [commitizen](https://commitizen-tools.github.io/commitizen/) to achieve this. Let&#39;s add this package to our developer dependencies: bash{outputLines: 2-23}{promptUser: adaltas} poetry add -D commitizen Using version ^2.17.0 for commitizen Updating dependencies Resolving dependencies... (3.1s) Writing lock file Package operations: 11 installs, 0 updates, 0 removals • Installing markupsafe (1.1.1) • Installing prompt-toolkit (3.0.18) • Installing argcomplete (1.12.2) • Installing colorama (0.4.4) • Installing decli (0.5.2) • Installing jinja2 (2.11.3) • Installing pyyaml (5.4.1) • Installing questionary (1.6.0) • Installing termcolor (1.1.0) • Installing tomlkit (0.7.0) • Installing commitizen (2.17.0) . To setup commitizen for your project, run the command cz init. It prompts us with a set of questions: . bash{outputLines: 2-100}{promptUser: adaltas}{promptHost: local} cz init ? Please choose a supported config file: (default: pyproject.toml) (Use arrow keys) » pyproject.toml .cz.toml .cz.json cz.json .cz.yaml cz.yaml . ? Please choose a cz (commit rule): (default: cz_conventional_commits) (Use arrow keys) » cz_conventional_commits cz_jira cz_customize . ? Please enter the correct version format: (default: “$version”) . ? Do you want to install pre-commit hook? (Y/n) . Choose all default choices here as they fit perfectly with our actual situation. The last question asks us if we want to use [pre-commit](https://pre-commit.com/) hook. We are going to come back to this later on. So just answer `no` for now. If we look at our `pyproject.toml` file we can see that a new entry named `[tool.commitizen]` has been added: toml [...] [tool.commitizen] name = &quot;cz_conventional_commits&quot; # commit rule chosen version = &quot;0.0.1&quot; tag_format = &quot;$version&quot; . To check your commit message, you can use the following command: . cz check -m &quot;all summarize_data tests now succeed&quot; commit validation: failed! please enter a commit message in the commitizen format. commit &quot;&quot;: &quot;all summarize_data tests now succeed&quot; pattern: (build|ci|docs|feat|fix|perf|refactor|style|test|chore|revert|bump)!?( ( S+ ))?:( s.*) . Our message is rejected because it does not respect the commit rules. The last line suggests some patterns to use. Take some time to read the conventional commits documentation and run the command cz info to print a short documentation: . bash{outputLines: 2-100}{promptUser: adaltas}{promptHost: local} cz info The commit contains the following structural elements, to communicate intent to the consumers of your library: . fix: a commit of the type fix patches a bug in your codebase (this correlates with PATCH in semantic versioning). . feat: a commit of the type feat introduces a new feature to the codebase (this correlates with MINOR in semantic versioning). . BREAKING CHANGE: a commit that has the text BREAKING CHANGE: at the beginning of its optional body or footer section introduces a breaking API change (correlating with MAJOR in semantic versioning). A BREAKING CHANGE can be part of commits of any type. . Others: commit types other than fix: and feat: are allowed, like chore:, docs:, style:, refactor:, perf:, test:, and others. […] . This command guides you on how to write your commit message. Here the format should be `&quot;[pattern]: [MESSAGE]&quot;`. For us, this leads to: bash{outputLines: 2-100}{promptUser: adaltas}{promptHost: local} cz check -m &quot;test: all summarize_data tests now succeed&quot; Commit validation: successful! . Very good, our commit message is valid. But hold on. Checking our messages each time with commitizen might be cumbersome and doesn’t provide the garanty to be applied. It would be better to check automatically the message each time we use the git commit command. That is where the pre-commit hook takes action. . Automatically enforce Git message conventions with pre-commit . Git hooks are useful to automate and perform some actions at specific place during the Git lifecycle. The pre-commit hook permits to run scripts before a Git commit is issued. We can use the hook to validate the commit messages and prevent Git from using a message which doesn’t match our expectations. The hook is active from the command line as well as from any tools interacting with the Git repository where the hook is registered, including your favoride IDE. . pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. If you want to know more about the inner workings and the spectrum of possibilities opened by the pre-commit hook, you can read its usage documentation. . To install pre-commit just run: . bash{outputLines: 2-100}{promptUser: adaltas}{promptHost: local} peotry add -D pre-commit . To automate the Git commit verification we first need to create a configuration file `.pre-commit-config.yaml` as followed: yaml repos: - repo: https://github.com/commitizen-tools/commitizen rev: master hooks: - id: commitizen stages: [commit-msg] . Next we can install the hook with its source defined in the repo property: . bash{outputLines: 2-100}{promptUser: adaltas}{promptHost: local} pre-commit install –hook-type commit-msg . Now that everything is set, we can use our Git hook: bash{outputLines: 12}{promptUser: adaltas} git commit -m &quot;test: all summarize_data tests now succeed&quot; [INFO] Initializing environment for https://github.com/commitizen-tools/commitizen. [INFO] Installing environment for https://github.com/commitizen-tools/commitizen. [INFO] Once installed this environment will be reused. [INFO] This may take a few minutes... commitizen check.........................................................Passed [INFO] Restored changes from /home/fbraza/.cache/pre-commit/patch1617970841. [master 1e64d0a] test: all summarize_data tests now succeed 2 files changed, 48 insertions(+), 5 deletions(-) rewrite tests/test_summarize_dataframe.py (98%) . pre-commit installs an environment to run its checks. As you can see here the commit message assessment passed. To finish we can commit and push the modifications made on the build files (poetry.lock, pyproject.toml) and our module: . bash{outputLines: 2-100}{promptUser: adaltas}{promptHost: local} git commit -m “build: add developer dependencies” -m “commitizen and pre-commit added to our dev dependencies” . commitizen check…………………………………………………Passed [master 1c6457c] build: add developer dependencies 2 files changed, 585 insertions(+), 1 deletion(-) . git commit -m “feat: implementation of the summary function to summarize dataframe” . commitizen check…………………………………………………Passed [master 5c053ad] build: add developer dependencies 1 file changed, 94 insertions(+) . We can now push everything to our GitHub repository: bash{outputLines: 2-100}{promptUser: adaltas}{promptHost: local} git push origin master . Conclusion . We covered a few topics: . On the first hand, we saw how to write unit tests for your code. You shall always start to write tests before coding. It helps you affinate your API and expectations before implementing them. You will definitively benefit from it. We used unittest which is already available in the Python standard library. I actually like its simple design and object-oriented approach but others prefer using the pytest library which is definitively worth checking. One very convenient aspect is that pytest supports the unittest.TestCase class from the beginning. You can then write your tests with either of the two libraries or even mix both depending on your needs and have one common command to run them all. | We saw how to enforce good practices when writing Git commit messages. Our proposed solution relies on the use of two distinct Python packages: commitizen and pre-commit. The first one provides with the tools to check if a message validate the conventions you have chosen. The second one automates the process using a Git hook. | . In our next and last article, we are going to go one step further. We automate testing using tox and integrate it inside a CI/CD pipeline. Once done we will show how to prepare our package and finally publish it on PyPi using poetry. . Cheat sheet . poetry . Add project dependencies: . poetry add [package_name] . | Add developer dependencies: . poetry add -D [package_name] . poetry add --dev [package_name] . | Run test: . poetry run pytest . | . commitizen . Initialize commitizen: . cz init . | Check your commit: . cz check -m &quot;YOUR MESSAGE&quot; . | . pre-commit . Generate a default configuration file: . pre-commit sample-config . | Install git hook: . pre-commit install --hook-type [hook_name] . | . Acknowledgments . This article was first published in Adaltas blogand kindly review by the CEO David Worms and one consultant Barthelemy NGOM. .",
            "url": "https://fbraza.github.io/BrazLog/python/devops/2021/06/24/modern-python-part2.html",
            "relUrl": "/python/devops/2021/06/24/modern-python-part2.html",
            "date": " • Jun 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Modern Python part 1 - start a project with pyenv & poetry",
            "content": "When learning a programming language, the focus is essentially on understanding the syntax, the code style, and the underlying concepts. With time, you become sufficiently comfortable with the language and you start writing programs solving new exciting problems. . However, when you need to move towards this step, there is an aspect that one might have underestimated which is how to build the right environment. An environment that enforces good software engineering practices, improves productivity and facilitates collaboration. At Adaltas, we manage several open source projects and we welcome a lot of contributions. They are mostly targeting the Node.js platform. Based on this experience, we already established common practices for managing large scale projects written in Node.js. . Another language we also use a lot in our daily job as data consultant is Python. Packaging and tooling with Python is often described as cumbersome and challenging. In this regard, several open-source projects emerged in the last years and aim at facilitating the management of Python packages along your working projects. We are going to see here how to use two of them: Pyenv, to manage and install different Python versions, and Poetry, to manage your packages and virtual environments. Combined or used individually, they help you to establish a productive environment. . This article is the first one from a series of three in which we share our best practices. . Part 1: project initialization with pyenv and poetry | Part 2: unit testing and commit enforcement | Part 3: CI pipeline with GitHub Actions and publication on PiPy | . Pre-requisites . pyenv installation . To install pyenv you require some OS-specific dependencies. These are needed as pyenv installs Python by building from source. For Ubuntu/Debian be sure to have the following packages installed: . sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl . To know the required dependencies on your OS go read this documentation. Once the dependencies are installed you can now install pyenv. For this, I recommend using pyenv-installer that automates the process. . curl https://pyenv.run | bash . From there on, you can install on your system any versions of Python you wish. You can use the following command to all versions and flavors of Python available: . pyenv install --list . In our case we are going to install the classical CPython in versions 3.7.10 , 3.8.7 , 3.9.2: . pyenv install 3.7.10 Downloading Python-3.7.10.tar.xz... -&gt; https://www.python.org/ftp/python/3.7.10/Python-3.7.10.tar.xz Installing Python-3.7.10... Installed Python-3.7.10 to /home/fbraza/.pyenv/versions/3.7.10 . Once the versions are installed you can see them by running: . pyenv versions * system 3.7.10 3.8.7 3.9.2 . You can see that pyenv identified recently installed Python versions and also the one installed by default on your system. The * before system means that the global version used now is the system version. pyenv permits to manage Python versions at different levels: globally and locally. Let’s say we are going to set version 3.7.10 as our global version. . pyenv global 3.7.10 . Let’s list our version again: . pyenv versions system * 3.7.10 (set by /home/&lt;username&gt;/.pyenv/version) 3.8.7 3.9.2 . You can see that pyenv sets 3.7.10 as our global Python version. This will not alter the operations that require the use of the system version. The path you can read between parenthesis corresponds to the path that points to the required Python version. How does this work? Briefly, pyenv captures Python commands using executables injected into your PATH. Then it determines which Python version you need to use, and passes the commands to the correct Python installation. Feel free to read the complete documentation to better understand the functionalities and possibilities offered by pyenv. . Don’t be confused by the semantic here. Change the global version will not affect your system version. The system version corresponds to the version used by your OS to accomplish specific tasks or run background processes that depend on this specific Python version. Do not switch the system version to another one or you may face several issues with your OS! This version is usually updated along with your OS. The global version is just the version that pyenv will use to execute your Python commands / programs globally. . poetry installation . Poetry allows you to efficiently manage dependencies and packages in Python. It has a similar role as setup.py or pipenv, but offers more flexibility and functionalities. You can declare the libraries your project depends on in a pyproject.toml file. poetry will then install or update them on demand. Additionally this tools allows you to encapsulate your working project into isolated environments. Finally, you can use poetry to directly publish your package on Pypi. . As a last pre-requisite we are going to install poetry by running the following command: . curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - . Project creation . We are going to see how to create a project and isolate it inside a Python environment using pyenv and poetry. . Setting the Python version with Pyenv . Let’s first create a directory named my_awesome_project and move inside: . mkdir my_awesome_project &amp;&amp; cd $_ . Once inside, set the local Python version we are going to use (we are going to use Python 3.8.7). This will prompt poetry to use the local version of Python defined by pyenv: . pyenv local 3.8.7 . This creates a .python-version file inside our project. This file will be read by pyenv and prompts it to set the defined local Python version. Consequently every directory or file created down this point will depend on the local Python version and not the global one. . Create your project with poetry . Poetry proposes a robust CLI allowing you to create, configure and update your Python project and dependencies. To create your Python project use the following command: . poetry new &lt;project_name&gt; . This command generates a default project scaffold. The content of our new project is the following: . . ├── &lt;project_name&gt; │   └── __init__.py ├── pyproject.toml ├── README.rst └── tests ├── __init__.py └── test_summarize_dataframe.py . Notice the pyproject.toml. This is where we define everything from our project’s metadata, dependencies, scripts, and more. If you’re familiar with Node.js, consider the pyproject.toml as an equivalent of the Node.js package.json. . [tool.poetry] name = &quot;your_project_name&quot; version = &quot;0.1.0&quot; description = &quot;&quot; authors = [&quot;&lt;username&gt; &lt;email address&gt;&quot;] [tool.poetry.dependencies] python = &quot;^3.8&quot; [tool.poetry.dev-dependencies] pytest = &quot;^5.2&quot; [build-system] requires = [&quot;poetry-core&gt;=1.0.0&quot;] build-backend = &quot;poetry.core.masonry.api&quot; . We can see several entries in our defaultpyproject.toml file. . [tool.poetry]: This section contains metadata about our package. You can put there the package name, a short description, author’s details, the version of your project, and so on. All details here are optional but will be required if you decided to publish the package on Pypi. | [tool.poetry.dependencies]: This section contains all required dependencies for our package. You can specify specific version numbers for these packages (packageX = &quot;1.0.0&quot;) or use symbols. The version of Python we want the project to use is defined here as well. In our case python = &quot;^3.8&quot; specifies the minimum version required to run our app. Here this is Python 3.8 and this has been based on the version of our local version defined with pyenv. | [tool.poetry.dev-dependencies]: This section contains all developer dependencies which are packages needed to work and iterate on this project. Nevertheless, these dependencies are not required to run the app and will not be downloaded when building the package. | [build-system]: Do not touch this section unless you updated the version of poetry. | . you can see the full list of available entries for the pyproject.toml file here . Install and activate the virtual environment . Here you have two approaches: whether you know in advance all dependencies you need and you can directly alter the .toml file accordingly or you decide to add later on when needed. In our example, we are going to add progressively our dependencies while writing code. Consequently, we just need to initialize the project and create the virtual environment. To do this run the command: . poetry install Creating virtualenv summarize-dataframe-SO-g_7pj-py3.8 in ~/.cache/pypoetry/virtualenvs Updating dependencies Resolving dependencies... (6.4s) Writing lock file Package operations: 8 installs, 0 updates, 0 removals • Installing pyparsing (2.4.7) • Installing attrs (20.3.0) • Installing more-itertools (8.7.0) • Installing packaging (20.9) • Installing pluggy (0.13.1) • Installing py (1.10.0) • Installing wcwidth (0.2.5) • Installing pytest (5.4.3) Installing the current project: summarize_dataframe (0.1.0) . Firstly the virtual environment is created and stored outside of the project. A bit similar to what we have when using conda. Indeed, Instead of creating a folder containing your dependency libraries (as virtualenv does), poetry creates an environment on a global system path (.cache/ by default). This separation of concerns allows keeping your project away from dependency source code. . You can create your virtual environment inside your project or in any other directories. For that you need to edit the configuration of poetry. Follow this documentation for more details. . Secondly, poetry is going to read the pyproject.toml and install all dependencies specified in this file. If not defined, poetry will download the last version of the packages. At the end of the operation, a poetry.lock file is created. It contains all packages and their exact versions. Keep in mind that if a poetry.lock file is already present, the version numbers defined in it take precedence over what is defined in the pyproject.toml. Finally, you should commit the poetry.lock file to your project repository so that all collaborators working on the project use the same versions of dependencies. . Now let’s activate the environment we just created with the following command: . peotry shell Spawning shell within ~/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8 . ~/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8/bin/activate . The command creates a child process that inherits from the parent Shell but will not alter its environment. It encapsulates and restrict any modifications you will perform to your project environment. . Create our git repository . For our last step here we are going to create a git repository, add README.md and .gitignore files and push everything to our remote repository. . git init git remote add origin https://github.com/fbraza/summarize_dataframe.git echo &quot;.* n!.gitignore&quot; &gt; .gitignore echo &quot;# Summarize dataframe&quot; &gt; README.md git add . git commit -m &quot;build: first commit. Environment built&quot; git push -u origin master . Conclusion . Herein we have seen how to install and manage different versions of Python on our machine using pyenv. We demonstrated how to leverage pyenv local to set a specific Python version in your project and then create a virtual environment using poetry. The use of poetry really smoothens the process of creation by proposing a simple and widely project scaffold. In addition, it includes the minimum build system requirements as defined by PEP 518. . In our next article, we are going to dive more into our project. We will write some code with their respective unit tests and see how we can use poetry to add the expected dependencies and run the tests. Finally, we are going to go a bit further and install all necessary dependencies with poetry to help us enforcing good practices with our git commits when using a Python project. . Cheat sheet . pyenv . Get all available and installable versions of Python . pyenv install --list . | Set the global Python version . pyenv global &lt;version_id&gt; . | Set a local Python version . pyenv local &lt;version_id&gt; . | . poetry . Create a project . poetry new &lt;project_name&gt; . | Install core dependencies and create environment . poetry install . | Activate environment . poetry shell . | . Acknowledgments . This article was first published in Adaltas blogand kindly review by the CEO David Worms and one consultant Barthelemy NGOM. .",
            "url": "https://fbraza.github.io/BrazLog/python/devops/2021/06/09/modern-python-part1.html",
            "relUrl": "/python/devops/2021/06/09/modern-python-part1.html",
            "date": " • Jun 9, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Faouzi is french but living in Belgium where he works full-time as a Data Engineer consultant at Dataroots . He previously worked during 8 years as a scientist in biology and got graduated in Data engineering from the Data ScienceTech Institute. He worked as an intern in Altran for a mission of 8 weeks focused on teh development of a computer vision POC for the portuguese national electricity company and for 5 months at Adaltas where he improved his skills in infrastructure with notably the deployment of Hadoop clusters. . He likes working in the Python ecosystem but feels comfortable switching to Scala and Spark. He has a focus on Data Engineering and Software engineering but likes to dive into Data Analysis and Machine Learning. He loves programming languages and is atempting to work his way through Go and JavaScript ecosystems. . Contact . Gmail: faouzi.brazza@gmail.com | Github: fbraza | LinkedIn: Faouzi Braza | .",
          "url": "https://fbraza.github.io/BrazLog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fbraza.github.io/BrazLog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}