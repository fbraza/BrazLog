{
  
    
        "post0": {
            "title": "Azure Data Engineer Notes for Storage accounts",
            "content": "I am going to share my notes taken during the preparation for the Azure Data Engineer certification (DP-203). I will share them through a series of articles that cover the main concepts and features behind the Azure services needed for Data Engineering. All articles are treated following a same structure. We describe the Azure services and highlight their specific features, security &amp; monitoring practices. The last set of articles will be focused on cloud architectures and pattern with Azure. If you want to access a specific topic you can choose the topic just below: . Cosmos DB | Azure Storage Account | Azure SQL | Azure Synapse Analytics | Azure Data Factory | Azure Databricks | Azure Event Hub and Azure Stream Analytics | Big Data Architectures on Azure | . Here, we are going to talk about the azure storage accounts which provides to the user a unique space to store all kind of data. We are going to catch the specific features and options of this service and capture the recommended procedures to deploy them in a secure and scalable way. . Definition . A storage account provides a unique namespace in Azure for your data. It supports blobs (including data lake storage Gen2), queue, table store and Azure files. Each object that you store in Azure Storage has an address that includes your globally unique storage account name. You have distinct storage accounts that proposes different services: . Type of storage account Supported storage services Redundancy options Usage . Standard general-purpose v2 | Blob (including Data Lake Storage1), Queue, and Table storage, Azure Files | LRS / GRS / RA-GRS / ZRS / GZRS / RA-GZRS2 | Standard storage account type for blobs, file shares, queues, and tables. Recommended for most scenarios using Azure Storage. Note that if you want support for NFS file shares in Azure Files, use the premium file shares account type. | . Premium block blobs | Blob storage (including Data Lake Storage1) | LRS / ZRS2 | Premium storage account type for block blobs and append blobs. Recommended for scenarios with high transactions rates, or scenarios that use smaller objects or require consistently low storage latency. | . Premium file shares | Azure Files | LRS / ZRS2 | Premium storage account type for file shares only. Recommended for enterprise or high-performance scale applications. Use this account type if you want a storage account that supports both SMB and NFS file shares. | . Premium page blobs | Page blobs only | LRS | Premium storage account type for page blobs only. Ideal to store VMs disks. | . Storage account services . Because Standard general-purpose v2 has the most features and is the most used account type, we are going to focus on the four services it proposes. . . The Blob service is used to store objects and large amount of unstructured data on the cloud (videos, images, log files, audio files). It has a flat structure. Everything is stored in a container. | The File service is used to create file shares on the cloud. The files are accessed via the SMB protocol. You can have both folders and files. | The Table service is used to store semi-structured NoSQL data in the cloud. It’s ideal for storage accessed by web applications and for datasets that don’t require complex joins, foreign keys or stored procedures. | The Queue service is used to store large number of messages. It works as a middleware because it is delivering cross-communication between applications. | Create storage account . Creation . Here a set of screenshots that underpin the needed steps to set up a storage account. . . Your account storage name must be globally unique. | You can change the performance of you account and set them to Standard or Premium. Setting to premium gives you access to other Account kinds with premium services. | Note the Replication and Access tier two very important configuration settings we will talk about later. | Network connectivity . . Here choose carefully your connectivity method. By default it uses a public endpoint available for all networks. You can also select the networks or use a private endpoint and set rules to access it. | Data protection . . Once you created your storage account, you can have access to it and use the four storage services as needed. . . Click on container to create your blob storage. You will have then to define its name and the connectivity rules to decide if the access is public or private. . Blob storage security . You have several ways of authorizing the access to your storage account: . First by using the storage account access keys. You get two keys for your storage account. The keys give access to all services and all data within the storage account. So giving the key is similar to make the new user an administrator. You can rotate keys for security purpose. If you rotate key1 for key2, regenerate the key1. . | Second by using Azure Active Directory. You can authorized users in Azure AD to work with Azure blob storage. For this you need to assign the required RBAC roles to users who want to use Azure Blob Storage. The default roles are (i) Storage Blob Data Contributor (read/write/delete permissions on Blob resources), Storage Blob Data Reader (read only access) . | Third by using shared access signatures. You can grant secure and temporary access at the level of the storage account without the need of compromising the storage account keys. There you can define which services you want to give access to and what kind of permissions the users will have. See below what it looks like to give permissions with shared keys’ . . You can also define access with Permissions and expiry data/time at the level of your blob object with the help of the shared keys. There you can specify the allowed IP addresses. To sign the shared access signature it uses the storage account key. It return a Blob SAS Token or Blob SAS URL. . | . . Fourth by using the string connection key with the AZ CLI or one of the available SDKs . For this you need two things: . *An Access key*: you have two keys by storage account to rotate them for security | *A REST API endpoint*: see the table above for the different type of endpoints. | . The simplest way to connect with the informations, is to use *storage account connection strings*. A connection string provides all needed connectivity information in a single text string. *BUT BE CAREFUL*, do not put the sensitive information as a plain text. Instead, store connectivity information in a Database, environment variable or configuration file (if config file do not track it with your version control service). You can also use shared access signatures to give an fine-grained access (time-limited, can restrict permissions). . For an .env file, create it and add the following variable: . AZURE_STORAGE_CONNECTION_STRING=&lt;value&gt; . Then use the Azure CLI do generate the connection string: . az storage account show-connection-string --resource-group NAME_OF_YOUR_RESOURCE_GROUP --query connectionString --name NAME_OF_THE_STORAGE_ACCOUNT . | . Storage Redundancy . Azure storage always sores multiple copies of the data. There are different options for that: . Locally-redundant storage (LRS) . Replicated three times within a single physical location in the primary region. | 99.999999999% (11 nines) durability of objects over a given year . | This is the lowest-cost redundancy option available. | The write operation returns successfully only after the data is written to all three replicas | This option does not protect against data center or region wide failures. | . Zone-redundant storage (ZRS) . Here the data is replicated three times synchronously across three Azure availability zones in the primary region. | 99.9999999999% (12 nines) durability of objects over a given year | Azure AZs have independent power, cooling and networking, consequently, this option protects against data center / AZs failures. | . Geo-redundant storage (GRS) . Here the data is replicated three times within a single physical location in the primary region using LRS. | 99.99999999999999% (16 nines) durability of objects over a given year | Then the data is copied asynchronously to a single physical location in the secondary region. | This helps to protect against region-wide outages. | If the application needs data access in primary and secondary regions you can set the read-access Geo-redundant storage. | . Geo-zone-redundant storage . Here the data is replicated three times synchronously across three Azure availability zones in the primary region using ZRS. | 99.99999999999999% (16 nines) durability of objects over a given year | The data is copied asynchronously to a single physical location in the secondary region. | This helps protect the data against data center failures in the primary region and region-wide outages overall. . | You can set also the read-access Geo-zone-redundant storage | . . Blob Storage access tiers . The Blob service in storage accounts provide different access that provide usage and cost flexibility. There three types of access tiers: . Hot access tier: it can be set at the account level and is optimized for data accessed frequently. | Cool access tier: it can be set at the account level and is optimized for data that is infrequently accessed and stored for at least 30 days. Storage cost for this tier is less than for the Hot access tier. However, accessing data (reading, writing) costs more. | Archive access tier: it can be set at the blob level and is optimized for data rarely accessed and stored for at least 180 days. Storage cost is very cheap however access cost is the highest when compared to the cool and hot tiers. Note that before accessing data in an Archive tier Blob, you first need to rehydrate it before can be accessed (this can take several hours). | . Azure Data Lake Storage Gen2 . A data lake is a storage repository that holds a vast amount of raw data in its native format until it is needed. You can store different types of structured, semi-structure or unstructured data. The Azure Data Lake Storage Gen2 permit to store all kind of data for low costs. Gen2 is built on top of Azure Blob storage (with all its features) to which has been added additional capabilities coming from the Hadoop-like Azure Data Lake storage Gen1. . Notably, Azure Data lake Gen2 provides a hierarchical namespace that helps to organize objects/files into a hierarchy of directories (very similar to a is a distributed file system). You can also define POSIX permissions of directories and files. Thank to its architecture Azure Data Lake is particularly optimized to store data for Big Data Analytics. . To make your storage account a Data Lake Storage Gen2, you need to enable this option when creating your account. . .",
            "url": "https://fbraza.github.io/BrazLog/cloud/azure/data%20engineering/2021/08/03/Notes-Azure-storage-accounts.html",
            "relUrl": "/cloud/azure/data%20engineering/2021/08/03/Notes-Azure-storage-accounts.html",
            "date": " • Aug 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Azure Data Engineer Notes for Azure Cosmos DB",
            "content": "Cosmos DB . To deal with NoSQL / semi-structure data Azure proposes a multi-model database called Cosmos DB. . Overview of the features . It is a globally distributed multi-model database serverless service. You don’t need to manage the infrastructure or database engine. | Cosmos DB provides 99,999% of availability for reads and write. | The infrastructure has the ability to scale automatically | It guarantees less than 10-ms latency for reads and indexed and offers replication across several regions | . To use Cosmos DB you need to create a Azure Cosmos account. With Cosmos DB we use documents stores, Key/Value store, columnar store or graph store. . . Next you need to choose the API depending of the data store you use: . If you have an existing deployed MongoDB document store, use the MongoDB API | For a new document store, use the SQL API | For a new key/value store use the Tale API | if you have an existing CassandraDB use the Cassandra API | For a new graph store use the Gremlin API | . Billing: . The metric used to assess the usage of resources and to be charged is based on the request unit (RU) that is a combination of CPU, Memory and IOPS usages. Billing is done on a hourly basis (read 1KB == 1 RU). . Use the SQL API . The Azure Cosmos DB SQL API allows one to query JSON documents using SQL. It gives you the flexibility of a document store with the familiarity of the SQL language. When creating a database you will go through the following steps: . Create the Azure Cosmos DB. | Create a new database (define the Database ID), you can define the number of RUs for the entire database. | Create a new container (define the Container ID), you can redefine the RUs at the container level. | Choose your partition key (/my_partition_key). | If you want you can create an analytic store using Azure Synapse Link. | . . Notice here two things: . The partition key named /course | A lot of metadata is associated with your data. Focus on the id property. It can be a self generated value or you can set it yourself. The combination of the partition key and the id key helps to identify uniquely the item within the container. The id must be a string. you can use the same id for different logical partition. | . Partitions in Cosmos DB . Partitioning is used to scale the individual containers in a database to meet the expected performances. Items in the container are divided into subsets called logical partitions. Indeed the items are divided into the various partitions using a partition key. . Let’s see the following item: . { &quot;participantID&quot;: &quot;1&quot;, &quot;participantName&quot;: &quot;Roger&quot;, &quot;Course&quot;: &quot;Analytics&quot; } . Here you could use any property as the partition key. Let’s choose the participantID for the next following items: . { &quot;participantID&quot;: &quot;1&quot;,{ &quot;participantID&quot;: &quot;3&quot;, &quot;participantName&quot;: &quot;Sandrine&quot;, &quot;Course&quot;: &quot;Analytics&quot; } &quot;participantName&quot;: &quot;Roger&quot;, &quot;Course&quot;: &quot;Analytics&quot; } { &quot;participantID&quot;: &quot;2&quot;, &quot;participantName&quot;: &quot;Julien&quot;, &quot;Course&quot;: &quot;Mathematics&quot; } { &quot;participantID&quot;: &quot;3&quot;, &quot;participantName&quot;: &quot;Sandrine&quot;, &quot;Course&quot;: &quot;Analytics&quot; } . Using the participantID property as partition key results in 3 logical partitions in the container. For each document you will have one partition. Although it is possible, this might be too much partitions and may cause performance issue when trying to query the data if you scale to more participants. . Let’s choose the Course as partition key. . // One parittion { &quot;participantID&quot;: &quot;1&quot;, &quot;participantName&quot;: &quot;Roger&quot;, &quot;Course&quot;: &quot;Analytics&quot; }, { &quot;participantID&quot;: &quot;3&quot;, &quot;participantName&quot;: &quot;Sandrine&quot;, &quot;Course&quot;: &quot;Analytics&quot; } // Second partition { &quot;participantID&quot;: &quot;2&quot;, &quot;participantName&quot;: &quot;Julien&quot;, &quot;Course&quot;: &quot;Mathematics&quot; } . We now have just two partitions and we can anticipate that with growing number of participants, we still keep a reasonable number of logical partitions in our container. . How to choose the right partition Key . Choose a property that has a value that does not change a lot. | But still try to have a reasonable high range of unique values. | When querying against the data, it is good to include the partition key in your query. Indeed doing that will prompt the engine to filter out the unnecessary partitions. If you do not include the partition key in your query, the engine has to read all the logical partitions. So choose a partition key that will be frequently used in your query. | You can choose the item id as partition key as it would improve reads efficiency. | The only way to have a new partition key is to create a new container. You may have containers with the same data but different partition keys based on the query you want to run against the data. | . Consistency levels in Cosmos DB . Remember that you can enable the replication of your data in Cosmos DB. But, when changes are made to a data located in a specific region, the data can take some time (ms) to replicate across other regions. Let’s take the following example: . . After the write operation one region did not replicate the change yet. So what should happen if a client wants to read the data? What version of the data will it read? The concept of consistency for databases has been frame in the CAP theorem. A fully consistent database means that the client will always read the most recent committed version of the data, to the cost of some latency. Cosmos DB proposes different levels of consistency: . Strong . Here the client has to wait before getting the data. It gives highest consistency but lowest throughput (reads). . . Bounded Staleness . Here the reads can lag behind the writes by at most “n” versions of an item or by “x” time intervals, depending on how you configure it. Outside the staleness windows, the guarantee is similar to what is observed with the strong consistency. But inside the staleness windows guarantee depends on some parameters: . If clients in the same region for an account with single write region then Strong | If clients in different regions for an account with a single write region = Consistent prefix | If clients write to a single region for an account with multiple write regions = Consistent prefix | If clients write to several regions for an account with multiple write regions = Eventual | . . Session . Here, within a single client session, reads are guaranteed to honor the consistent-prefix, monotonic reads, monotonic writes, read-your-writes, and write-follows-reads guarantees. This the default and most used consistency model used for Cosmos DB. Outside of the sessions this will depend on the situation: . If clients in same region for an account with single write region then Consistent Prefix . | if clients in different regions for an account with single write region then Consistent Prefix . | if clients write to a single region for an account with multiple write regions then Consistent Prefix . | if clients write to multiple regions for an account with multiple write regions = Eventual . | . . Consistent prefix . In consistent prefix option, updates that are returned contain some prefix of all the updates, with no gaps. Consistent prefix consistency level guarantees that reads never see out-of-order writes. . . Eventual . In eventual consistency, there’s no ordering guarantee for reads. In the absence of any further writes, the replicas eventually converge. Eventual consistency is the weakest form of consistency because a client may read the values that are older than the ones it had read before. . . Security aspect with Comos DB . You can set and manage security at three levels: . Network: use the IP firewall rules . | Authorization: two master keys (primary and secondary keys, for read/write and read-only keys) . | Role-based access control: it provides different levels of access to users defined in Azure AD. Different roles can be assigned: . DocumentDB Account Contributor, he can manage Azure Cosmos accounts . | Cosmos DB Account reader, he can read the data in an Azure Cosmos DB . | Cosmos DB Operator, he can provision Azure Cosmos accounts, databases and containers but cannot manage the keys to access the data . DocumentDB was the previous name for Cosmos DB . | . | . Network security on Azure: . . All networks or selected networks. The first allow traffic from any Azure services. This the option by default. If you want to restrict the connection some some privileged source choose the selected networks. | With selected networks option you can choose to add an existing virtual network or create a new one. | Or you can define firewall rules by adding specific IP or IP-ranges to be allowed to connect. | Authorization Keys . . Read-write keys or Read-only keys | To connect an application Azure Storage Explorer to your Cosmos DB instance, use the connection string. | Role-based access control . . You can choose the role | Assign the role to an Azure AD user, group or service principal |",
            "url": "https://fbraza.github.io/BrazLog/cloud/azure/data%20engineering/2021/07/30/Notes-Azure-Cosmos-db.html",
            "relUrl": "/cloud/azure/data%20engineering/2021/07/30/Notes-Azure-Cosmos-db.html",
            "date": " • Jul 30, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Azure Data Engineer Notes for Azure Data Factory",
            "content": "I am going to share my notes taken during the preparation for the Azure Data Engineer certification (DP-203). I will share them through a series of articles that cover the main concepts and features behind the Azure services needed for Data Engineering. The last set of articles will be focused on cloud architectures and pattern with Azure. If you want to access a specific topic have click to the desirable article below: . Cosmos DB | Azure SQL | Azure Synapse Analytics | Azure Data Factory | Azure Databricks | Azure Event Hub and Azure Stream Analytics | . Here, we are going to talk the cloud-based ETL tool on Azure called, Azure Data Factory. it is a critical service to master if you want to create data scalable and reliable data pipeline in Azure cloud. We are going to browse through the main components and features proposed by Azure Data Factory. . Anatomy of Azure Data Factory . Azure Data Factory is a cloud-based ETL and data integration service. It allows ingest data for multiple sources and to create data-driven workflow for orchestrating data movement, transformations and processing at scale. Importantly, note that you have the ability to create and schedule data-driven workflows. Let’s see a very simple example of a orchestration pipeline with Azure Data Factory. We are going to move a CSV file from a blob storage to a SQL database. See below a global picture of the key components of Data Factory for this job. . . Data Factory permits to connect you to a large variety of data sources by using an tool called Linked Service. They are like connection strings that define the connection information needed to connect to external resources. This helps to connect to external data sources. | Once the linked service is defined, you can point to the data you want to use thank to Datasets objects. A dataset is a named view of data that simply points or references the data you want to use in your activities as inputs and outputs. Datasets identify data within different data stores, such as tables, files, folders, and documents. | Activities contain the processing and transformation logic of your ETL. Here we have an example with the Copy Data activity that is used to ingest from and land data into data stores. You can actually group them into sub-pipelines and decide to schedule them or run them based on specific triggers. in conclusion we distinguish three type of activities: data movement, data transformation and control activities. Pipelines is a logical grouping of activities. | The integration runtime provides the compute environment you need for your pipelines. There are three types of Integration runtime: Azure, Self-hosted and Azure-SSIS. | . Integration runtimes . The runtime is used to bridge the gap between the activity and the linked service. To copy data for Blob to Azure SQL, you need to have some compute runtime to execute the COPY activity. This is what the integration runtime is: a compute environment for the activity to run. There are three types of integration runtimes: . Azure integration runtime which helps to execute data flows in Azure by connecting data stores and compute services with public accessible endpoints. it is fully managed and serverless compute environment. you don’t need to care about patching, scaling or maintenance. it has all required components to move / process data between cloud stores in a secure, performance and reliable manner. . | Self-hosted integration runtime: this used to run copy activities between cloud data stores and a data store in a private network. Consequently if you have the data source or destination, consider using this integration runtime. Use self-hosted integration runtime to support data stores that requires bring-your-own driver such as SAP Hana, MySQL, etc. . You can also use the Self-hosted runtime for dispatching the following transform activities against compute resources in on-premises or Azure Virtual Network: . HDInsight Hive activity (BYOC-Bring Your Own Cluster) | HDInsight Pig activity (BYOC) | HDInsight MapReduce activity (BYOC) | HDInsight Spark activity (BYOC) | HDInsight Streaming activity (BYOC) | Azure Machine Learning Studio (classic) Batch Execution activity | Azure Machine Learning Studio (classic) Update Resource activities | Stored Procedure activity | Data Lake Analytics U-SQL activity | Custom activity (runs on Azure Batch) | Lookup activity | Get Metadata activity. | . Just work for Windows machine as you need to download and install integration runtime on your machine. . | Azure-SSIS integration runtime used to lift and shift existing SSIS workloads. you can provision these in either public or private networks. . | . Improve Copy activity . Azure Data Factory offers a serverless architecture that permits to develop pipelines to maximize data movement. but these pipelines can consume a lot of network bandwidth and IOPS. There are different ways of measuring and improving the performance of Azure Data Factory Copy activity: . If you are using the Azure integration runtime check the Data Integration Unit (DIU). It is a measure that represents the power of a single unit in Azure Data Factory. Power is a combination of CPU, memory, and network resource allocation. | If you are using the slef hosted integration runtime, try to increase concurrent workload by increasing the number of concurrent jobs by node or add more nodes. | Using a staged copy of your data can help also but in the case where your pipeline is slowing down a lot check the DIU. | . Data Factory transformation methods . Azure Data Factory offers a variety of methods to prepare / clean and transform the data. You can choose the one that best fit your needs depending on your projects. . Transforming data using Mapping Data Flow . The Mapping Data Flows provide tools for establishing a wide set of data transformation without the need of code. In practice Mapping Data Flow help to visually design your data transformations in Azure Data Factory. The flows are then executed as activities within a pipeline using scaled-out Apache Spark clusters. To test your Data lows you can use the built-in debugger. . You have also the Wrangling Data Flow that to perform code free data cleaning and preparation. It allows to do basic data prepation in an interface very similar to excel. . When creating your Mapping Data Flow instance in Azure Data Factory you first need to specify the source of your data flow (creation of linked service and dataset). . . Once the source is set up. You can choose between several operation to perform on your data. . . To save your transformed data, you set a sink destination and map the columns to your destination table (in Azure SQL for example). Once your pipeline is structured and set up you first need to debug it. For that, you need to enable the Data flow debug option. It will spin-up a Spark cluster for your debug environment and this may take 5-6 minutes. Once you set the debug option everyhting you do will be done in this debug environment. If all the process is errorless you can next publish your pipeline. . . Once published go to the pipeline location, create a pipeline and use the Data Flow you created. . Transforming data using compute resources . Azure Data Factory can spin-up compute resources to transform data using specific Big Data / SQL services. For example you can create a pipeline using an analytical data platform such as Spark pools in an Azure Synapse Analytics instance to perform a complex calculation using python. An alternative would be to run the data trasnformation in Azure Databricks using notebooks and code in Scala, Java, R or Python. Below is a table that contain most of the compute resources you can use for your data processing. . Compute environment activities . On-demand HDInsight cluster or your own HDInsight cluster | Hive, Pig, Spark, MapReduce, Hadoop Streaming | . Azure Batch | Custom activities | . Azure Machine Learning Studio Machine | Learning activities: Batch Execution and Update Resource | . Azure Machine Learning | Azure Machine Learning Execute Pipeline | . Azure Data Lake Analytics | Data Lake Analytics U-SQL | . Azure SQL, Azure SQL Data Warehouse, SQL Server | Stored Procedure | . Azure Databricks | Notebook, Jar, Python | . Azure Function | Azure Function activity | . Transforming data using SSIS packages . Azure Data Factory provides the ability to lift and shift existing SSIS workload, by creating an Azure-SSIS Integration Runtime to natively execute SSIS packages. . Data Factory control flow . In Azure Data Factory control flow permits to chain activities, branch them altogether based on conditions and define parameters at the pipeline level. It also includes some looping capacity where data is passed to a looping container (ForEach loop for example). Some of the common control flow are listed below: . Chaining activities: chain activities in a sequence | Branching activities: depends on conditions like if-condition activity similar to an if statement in programming. When the condition evaluates to true a set of activities is executed. If false alternative activities are executed. | Parameters: definition of parameters at the pipeline level which allows to pass arguments when invoking the pipeline on-demand or from a trigger. | Looping containers: these containers defines repetition in a pipeline. You can use ForEach activity or the Until activity. | Trigger based flows: pipelines can be triggered on-demand (event-based, blob posts) or by clock-time. | . Triggers . A pipeline can run in response to three triggers signals: . Schedule trigger that invokes the trigger based on a schedule (time). | Tumbling window trigger that invokes the trigger based on periodical interval. | Event-based trigger that invokes trigger based on a specific event (when new data is uploaded to a blob storage). | . Azure Data Factory security . To create Azure Data Factory instances, you need to be a member of the contributor or owner roles or an administrator of the Azure subscription. More specifically to create and manage resources from the Azure portal, you must be in the Data Factory Contributor role at the resource group level or above. If you manage resources with PowerShell or the SDK, the contributor role at the resource level or above is sufficient. Here some of the permissions you get once you have the contributor role: . Create, edit, delete data factories and child resources (i.e., datasets, linked services, pipelines, triggers and integaration runtimes). | Deploy Resources manager templates. | Manage App insights alerts for Data Factory | .",
            "url": "https://fbraza.github.io/BrazLog/cloud/azure/data%20engineering/2021/07/28/Notes-Azure-data-factory.html",
            "relUrl": "/cloud/azure/data%20engineering/2021/07/28/Notes-Azure-data-factory.html",
            "date": " • Jul 28, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Azure Data Engineer Notes for Azure Synapse Analytics",
            "content": "I am going to share my notes taken during the preparation for the Azure Data Engineer certification (DP-203). I will share them through a series of articles that cover the main concepts and features behind the Azure services needed for Data Engineering. The last set of articles will be focused on cloud architectures and pattern with Azure. If you want to access a specific topic have click to the desirable article below: . Cosmos DB | Azure SQL | Azure Synapse Analytics | Azure Data Factory | Azure Databricks | Azure Event Hub and Azure Stream Analytics | . Here, we are going to talk about the Azure Synapse Analytics service which provides a comprehensive approach for ETL / ELT. It allows you to ingest data, prepare and train it (Synapse SQL and Spark pool) and serve it to a Data warehouse. Several connectors exists to then analyze and visualize the data like power BI. We are going to focus mainly and the SQL solutions proposed by Azure Synapse and see their features. . Azure Synapse Analytics . Azure Synapse Analytics (ASA) is an integrated analytics platform which combines data warehousing, big data analytics, data integration and visualization into a single environment. You can use ASA to answer the following questions: . What happened: This corresponds to descriptive analytics that involves the use of a Data warehouse using the dedicated SQL pool. | Why did it happen: This corresponds to diagnostic analysis where you may need to extends your scope of analysis by exploring and finding more data. This can still be done with the SQL pool. | What will happen: This corresponds to predictivie analysis based on previous trends or patterns, using the integrated Spark engine with the Azure Synapse Spark pools. These can be used with other services such as Azure Machine learning services or Azure Databricks. | What should I do: This corresponds to prescriptive analysis where you need to take actions based on real-time or near-real time analysis of data using predictive analytics. ASA provides this capability by harnessing both Apache Spark, Azure Synapse link and by integrating streaming technologies such as Azure Stream Analytics. Power BI is integrated to the service allowing to build interactive dashboard for real-time analysis. | . As such ASA is the right choice if you need to do modern data warehousing, advanced analytics, Data exploration and discovery, real time analytics and data integration to ingest, prepare, model and serve the data to be used by downstream systems (outside or inside AZA). . . Architecture of Azure Synapse Analytics . . We could divide the architecture in 4 main groups: . Compute options: Provisioned SQL Pools (old Azure DW), on-demand SQL Pools (Serverless, can directly query Azure Data Lake, useful for ad-hoc analysis). We also got Provisioned Spark Pools (C#, Scala, Python, R, Java). be careful this is not Databricks. | Orchestration options: Azure Data Factory with ADF Mapping Data Flows | Storage options: Data Lake Store Gen2, a shared Metadata store that permits to share databases and tables between the different engines. You can for example access Spark tables with the SQL engines to create their own objects. | Workspace options: Azure Synapse Studio (a complete development environment to use), Monitoring and Management. | . Synapse SQL Pools . You have two approaches for the Synapses SQL pool: . The dedicated model is referred to as dedicated SQL Pools. It refers to the data warehousing features that are generally available in Azure Synapse Analytics. Dedicated SQL pools represent a collection of resources (nodes) that are being provisioned when using Synapse SQL. Use the dedicated setup when you need (i) predictable performance and cost, (ii) create dedicated SQL pools to allocate processing power for data permanently stored in SQL tables. | The serverless model is ideal for unplanned or ad hoc workloads as performing data exploration or preparing data for data virtualization. | . With Synapse SQL pool you can import Big Data by different ways. You can: . directly load data with T-SQL and the COPY statement . COPY INTO [dbo].[YOUR_TABLE] FROM &#39;https://URI_TO_YOUR_BLOB&#39; WITH ( FIELDTERMINATOR=&#39;,&#39;, ROWTERMINATOR=&#39;0x0A&#39; ); . | load data with the pipeline and the Copy Data activity . | use T-SQL and PolyBase to define external tables. Polybase is tool that virtualizes the external data through the SQL pool, so that it can be queried in place like any other table. In Azure Synapse you can use Polybase only with data stored in Azure Data Lake. Loading data with polybase implies to follow some steps (First set the external data source typically Azure Data Lake. Second create an database scoped credential to import data. Third Create the file format.) . | . Architecture of Synapse SQL . . Synapse SQL is a distributed computational architecture to process data across several nodes. | Compute is uncoupled from storage allowing to scale compute resource independently of the data / storage. For dedicated pool scaling is based on the data warehouse unit (DWU) that is a combination of CPU, memory, and IO whereas scaling is done automatically for serverless SQL pool. | Query are sent to the control node. The control node will use the massively parallel processing engine (MPP engine) to distribute the query across several worker nodes. The MPP engine optimizes the queries for parallel processing. | The compute nodes store all of the user data in Azure Storage (Data Lake). More interestingly note the presence of a Data Movement Service implemented to optimize the movement of Data across nodes. | . Usage cost . To monitor usage cost, Azure uses a metric called Data Warehousing Unit (DWU) that is a bundle of resources (CPU, memory and IOs). You need to specify the number of DWU you need for the pool. If you want higher performance for your workload at any point in time, just increase the numbers of DWU. . Create an Azure Synapse Analytics instance . The procedure is very similar to create an Azure SQL instance. You need to specify the SQL pool name and choose a Server. If you already have deployed SQL server you can use them. Otherwise create one. . . Designing Tables in Azure Synapse Analytics . The concepts described now are some definitions of common Data Warehousing approaches and do not strictly apply to Azure Synapse. Typically in a data warehouse you have three types of tables: . Integration tables that are used to integrate the raw data into the staging area. | Fact tables that contain quantitative data generally loaded from transactional table (e.g, sales). | Dimension tables that contain attribute data that does not change frequently and use to make joins with the fact tables. | . When using Azure Synapse Analytics or another data warehousing tool like Hive, you have different choice for the persistence of your tables. In Azure Synapse you have: . Regular tables that are stored in Azure storage as part of the SQL. | Temporary tables that last as long as the session and use local storage for performance. | External tables that are useful to first load data. In Azure, external tables point to and reference data in an Azure Storage blob or Azure Data Lake. | . When created tables you need to choose the types of tables you want. be careful choosing the type will have impact on performance: . Hash-distributed tables - The table rows are distributed across the Compute nodes by using a deterministic hash function to assign each row to one distribution. A distribution is the basic unit of storage and processing for parallel queries. This particularly good for querying large tables (more than 2 GB) or if you have frequent INSERT, UPDATE, DELETE operations. . The choice of the distributed columns is important. NULLABLE columns are bad candidates as they are going to generate the same hashed result. As such, rows will end up on the same skewed distribution. Similarly, fact tables with columns containing default values or date values are not good candidates for hashed distribution. Large fact tables or historical transaction tables are usually stored as hash-distributed tables. These tables usually have a surrogate key that is monotonically increasing and are used in JOIN with other facts or dimension tables. These surrogate keys are good candidates for distributing the data as they are unique. . | Replicated tables - A table that is replicated caches a full copy of the table on each compute node. Consequently, replicating a table removes the need to transfer data among compute nodes before a join or aggregation. Replicated tables are best utilized with small tables. Extra storage is required and there is additional overhead that is incurred when writing data, which make large tables impractical. . | Round-robin tables - A round-robin distributed table distributes data evenly across the table but without any further optimization. A distribution is first chosen at random and then buffers of rows are assigned to distributions sequentially. It is recommended to load data into a round-robin table for staging. JOIN operations on round-robin tables require reshuffling data, which takes additional time. . Dimension tables or other lookup tables are stored as round-robin tables. These tables connects to more than one fact tables and optimizing for one JOIN may not be the best idea. Usually, dimension tables are smaller which may leave some distributions empty when hash distributed whereas round-robin by definition, guarantees a uniform data distribution. . | . Spark pools . Spark pool clusters are groups of computers that are treated as a single computer to handle the execution of commands issued from notebooks. Clusters allow processing of data to be parallelized across many computers to improve scale and performance. It consists of a Spark Driver and Worker nodes. The Driver node sends work to the Worker nodes and instructs them to pull data from a specified data source. Moreover, you can configure the number of nodes that are required to perform the task. . Some usage details: . It takes around two minutes for fewer than 60 nodes and around 5 minutes for more to spin-up. The instance shut down automatically and by-default 5 minutes after the last job has been executed unless it is kept alive by a notebook connection. | You can create Spark pool with the portal, Power Shell or C# SDK | You can enable auto-scale to let pools automatically scale-up or down based on the workloads. Spark pools can be shut down with no loss of data since data is stored elsewhere on a Data Lake. | Spark pools can use Data Lake Store Gen2 and Blob storage. | Come with Anaconda librairies pre-installed. | . If you need to process big data workloads that cannot be handle by Azure Synapse SQL use the Spark pools. If you have some Spark code already implemented use it! Do not re-implement it with Spark pools. .",
            "url": "https://fbraza.github.io/BrazLog/cloud/azure/data%20engineering/2021/07/27/Notes-Azure-synapse-analytics.html",
            "relUrl": "/cloud/azure/data%20engineering/2021/07/27/Notes-Azure-synapse-analytics.html",
            "date": " • Jul 27, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Azure Data Engineer Notes Azure SQL",
            "content": "I am going to share my notes taken during the preparation for the Azure Data Engineer certification (DP-203). I will share them through a series of articles that cover the main concepts and features behind the Azure services needed for Data Engineering. The last set of articles will be focused on cloud architectures and pattern with Azure. If you want to access a specific topic have click to the desirable article below: . Cosmos DB | Azure SQL | Azure Synapse Analytics | Azure Data Factory | Azure Databricks | Azure Event Hub and Azure Stream Analytics | . Here, we are going to talk about the solutions proposed by Azure to deploy and maintain SQL databases. We will focus on understanding what are the different options proposed and how we can deploy and maintain them according to our needs. . Azure SQL service . There are three main deployment options and choices that can fit your need if you want to deal with Azure SQL services. . . SQL database . This is a fully managed platform as a service (PaaS) that takes care of the database software upgrades,the patches, the backups and the monitoring. using this service you can provide a highly available storage layer for your applications. You have two deployment options: (i) the single database which is a managed and isolated database and the (ii) the elastic pool which is a collection of single databases that have a shared set of resources (CPU and memory). . SQL managed instance . They are considered as Platform as a Service (PaaS) deployments. These options propose a fully managed engine that automates most of the database management tasks (upgrades, patches, backups and monitoring). Both have a set of common features but have distinct use cases. Notably, use the managed instances if you want to have access to some specific services / functions withing your DB (Machine learning services for example). Additionally, you can specify if you want a single instance or a pool of instances. . SQL Server on Azure Virtual Machines . It is a version of SQL server running in a Azure VM. It’s basically just SQL Server. This is an Infrastructure as a service (IaaS) with which you need to update and patch both OS and SQL server yourself. If your use case need such deployment, here a few things to consider that we gather in the methodology column in the table below: . Service tiers . DTU model . Known as the Database Transaction Unit purchasing model. This model is based on a bundled measure of compute, storage and I/O resources. There are different service tiers available for the DTU model: .   Basic Standard Premium . Target workload | Development and production | Development and production | Development and production | . Uptime SLA | 99.99% | 99.99% | 99.99% | . Maximum backup retention | 7 days | 35 days | 35 days | . CPU | Low | Low, Medium, High | Medium, High | . IOPS (approximate)* | 1-4 IOPS per DTU | 1-4 IOPS per DTU | &gt;25 IOPS per DTU | . IO latency (approximate) | 5 ms (read), 10 ms (write) | 5 ms (read), 10 ms (write) | 2 ms (read/write) | . Columnstore indexing | N/A | S3 and above | Supported | . In-memory OLTP | N/A | N/A | Supported | . Note that the backup retention time is different across the different tiers. However you can still enabled a Long-Retention period up to 10 years. In memory OLTP is enabled for the premuim tier. Premium are chosen for intensive I/O workload. . Please keep in mind that DTU is not available for Azure SQL managed instances . Virtual core model . This model allows a lot of flexibility given your use case. You have access to three groups of tiers: . General purpose tier . It is a budget-friendly tier designed for most workloads with common performance and availability requirements. Here you can choose: . Provisioned compute: for regular usage pattern, fixed amount of resources over time and you are billed regardless of usage. You need to manage the sizing compute resources depending on your workload. | Serverless compute: for intermittent and unpredictable usage. It provides automatic scaling for compute resources and, you are billed only for the amount of compute used. This option also supports automatic pausing (in this setting you only pay for the storage). | . The business critical tier . Business critical tier is designed for performance-sensitive workloads with strict availability requirements. This is particularly useful if you want low-latency, high resilience to failures with isolated replicas and use the In-Memory OLTP featue to improve performances. . The hypescale tier . It is designed for business workload that need highly scalable storage (+100TB) and read-scale requirements. Cost are between the general purpose and the business critical tiers. This tier is not available for managed instances. if you choose this one you cannot go back and change another tier. This is due to the underlying architecture needed to deploy a hyperscale Azure SQL database. . Wrap-up . Please see below a table that contains all strucutre the difference between the differnet tiers and display more specific details. . - Resource type General Purpose Hyperscale Business Critical . Best for |   | Offers budget oriented balanced compute and storage options. | Most business workloads. Auto-scaling storage size up to 100 TB, fluid vertical and horizontal compute scaling, fast database restore. | OLTP applications with high transaction rate and low IO latency. Offers highest resilience to failures and fast failovers using multiple synchronously updated replicas. | . Available in resource type: |   | SQL Database / SQL Managed Instance | Single Azure SQL Database | SQL Database / SQL Managed Instance | . Compute size | SQL Database | 1 to 80 vCores | 1 to 80 vCores | 1 to 128 vCores | .   | SQL Managed Instance | 4, 8, 16, 24, 32, 40, 64, 80 vCores | N/A | 4, 8, 16, 24, 32, 40, 64, 80 vCores | .   | SQL Managed Instance pools | 2, 4, 8, 16, 24, 32, 40, 64, 80 vCores | N/A | N/A | . Storage type | All | Remote storage | Tiered remote and local SSD storage | Local SSD storage | . Database size | SQL Database | 1 GB – 4 TB | 40 GB - 100 TB | 1 GB – 4 TB | .   | SQL Managed Instance | 32 GB – 8 TB | N/A | 32 GB – 4 TB | . Storage size | SQL Database | 1 GB – 4 TB | 40 GB - 100 TB | 1 GB – 4 TB | .   | SQL Managed Instance | 32 GB – 8 TB | N/A | 32 GB – 4 TB | . TempDB size | SQL Database | 32 GB per vCore | 32 GB per vCore) | 32 GB per vCore | .   | SQL Managed Instance | 24 GB per vCore | N/A | Up to 4 TB - limited by storage size | . Log write throughput | SQL Database | Single databases: 4.5 MB/s per vCore (max 50 MB/s) Elastic pools: 6 MB/s per vCore (max 62.5 MB/s) | 100 MB/s | Single databases: 12 MB/s per vCore (max 96 MB/s), Elastic pools: 15 MB/s per vCore (max 120 MB/s) | .   | SQL Managed Instance | 3 MB/s per vCore (max 22 MB/s) | N/A | 4 MB/s per vcore (max 48 MB/s) | . Availability | All | 99.99% | 99.95% with one secondary replica, 99.99% with more replicas | 99.99%, 99.995% with zone redundant single database. | . Backups | All | RA-GRS, 1-35 days (7 days by default) | RA-GRS, 7 days, fast point-in-time recovery (PITR) | RA-GRS, 1-35 days (7 days by default) | . In-memory OLTP |   | N/A | Partial support. Memory-optimized table types, table variables, and natively compiled modules are supported. | Available | . Read-only replicas |   | 0 built-in 0 - 4 using geo-replication | 0 - 4 built-in | 1 built-in, included in price 0 - 4 using geo-replication | . Pricing/billing | SQL Database | vCore, reserved storage, and backup storage are charged. IOPS is not charged. | vCore for each replica and used storage are charged. IOPS not yet charged. | vCore, reserved storage, and backup storage are charged. IOPS is not charged. | .   | SQL Managed Instance | vCore, reserved storage, and backup storage is charged. IOPS is not charged |   |   | . Create, deploy, and verify Azure SQL . For Azure SQL managed instance, giving a server name is the same as in SQL Server. For Databases and elastic pools, an Azure SQL Database server is required. This is a logical server that acts as a central administrative point for single or pooled database. It contains logins, firewall rules, auditing rules, threat detections policies and failover groups. For Azure SQL Database server the name must be unique across all Azure. . With the vCore model we can choose one of the three service-tiers we described before. Here we chose the General Purpose tier with the Provisioned compute tier. . Next note how the vCore model allows you to specify the amount of resources you want to allocate to your Database Server. The estimated cost is calculated in function of the selected resources. . Note that you may use some tools like the Data Migration Assistant SKU to estimate the number of vCores and the Data max size if you are not sure about what you should do, especially if you migrate your on-premises databases on Azure Cloud. . . Next we can choose the connectivity options we want to access the Database. Your choice will depend on your specific requirements. Choices for Azure SQL Database or Managed Instance are different: . For SQL Databases the default is No access. You can then choose a select or private endpoint. You can also add your client IP address if you want to connect from your current client. | For Managed Instance you deploy inside an Azure virtual network with a dedicated subnet. As such you have a secure private IP. You can also enable public endpoint if you want to connect through the Internet without VPN. This access is disabled by default. | . . Next you can decide which data source you want to use. You can start from scratch, use a sample of data or a Backup. Additionally you also need to specify the data collations. It is to tell the database engine how to treat certain characters (it affects the characteristic of several operations). . . **Notes about Database collation: . In SQL server it is defined by OS locale. | In Azure SQL managed instance you can set collation upon the creation of the instance (cannot be changed later for the instance but can be changed at the database or column level). | With SQL Database you cannot change it at the server level but can alter it at the database level. | . Working with Elastic pools . Elastic pools are a cost effective solution you can choose when you need to manage several databases. This is particularly useful if you have unpredictable usage demands and don’t know the right setting to apply for your databases. Indeed you can group them into a pool. . All the databases on a elastic pool are on a single server and share a set of resources. . Similarly to single Azure SQL single Database you can choose between the DTU-based or the vCore-based pricing models. You can also specify the minimum and maximum number of resources shared between the databases that are added to the pool. As such, the database that are not used a lot will not consume much resources in the pool. . Geo-replication . This feature allows you to create a readable databases on a server in the same or different region. This particularly useful if you want to ensure business continuity so that an application recovers quickly from some unpredicted disaster and initiate a failover to the secondary database. . You can have up to four secondaries databases in the same or different regions . Note some specificities: . The secondaries are read-only copies of the database. | Consequently they are useful to offload / redirect read-only workload on the secondary database. | . To set geo-replication go to the Ge-Replication section and select a secondary target region. If the targeted region does not have one SQL server already deployed you will need to create one. . . Security . Network security . That is the first security layer you need to engineer and think about. Selective access to your database by services, applications and machines is critical to pave the way towards an secure database architecture. There are different way to tackle this. . . With the Allow access to Azure services and resources option set to “yes”, you permit to any resources from any region or subscription to access the SQL database. It is a very easy setup to get things up and running and get your SQL database connected to other Azure services as Azure VMs, Azure App Service. However, It is of good practice to not use this approach and prefer to set firewall rules instead of allowing all services to connect to your SQL database. With firewall rules you add a unique firewall rule for each service or resource to connect. With thses rules you can permit to your on-premises environment to connect to your SQL database. You just need to also add the rules in your on-premise environment. . . Setting the connectivity between resources with firewall rules is tedious. First you will have to enter manually all rules and all IP addresses. Second you might struggle if some IPs are dynamic. Instead you can use virtual network rules that control access for specific networks that contains machines and / or services. You can allow all connections that come from one specific virtual net- work. This simplifies the challenge of configuring access for all static and dynamic IPs. In Virtual networks your machines expose a private IP address. Beside having private Ips you still connect through a public endpoint. . . In the last scenario for Azure SQL database, we get rid of the public endpoint and make it private: this is the concept of private link. You can connect to your Database using a private endpoint. It means that it has a private IP within a specific virtual network. We still have our architecture with Virtual Network, but here rules are unnecessary. Resources that need to connect to the Database must have access to the virtual network where the endpoint is located. As such any connection coming from the Internet will be denied. . For Azure SQL managed instances . . You cannot use Azure Private link with managed instances. For an Azure SQL managed instance, you first need a specific subnet (here the MI-subnet). The subnet is a logical grouping in a virtual network. After the deployment the instances are configured similarly to a private endpoint in a database in Azure SQL Database. With standard networking practice you must enable access to the virtual network where the managed instances are located. . Dynamic Data masking . Dynamic data masking is a technique used to limit the exposure of sensitive data to non-privileged users. You can mask your data so that non-privileged users can’t see it but are still able to use query that include the data. For example if you have data with names and e-mail addresses you can mask columns with the following T-SQL commands: . ALTER TABLE Data.Membership ALTER COLUMN FirstName ADD MASKED WITH (FUNCTION = &#39;partial(1, &quot;xxxxx&quot;, 1)&#39;) ALTER TABLE Data.Membership ALTER COLUMN Email ADD MASKED WITH (FUNCTION = &#39;email()&#39;) ALTER TABLE Data.Membership ALTER COLUMN DiscountCode ADD MASKED WITH (FUNCTION = &#39;random(1, 100)’) GRANT UNMASK to DataOfficers . There are different policies in place to mask the data: . Default - This is a full masking of the data. For numeric data types the value will be set to 0 and for strings,XXX characters are used to mask fields. | Credit card - Here only the last four digits of the credit card are shown. | Email - This exposes the first letter and replaces the domain with XXX.com. | Random number - This function generate a random number based on the selected boundaries and the actual data type. | Custom text - Here you can define the exposed prefix, the padding string and the exposed suffix, | . Once data masking is et up, you can identify when a user tries to access data from any of the masked column using Auditing. Indeed Azure SQL auditing tracks all databases events and writes them to an audit log in your Azure Storage account in Append Blobs. . Data encryption . Transparent data encryption (TDE) . It is used to protect data at rest for an Azure SQL database. When using TDE, your data, but also backups and transaction logs are encrypted at rest. With the default settings TDE uses a database encryption key that is protected by a built-in server certificate. . You have one distinct built-in certificate by database server | You can also use customer-managed keys for the encryption (Bring Your Own Key service - BYOK). Here the encryptions keys can be managed by by the Azure Key vault service. With this scenario you are responsible for the control of the key life-cycle, key usage permissions and auditing operations on keys. | To enable TDE for SQL server here the steps yo must follow: . Create a master key. | Create or obtain a certificate protected by the master key. | Create a database encryption key and protect it by using the certificate. | Set the database to use encryption. | . | . Always encrypted . This is used to protect sensitive data at rest on the server, in-transit between client and server and when the data is in use. As expected, keys are used to encrypt the data and, only client applications or applications servers with access to the keys, will be able to see the data in plain text. You have two types of keys used for the encryption process: . Column encryption keys - It is used to encrypt the data. | Column master keys - It is used to encrypt the column encryption keys. These need to be stored in a trusted location like the Windows Certificate store or the Azure Key Vault service. | . Always encrypted feature support two types of encryption: . Deterministic encryption - This will ALWAYS generate the same encrypted value for any text value. You can still perform point lookups, equality joins, grouping and indexing on deterministically encrypted columns. | Randomized encryption - This is more secure because the encrypted value is less predictable, however, you cannot perform, searching, indexing, grouping or joins if columns are encrypted with this method. | . To be able to use this feature you need to . Create an instance of the Azure key Vault service. | Encrypt columns on SQL server / instance where you can also choose the type of encryption and the key store (Azure key vault) This will create the new master key, the new encryption key and encrypt your column. | . | . Manage security . Once your instance is secured (network, authentication and data), we need to manage security on an ongoing basis. This includes auditing, monitoring, data classification and in the case of Azure SQL, Azure Defender. Here let’s give a few words about Azure Defender that is a unified package for advance SQL security capabilities that enables: . Vulnerability Assessment: a scanning service that provides visibility into security state and suggest approaches to address any potential concerns. You can activate the periodic recurring scans for database checking every seven days. Reports can be sent to an administrator and store in a storage account (Blob store). | Advanced Threat Protection: uses advanced monitoring and artificial intelligence to detect whether any of the following threats have occurred: SQL injection, SQL injection vulnerability, Data exfiltration, unsafe action, brute force attempt, anomalous client login. | . For example if you need to prevent data leakage here what you should do: . Enable Advanced threat detection | Configure the service to send alerts for threat detections of type “Data Exfiltration” | Configure the service to send email alerts to the IT | . Monitor and mange your Azure SQL deployment . Backup and restore . Azure SQL manages backups and runs if the full recovery model is used. It can restore your database to any point in time. You can even restore a deleted database within the configured retention policy. Most interestingly your backups can be automatically encrypted if TDE is enabled on the logical server or instance. By default, a full database backup is taken once a week. Log backups occur every 5-10 minutes and differential backups every 12-24 hours. Backups files are stored in Azure storage in read-access geo-redundant storage (RA-GRS) by default (it is possible to set ZRS or LRS). . The retention period for your data varies between 1 and 35 days. If not enough you can choose long-term retention (LTR). This option automatically creates full backups stored in RA-GRS by default for up to 10 years. LTR is available for Azure SQL Database and in preview for Azure SQL managed instanced. . Tuning performance in Azure SQL . Automatic tuning . Automatic tuning ensures peak performance and stable workloads for Azure SQL Database and Azure SQL Managed Instance through continuous performance tuning based on AI and machine learning. The automatic tuning options available for both Azure SQL Database and Azure SQL Managed Instance are the following: . Automatic tuning option Single database and pooled database support Instance database support . CREATE INDEX - Identifies indexes that may improve performance of your workload, creates indexes, and automatically verifies that performance of queries has improved. | Yes | No | . DROP INDEX - Drops unused (over the last 90 days) and duplicate indexes. Unique indexes, including indexes supporting primary key and unique constraints, are never dropped. This option may be automatically disabled when queries with index hints are present in the workload, or when the workload performs partition switching. On Premium and Business Critical service tiers, this option will never drop unused indexes, but will drop duplicate indexes, if any. | Yes | No | . FORCE LAST GOOD PLAN (automatic plan correction) - Identifies Azure SQL queries using an execution plan that is slower than the previous good plan, and queries using the last known good plan instead of the regressed plan. |   |   | . By default the following parameters will be enabled or disabled: . FORCE_LAST_GOOD_PLAN = enabled | CREATE_INDEX = disabled | DROP_INDEX = disabled | . Data Discovery &amp; Classification .",
            "url": "https://fbraza.github.io/BrazLog/cloud/azure/data%20engineering/2021/07/25/Notes-Azure-SQL.html",
            "relUrl": "/cloud/azure/data%20engineering/2021/07/25/Notes-Azure-SQL.html",
            "date": " • Jul 25, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Guide for optimizing performance with Spark",
            "content": "Apache Spark is an in-memory processing and analytics engine designed to be particularly efficient to process Big Data. You can use Spark on an on-premise or a cloud-deployed Hadoop cluster or through the Databricks platform. In any of these setups, using Spark efficiently is critical if you want to control and reduce costs. For that you should be able to diagnose and resolve some common performance issues. These usually fall into the five following categories: spill, skew, shuffle, storage and serialization. Here we are going to go over each of them to understand what they are, see how to identify them and mitigate their impact on your workflow. . Skew . Definition and root causes . In Spark, data is distributed across a cluster as partitions that are processed by different worker nodes. Usually partitions are 128 MB sized and evenly distributed. In reality however, some of your partitions can have significantly more records than others. Typically you face data skewness when using join or groupBy operations using a key that is not evenly distributed. This is not a Spark specific problem but keep in mind that the distribution of the data dramatically impacts on the performance of distributed systems. Let’s imagine that in your workflow your data ends up being partitioned as shown below: . . As partition one (P1) is around four times bigger than the others, it takes four time as much time and requires four time as much RAM to process P1. Consequently, the entire Spark job is slower. More specifically, the stage, including these tasks, takes as much time as the P1 processing task. Finally, when P1 does not fit in memory, Spark raises out-of-memory (OOM) errors or undergo some spill on disk, another issue described later. . Monitoring skew . To monitor if your data is skewed, on the Spark UI go on the Stages tab and read the timeline. If the tasks execution time is not evenly distributed and some task takes a dramatic amount of time compared to others, you data is skewed. . Mitigating skew issues . Remember skew is a data problem. Several approaches exist to solve and mitigate it. Here three of them that you should consider when using Spark: . Use the well-known “salting-key” strategy which briefly consists on concatenating the key with a random number. This randomizes the data and redistribute it more evenly. . | Use query hints to annotate and help the optimizer engine to improve logical execution plans. . | Use the Adaptive Query Execution framework that shipped with Apache Spark 3.0 by enabling its features: . // Enable AQE and the adaptive skew join spark.conf.set(&quot;spark.sql.adaptive.enabled&quot;, true) spark.conf.set(&quot;spark.sql.adaptive.skewedJoin.enabled&quot;, true) . | . Spill . Definition and root causes . When the partitions are to big and cannot fit in memory, Spark moves the data on disk and gets it back later in memory. This phenomenon, called spill, is made possible thank to the ExternalAppendOnlyMap collection class. This prevents OOM errors when partitioned data is too big to fit in memory. Potentially expensive disk I/O operations arise as a trade-off from this operation. There are different scenarios where spill happens: . Ingesting too large partitions. . | Aggregating tables on a skewed column. . | Using join(), crossjoin() or the explode() operations may create very large partitions. . | Using the union() operation. This operation takes two DataFrames and combine them into one and always use the same number of partitions that it started with. As depicted below we start with two DataFrame (DF1 &amp; DF2) with a certain number of partitions and end up with the same number of concatenated partitions that are bigger. . . | Setting an inappropriate value (too big usually) to the spark.sql.files.maxPartitionBytes parameter (set to 128 MB by default). Our advice is to keep it at default and only alter it after some testing. . | . Monitoring spill . To assess whether spill happened during your jobs, the easiest way is to go to the Stages tab from the Spark U and read the summary metrics table for all completed tasks. There, spill is represented by two values: . Spill (Memory): this is the size of the data as it existed in memory | Spill (Disk): this is the size of the data as it existed in disk | . Please note two things: first, the Spill value in disks will be always lower than Spill value in memory due to compression. Second, if no spill occurred you won’t find these values in the summary metrics table. . To know whether spill is occurring during the execution of your jobs, and not wait its end, use the Spark.Listener class in your code. A SparkListener object captures events from the Spark scheduler over the course of a Spark application execution (it is used to output the logs and metrics in the Spark UI). You can implement your own custom SpillListener to track spill. A very nice example can be found in the spark Github repository: . // from https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/TestUtils.scala /** * A `SparkListener` that detects whether spills have occurred in Spark jobs. */ private class SpillListener extends SparkListener { private val stageIdToTaskMetrics = new mutable.HashMap[Int, ArrayBuffer[TaskMetrics]] private val spilledStageIds = new mutable.HashSet[Int] def numSpilledStages: Int = synchronized { spilledStageIds.size } override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized { stageIdToTaskMetrics.getOrElseUpdate( taskEnd.stageId, new ArrayBuffer[TaskMetrics]) += taskEnd.taskMetrics } override def onStageCompleted(stageComplete: SparkListenerStageCompleted): Unit = synchronized { val stageId = stageComplete.stageInfo.stageId val metrics = stageIdToTaskMetrics.remove(stageId).toSeq.flatten val spilled = metrics.map(_.memoryBytesSpilled).sum &gt; 0 if (spilled) { spilledStageIds += stageId } } } . Mitigating spill issues . A quick answer would be to add more memory to your cluster’s workers. If not possible, decrease the size of each partition by increasing the number of partitions generated during data processing. In Spark you can: . configure the default number of partitions. . spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, number_of_partitions) . | repartition the data with the repartition() method (be careful this is an expensive operation). . | configure the size of each partition with the spark.sql.files.maxPartitionBytes Spark setting. . | solve any issues related to skewed data first. . | . Shuffle . Definition and root causes . Shuffle occurs when Spark needs to regroup data from different partitions to compute a final result. It is a side effect observed with wide transformations. These include for example the groupBy(), distinct() or join() operations. Let’s explain shuffle by going through a quick and simple example involving a groupBy() combined with a count() operation. . . First the data is read from source (i.e., HDFS, cloud storage, previous stage) (1). At stage 1, Spark performs a “mapping” operation to identify which record belongs to which group (2). Then data is prepared for the next partitions and written on disk in shuffle files (3). For the next stage, data is read from the shuffle files and transferred through the network to the next executors (4) where a “reduce” operation is performed. Our final result is computed (5) and data written on disk (6). . Shuffle is a potentially very expensive operation that involves a lot of disk and network I/O which impact on Spark performance. Additionally, keep in mind that whatever type of wide transformations you are executing, the mapping and the reduce operations are performed in-memory and remain susceptible to some spill on disk which will add to the overhead of disk I/O. . Monitoring shuffle . The Spark UI is useful to get some statistics about the shuffled data in the Stages tab. In the summary metrics table, have a look to the following entries: . Shuffle Read Size / Records: this the total of shuffle bytes read locally or from remote executors. | Shuffle Remote Reads: this is the total of shuffle bytes read only from remote executors. | Shuffle Read Blocked Time: this the time spent awaiting for shuffle data to be read from remote executors. | . While these numbers are interesting, there are no conventional threshold. It depends on your data and the way you process it. Reducing the amount of shuffle is something you should target but keep in mind that shuffle is a necessary evil. With the Spark UI, you can identify the most expensive tasks and try to reduce shuffle in these cases. Check also if data is not skewed (watch the timeline) or if spill occurred before focusing on shuffle. Finally knowing how to mitigate it properly should permit you to keep these metrics below acceptable thresholds. . Mitigating shuffle issues . There are different approaches against shuffle: . to reduce the network I/O, design your cluster by favoring fewer but larger workers. This limits the number of machines where the data is shuffled across. . | to reduce the amount of data being shuffled, filter out columns or unnecessary records for your processing and analysis. . | to optimize join queries, when the size of one DataFrame is small (below 10 MB), Spark uses Broadcast joins (see the spark.sql.autoBroadcastJoinThreshold, in the documentation). But be careful with it and be sure to have enough memory on the driver as smallest partitions are processed there. . | to optimize joins, use bucketing (spark.sql.sources.bucketing.enabled is set to true by default). Bucketing is an optimization technique to pull down data into distinct manageable parts named “buckets”. Use the bucketBy() method to bucket your data based on specific columns. Notably bucketing your data by sorted keys permits to avoid expensive shuffle operations. . | to reduce the shuffle behavior you can edit several spark configuration properties. You should not blindly play with these but rather test your changes before going into production. We advise the reader to focus on the following properties to limit shuffle and expensive I/O activity: . Configuration Description and recommendation . spark.driver.memory | The default value is 1GB. This is the amount of memory allocated to the Spark driver to receive data from executors. You can change while submitting a spark job with the spark-submit command. Increase the value if you expect the driver to process more a great amount of data notably in the context of a broadcast join. | . spark.shuffle.file.buffer | The default value is 32 KB. If your workload increased, set it to larger values (1 MB). The more you have the more Spark will buffer your data before writing mapping result on disk. | . spark.file.transferTo | Set to true by default. Set it to false if you want Spark to use the file buffer before writing on disk. This decrease the I/O activity. | . spark.io.compression.lz4.block.Size | The default value is 32 KB. By increasing it you can decrease the size f the shuffle file (don’t go over 1 MB). By default Spark uses lz4 compression but you can change the compression codec by altering the property spark.io.compression.codec | . | . Storage . Definition and root causesStages . When talking about the impact of storage on performance, we talk about the overhead I/O cost of data ingestion. The most common example relate to: . reading tiny files . The “tiny files problem” has been pinpointed and described since the existence of distributed system like Hadoop. Things are similar with Spark. Before executing any query on your data, Spark will assess how many tasks are required to read the input data and determine on which worker it should schedule these tasks. Moreover, some files contain metadata (i.e., ORC, Parquet…) to be read and parsed. Then with a huge number of small files, you increase the workload on the Spark Scheduler, number of read / close file operations and metadata to parse. Collectively these operations greatly impact on Spark performance. . | scanning repositories . Directory scanning adds overhead to the tiny files problem. But it also exists for terabytes files especially in the context of highly partitioned datasets on disks. For each partition you have one directory. If we consider some data partitioned by year, month day and hour we will have 8640 directories to scan! If you let your data scale for 10 years you will end with 86400 directories. Keep in mind that the Spark driver scan the repository one at the time. . | dealing with dataset schemas . inferring schema with Spark for csv and json files also impairs performance in Spark. Indeed it requires to do a full scan of the data to assess all types. In contrast, Spark only reads one file when dealing with the Parquet format. This is under the assumption that all Parquet files under the same partition have the same schema. But be careful if you wish to support Parquet schema evolution. For each new schema evolution, you have a new partition with new files. If you alter the schema a lot you progressively fall into the scanning issue as described before. By default, schema evolution is disabled in Spark 2 &amp; 3 but if you need it use the spark.sql.parquet.mergeSchema property. . | . Monitoring storage . On the Spark UI you have access to some interesting metrics to monitor file scanning and count: . In the Stages tab when looking at the stage details, have a look to the Input Size / Records metrics which gives you an idea about the total amount of data that is ingested versus the number of records. | In the SQL tab, select your job and stage to have access to more details as the total number of files read, scan time total and filesystem read time (sampled) total (min, med, max). | . Mitigating storage issues . Measures to mitigate these issues are pretty simple: . avoid using tiny files if possible, or merge them into bigger files before performing any operations on your data. | keep in mind that the reading / scanning problem cannot be solved by adding more resources to your workers. Everything is handled by the driver. | partition your data according to your needs. Avoid over-partitioning, if not necessary, although this will depend on the data problem you are tackling. | . Serialization . Definition and root causes . Serialization improves performance on distributed applications by converting code objects and data into a stream of bytes and vice-versa. For distributed systems like Spark, the majority of the compute time is spent on data serialization. . When writing and executing code, the Spark driver serializes the code, send it to the executors that deserialized the code to execute it. By default, Spark uses Java serialization with the ObjectOutputStream framework that works with any types and classes that implement java.io.Serializable. The Kryo serialization permits to serialize data faster but is not compatible with every classes and types. You also need to do extra-work before to register the classes you want to be serialized. . At the beginning of Spark, users were mostly dealing with resilient distributed datasets (RDDs) by writing empiric code which describes how you want to do things. For RDDs, Spark uses Java serialization to serialize individual Scala and Java objects. This process is expensive especially because the Java objects model is highly memory consumptive. This is even more expensive with Pyspark where code and data are serialized / deserialized twice: first to Java/Scala and then to Python. . Note: For PySpark all data that come to and from a Python executor has to be passed through a socket and a Java Virtual Machine (JVM) worker. Briefly, with PySpark, the SparkContext uses Py4J to launch a JVM to create a JavaSparkContext. That is the communication between Py4J and the JVM that orchestrate the data flow. It is worth noting that Py4J calls have pretty high latency. That is why all operations on RDDs takes much more time on PySpark than on Spark. . The project Tungsten in 2004 and the design of the DataFrame API were critical steps towards performances improvement of the Spark engine. The first altered and improved the Java objects model allowing Spark to manage data expressed as DataFrame much more efficiently. The API permits us to write more declarative code that will be processed as instructions for the transformation chain. Both minimize the amount of work required by the Spark JVMs. And if you work with PySpark note that in this context nothing will be done in Python then excluding the double serialization needed with RDDs. . All these theoretical details are not easy to grasp but shed light on a very important aspect of Spark: each time you will get away from the DataFrame API in your code, you will lose all these optimizations and encounter some performance hits. This is notably the case when you: . process manually RDDs (writing map() and / or lambda functions). | use user-defined functions (UDFs) that are useful and easy-to-use to extend Spark SQL functionalities. However, UDFs are “black-box” and prevent several Spark optimization processes including the way Spark deals with Java objects. For example, using UDFs in PySpark will bring you back to the double serialization issue. | . Mitigating storage issues . Here the rules are simple: . USE THE DATAFRAME API. Dig into the API to know exactly the possibilities offered to you. | Use UDFs only when strictly necessary. By design Scala will be faster than Python when using UDFs. With Python however you can still give a try to the pandas UDFS (also called vectorized UDFs) that use apache Arrow to give a performance boost to PySpark in this context. | Give a try to the Kryo serialization framework despite its limitations. You may benefit from its performance boost for your current data problem. It is actually the default framework used by Spark in Apache Hudi. | . Conclusion . Using Spark efficiently requires a good knowledge of its inner parts and an ability to identify technical and performance issues. Here we framed and articulated the five main problems that you may encounter and have discussed a number of techniques to bypass them and optimize your Spark applications. Noteworthy, do not take this article as a step-wise guide that will solve all your problems for all your situations. Instead, it should give you an idea of what is happening and what are the solutions you can implement based on your project. This lets a lot of rooms for experimentations and testing that are necessary to find the right and optimal balance for your Spark applications. .",
            "url": "https://fbraza.github.io/BrazLog/spark/python/scala/2021/07/08/spark-optimization.html",
            "relUrl": "/spark/python/scala/2021/07/08/spark-optimization.html",
            "date": " • Jul 8, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Modern Python part 3 - run a CI pipeline & publish your package to PiPy",
            "content": "To propose a well-maintained and usable Python package to the open-source community or even inside your company, you are expected to accomplish a set of critical steps. First ensure that your code is unit tested. Second respect the common writing and format styles. Automate these steps and integrate them in a continuous integration pipeline to avoid any regression that stems from modifications applied to your source code. Finally, provide enough documentation for future users. Once done it is common to publish your Python package on the Python Package Index (PyPI). Here we are going to see how to accomplish each of these steps using Poetry, Tox and GitHub Actions. The code used for our use case can be found on our repository. . This article is the last one from a series of three in which I share some best practices. . Part 1: project initialization with pyenv and poetry | Part 2: unit testing and commit enforcement | Part 3: CI pipeline with GitHub Actions and publication on PiPy | . Automate linter checks and tests with tox . If not done, activate your virtual environment. . poetry shell . To check the conformity of our code, we use a couple of packages that are going to evaluate if the code respects the common Python writing guidelines. Then, to automate their execution as well as our unit tests, we use tox. To install them run: . poetry add black flake8 pylint tox --dev . tox and poetry don’t work well together by default. They are somewhat redundant. To use them together, we need to implement a few tricks (see issues 1941 and 1745). tox install its own environment and dependencies to run its tasks. But to install dependencies, you have to declare the command poetry install in our tox configuration. This brings redundancy and can lead to some issues. Moreover, this does not allow to install developers dependencies needed to execute our tests. It is more productive to let tox use the poetry.lock file to install necessary dependencies. For this, I advise you to use the tox-poetry-installer package developed to solve these problems: . poetry add tox-poetry-installer[poetry] --dev . Now we declare our tox configuration in a tox.ini file whose content is: . [tox] envlist = py38 isolated_build = true [testenv] description = Linting, checking syntax and running tests require_locked_deps = true install_dev_deps = true commands = poetry run black summarize_dataframe/summarize_df.py poetry run flake8 summarize_dataframe/summarize_df.py poetry run pylint summarize_dataframe/summarize_df.py poetry run pytest -v . You can see two sections here: . [tox]: Define the global settings for your tox automation pipeline including the Python version of the test environments. | [testenv]: Define the test environments. In our case we have some extra-variables require_locked_deps and install_dev_deps that are brought by the tox-poetry-installer package. require_locked_deps is to choose whether or not you want tox to harness the poetry.lock file to install dependencies. install_dev_deps is to choose if tox installs the developer dependencies. | . Refer to the tox documentation to learn more about the configuration as well as the tox-poetry-installer documentation to learn more about it extra configuration. . Run the tox pipeline: . tox py38 run-test: commands[0] | poetry run black summarize_dataframe/summarize_df.py All done! ✨ 🍰 ✨ 1 file left unchanged. py38 run-test: commands[1] | poetry run flake8 summarize_dataframe/summarize_df.py py38 run-test: commands[2] | poetry run pylint summarize_dataframe/summarize_df.py ************* Module summarize_dataframe.summarize_df summarize_dataframe/summarize_df.py:1:0: C0114: Missing module docstring (missing-module-docstring) summarize_dataframe/summarize_df.py:4:0: C0103: Argument name &quot;df&quot; doesn&#39;t conform to snake_case naming style (invalid-name) summarize_dataframe/summarize_df.py:11:4: C0103: Argument name &quot;df&quot; doesn&#39;t conform to snake_case naming style (invalid-name) summarize_dataframe/summarize_df.py:23:4: C0103: Argument name &quot;df&quot; doesn&#39;t conform to snake_case naming style (invalid-name) summarize_dataframe/summarize_df.py:43:0: C0103: Argument name &quot;df&quot; doesn&#39;t conform to snake_case naming style (invalid-name) Your code has been rated at 7.62/10 (previous run: 7.62/10, +0.00) ERROR: InvocationError for command /home/fbraza/Documents/python_project/summarize_dataframe/.tox/py38/bin/poetry run pylint summarize_dataframe/summarize_df.py (exited with code 16) ________________________________________________________ summary ________________________________________________________________ ERROR: py38: commands failed . An error is raised because pylint shed light on some style inconsistencies. By default, tox quits if any warnings or errors occurred during the execution of the commands. The errors are by themselves quite explicit. After correcting them, run again the pipeline: . tox # shorten for brevety [...] py38 run-test: commands[0] | poetry run black summarize_dataframe/summarize_df.py All done! ✨ 🍰 ✨ 1 file left unchanged. py38 run-test: commands[1] | poetry run flake8 summarize_dataframe/summarize_df.py py38 run-test: commands[2] | poetry run pylint summarize_dataframe/summarize_df.py -- Your code has been rated at 10.00/10 (previous run: 10.00/10, +0.00) py38 run-test: commands[3] | poetry run pytest -v ================================================= test session starts ============================================================= platform linux -- Python 3.8.7, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 -- /home/fbraza/Documents/python_project/summarize_dataframe/.tox/py38/bin/python cachedir: .tox/py38/.pytest_cache rootdir: /home/fbraza/Documents/python_project/summarize_dataframe collected 2 items tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary PASSED [ 50%] tests/test_summarize_dataframe.py::TestDataSummary::test_display PASSED [100%] ================================================= 2 passed in 0.30s =============================================================== ______________________________________________________ summary ____________________________________________________________________ py38: commands succeeded congratulations :) . Perfect. The tox automation pipeline succeed locally. The next step start implements the CI pipeline with GitHub Actions. . Continuous Integration with GitHub Actions . GitHub Actions make it easy to automate all your software workflows. This service is event-driven meaning that a set of commands is triggered when a specific event occurs. Such events could be a commit pushed to the branch or a pull request. GitHub Actions are pretty convenient to run all needed tests against your code. . Most importantly, GitHub Actions provide the ability to test your Python package using several Python versions and on different operating systems (Linux, macOS and Windows). The only thing you need is an existing repository and a .github/workflows/&lt;file_name&gt;.yaml file: . mkdir -p .github/workflows touch .github/workflows/ci.yml . The content of the .github/workflows/ci.yml file is: . name: CI Pipeline for summarize_df on: - push - pull_request jobs: build: runs-on: $ strategy: matrix: platform: [ubuntu-latest, macos-latest, windows-latest] python-version: [3.7, 3.8, 3.9] steps: - uses: actions/checkout@v1 - name: Set up Python $ uses: actions/setup-python@v2 with: python-version: $ - name: Install dependencies run: | python -m pip install poetry poetry install - name: Test with tox run: poetry run tox . A few words about the different fields: . on: this field defines the type of event that is going to trigger the pipeline. | jobs: this field defines the multiple steps of your pipeline. They run in an instance of a virtual environment. | build: this is where all the magic happens: The strategy.matrix.platform field defines the different OS you want to use to test your package. Use templating to pass these values to the build.runs-on field ($). | The strategy.matrix.python-version field defines the different versions of Python you want to use to test your package. | The steps field permits you to specify which actions you use (steps.uses) and which command you want to run (steps.run) | . | . Before finishing, alter the tox.ini and pyporject.toml files accordingly. Initially we chose the 3.8 Python version for tox. But we want it to be compatible with 3.7 and 3.9. For the pyproject.toml file, choose a version expected to be compatible with your package. Here we choose to make our package compatible from 3.7.1 and above. Below are the changes added to our files: . # content of: tox.ini [tox] envlist = py37,py38,py39 isolated_build = true skip_missing_interpreters = true [...] . Having several python version defined in your tox.ini file causes issue with your local testing. Running the tox raises an error because of lacking python versions. If you still want to test you module locally just use the tox -e py command. . # content of: pyproject.toml [...] [tool.poetry.dependencies] python = &quot;^3.7.1&quot; [...] . When you modify the pyproject.toml file, always run the poetry update command that can check some unexpected incompatibilities between your dependencies and the version of Python you wish to use. . To finish, we are going to install a package, called tox-gh-actions, to run tox in parallel on GitHub while using several versions of Python: . poetry add tox-gh-actions --dev . The pipeline is ready. Add, commit and push your changes to see the pipeline running: . echo &quot;!.github/&quot; &gt;&gt; .gitignore git add .gitignore git commit -m &quot;build: update .gitignore to unmask .github/ folder&quot; git add pyproject.toml tox.ini poetry.lock `.github/workflows/ci.yml` git commit -m &quot;build: tox pipeline + github actions CI pipeline&quot; . Go to your GitHub repository and click on the Actions tab: You see all the previous and ongoing pipelines: Let’s click on the ongoing pipeline. The pipeline runs on each OS and for each Python version. Wait a couple of minutes to see the results: All the pipelines succeed! We are ready to publish our package on the PyPi registry. . Publish packages on PyPi with poetry . To make your package publishable, add some details in the [tool.poetry] section of your pyproject.toml file: . [tool.poetry] name = &quot;summarize_dataframe&quot; version = &quot;0.1.0&quot; description = &quot;A package to provide summary data about pandas DataFrame&quot; license = &quot;MIT&quot; authors = [&quot;fbraza &lt;fbraza@tutanota.com&gt;&quot;] keywords = [&quot;pandas&quot;, &quot;dataframe&quot;] readme = &quot;README.md&quot; homepage = &quot;https://github.com/fbraza/summarize_dataframe&quot; repository = &quot;https://github.com/fbraza/summarize_dataframe&quot; include = [&quot;CHANGELOG.md&quot;] [...] . All the variables here are quite explicit. These are metadata needed for the publication of the package. The include variable is interesting to add any files you want. In our case we are going to add a CHANGELOG.md file. Do you remember commitizen? If not please take the time to read our article on commitizen and conventional commits. Use the following command: . cz bump . It prints the semantic version from your pyproject.toml file and ask you to create a Git tag. The version will be updated based on your Git commit. Next we create the CHANGELOG.md: . cz changelog cat CHANGELOG.md ## Unreleased ## 0.1.0 (2021-04-28) ### Refactor - correct pylint warnings - split the function into two: one returning df other for output ### Feat - implementation of the summary function to summarize dataframe . Your CHANGELOG.md has been created based on the Git history you generated thanks to commitizen. Pretty neat isn’t it?! Once done let’s focus on publishing our package: . poetry build Building summarize_dataframe (0.1.0) - Building sdist - Built summarize_dataframe-0.1.0.tar.gz - Building wheel - Built summarize_dataframe-0.1.0-py3-none-any.whl . This creates a folder called dist where the built package is located. To test if everything works you can use pip: . Do this outside of your virtual environment to not pollute it. . pip install path/to/your/package/summarize_dataframe-0.1.0-py3-none-any.whl . Now we need to create an account on PyPi. Just enter the expected details, validate your email and execute: . poetry publish Username: *********** Password: *********** Publishing summarize_dataframe (0.1.0) to PyPI - Uploading summarize_dataframe-0.1.0-py3-none-any.whl 100% - Uploading summarize_dataframe-0.1.0.tar.gz 100% . The package is now online and shared with the community. . . Conclusion . tox provides a nice interface to automate all your unit tests and validation checks. The ecosystem around poetry is getting more mature and provides solutions to work with tox without too much hassle. Collectively, these two solutions permit to establish a very efficient and coherent CI pipeline. To run the pipeline and test your packages against different OS or versions of Python, you can leverage GitHub Actions as described above. . poetry was at the center of our approach. From the project initialization to its publication and going through the management of its packages and dependencies. poetry demonstrated its ease of use and efficacy that will definitely facilitate the life of developers, Data Scientists or Data Engineers who develop projects in Python. . Our articles describe a full setup that you can leverage to build your own Python project to respect good software engineering practices. . Cheat Sheet . tox . Run your tox pipeline . tox . | . poetry . Build your package . poetry build . | Publish your package . poetry publish . | . Acknowledgments . This article was first published in Adaltas blog and kindly reviewed by the CEO David Worms and one consultant Barthelemy NGOM. .",
            "url": "https://fbraza.github.io/BrazLog/python/devops/2021/06/29/modern-python-part3.html",
            "relUrl": "/python/devops/2021/06/29/modern-python-part3.html",
            "date": " • Jun 29, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Modern Python part 2 - write unit tests & enforce Git commit conventions",
            "content": "Good software engineering practices always bring a lot of long-term benefits. For example, writing unit tests permits you to maintain large codebases and ensures that a specific piece of your code behaves as expected. Writing consistent Git commits also enhance the collaboration between the project stakeholders. Well-crafted Git commit messages open the door to automatic versioning and generated change log files. Consequently, a lot of attempts are currently ongoing and applied to normalize the messages written in our Git commits. . In the first part of this serie, we setup, our project by installing different Python versions with pyenv, setting a local version of Python with pyenv, encapsulating it into a virtual environment with poetry. Here we show more precisely how to unit test your Python application and how to enforce and validate your Git commit messages. The source code associated with this article is published on GitHub. . This article is the second one from a series of three in which I share our best practices. . Part 1: project initialization with pyenv and poetry | Part 2: unit testing and commit enforcement | Part 3: CI pipeline with GitHub Actions and publication on PiPy | . Testing our code . The project is a simple python function that summarizes data present in a pandas DataFrame. The function outputs the number of rows and columns and the frequency of each data types present in the pandas DataFrame: . - Data Summary Values Number of rows 230 Number of columns 9 float64 3 int64 4 object 2 . Go to your project root directory and activate your virtual environment: . poetry shell . We add a couple of dependencies using poetry: . poetry add -D pynvim numpy pandas Using version ^0.4.3 for pynvim Using version ^1.20.2 for numpy Using version ^1.2.3 for pandas Updating dependencies Resolving dependencies... (1.4s) Writing lock file Package operations: 8 installs, 0 updates, 0 removals • Installing six (1.15.0) • Installing greenlet (1.0.0) • Installing msgpack (1.0.2) • Installing numpy (1.20.2) • Installing python-dateutil (2.8.1) • Installing pytz (2021.1) • Installing pandas (1.2.3) • Installing pynvim (0.4.3) . The -D flag indicates that the dependency only apply to development environments. . Note: I personally use NeoVim for coding that is why I need the pynvim package to support NeoVim python plugins. . Based on the expected output defined above, our program is made of three steps: . Getting the shape of the pandas DataFrame. | Getting the pandas dtypes frequency. | Concatenating the two results into a unified DataFrame that we will use to output the final result. | Once the final DataFrame is obtained we output the result as depicted above. In this regard our code scaffold could look as the following: . import pandas as pd def data_summary(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function defined to return a DataFrame containing details about the number of rows and columns and the column dtype frequency of the passed pandas DataFrame &quot;&quot;&quot; def _shape(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function defined to return a dataframe with details about the number of row and columns &quot;&quot;&quot; return None def _dtypes_freq(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function defined to return a dataframe with details about the pandas dtypes frequency &quot;&quot;&quot; return None return None def display_summary(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function define to print out the result of the data summary &quot;&quot;&quot; result_df = True message = &#39;- Data summary -&#39; print(message, result_df, sep=&#39; n&#39;) . Let’s now start writing our unit tests. We are going to use the unittest tool available with the Python standard library. You may remember in the previous article that pytest was defined as a developer dependency for testing. It is not an issue with pytest because it natively runs tests written with the unittest library. . Unit tests are single methods that unittest expects you to write inside Python classes. Choose a descriptive name for your test classes and methods. The name of your test methods should start with test_. Additionally, unittest uses a series of special assertion methods inherited from the unittest.TestCase class. In practice, a test should precisely cover one feature, be autonomous without requiring external cues, and should recreate the conditions of their success. . To recreate the necessary environment, setup code must be written. If this code happens to be redundant, implements a setUp() method, that will be executed before every single test. This is pretty convenient to re-use and re-organize your code. Depending on your use case you may have to perform systematic operations after the tests ran. For that, you may use the tearDown() method. . First you can read below the unit test we implemented for the data_summary() function: . import unittest import pandas as pd from summarize_dataframe.summarize_df import data_summary class TestDataSummary(unittest.TestCase): def setUp(self): # initialize dataframe to test df_data = [[1, &#39;a&#39;], [2, &#39;b&#39;], [3, &#39;c&#39;]] df_cols = [&#39;numbers&#39;, &#39;letters&#39;] self.df = pd.DataFrame(data=df_data, columns=df_cols) # initialize expected dataframe exp_col = [&#39;Values&#39;] exp_idx = [&#39;Number of rows&#39;, &#39;Number of columns&#39;, &#39;int64&#39;, &#39;object&#39;] exp_data = [[3], [2], [1], [1]] self.exp_df = pd.DataFrame(data=exp_data, columns=exp_col, index=exp_idx) def test_data_summary(self): expected_df = self.exp_df result_df = data_summary(self.df) self.assertTrue(expected_df.equals(result_df)) if __name__ == &#39;__main__&#39;: unittest.main() . The setUp() method initializes two distinct pandas DataFrame. self.exp_df is the resulting DataFrame we expect to get after calling the data_summary() function and self.df is the one used to test our functions. At the moment, tests are expected to fail. The logic has not been implemented. To test with poetry use the command: . poetry run pytest -v ============================================== test session starts ============================== platform linux -- Python 3.8.7, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 -- /home/fbraza/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8/bin/python cachedir: .pytest_cache rootdir: /home/fbraza/Documents/python_project/summarize_dataframe collected 1 item tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary FAILED [100%] =============================================== FAILURES ========================================= ___________________________________TestDataSummary.test_data_summary _____________________________ self = &lt;tests.test_summarize_dataframe.TestDataSummary testMethod=test_data_summary&gt; def test_data_summary(self): expected_df = self.exp_df result_df = data_summary(self.df) &gt; self.assertTrue(expected_df.equals(result_df)) E AssertionError: False is not true tests/test_summarize_dataframe.py:26: AssertionError ============================================== short test summary info ============================= FAILED tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary - AssertionError: False is not true ============================================== 1 failed in 0.32s =================================== . Using the -v flag returns a more verbose output for your test results. You can see that your tests are labeled according to the classes and functions names you gave (i.e., &lt;test_module.py&gt;::&lt;class&gt;::&lt;test_method&gt;). . The code is updated to conform with the unit tests: . import pandas as pd def data_summary(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to output details about the number of rows and columns and the column dtype frequency of the passed pandas DataFrame &quot;&quot;&quot; def _shape(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to return a dataframe with details about the number of row and columns &quot;&quot;&quot; row, col = df.shape return pd.DataFrame(data=[[row], [col]], columns=[&#39;Values&#39;], index=[&#39;Number of rows&#39;, &#39;Number of columns&#39;]) def _dtypes_freq(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to return a dataframe with details about the pandas dtypes frequency &quot;&quot;&quot; counter, types = {}, df.dtypes for dtype in types: tmp = str(dtype) if tmp in counter.keys(): counter[tmp] += 1 else: counter[tmp] = 1 values = [[value] for value in counter.values()] return pd.DataFrame(data=values, columns=[&#39;Values&#39;], index=list(counter.keys())) result_df = pd.concat([_shape(df), _dtypes_freq(df)]) return result_df def display_summary(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function define to print out the result of the data summary &quot;&quot;&quot; result_df = True message = &#39;- Data summary -&#39; print(message, result_df, sep=&#39; n&#39;) . Run our test again: . poetry run pytest -v =============================================== test session starts =============================================================== platform linux -- Python 3.8.7, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 -- /home/fbraza/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8/bin/python cachedir: .pytest_cache rootdir: /home/fbraza/Documents/python_project/summarize_dataframe collected 1 item tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary PASSED [100%] =============================================== 1 passed in 0.28s ================================================================= . One last thing here. In our tests, we did not test the actual output. Our module is designed to output a string representation of our DataFrame summary. There are solutions to achieve this goal with unittest. However we are going to use pytest for this test. Surprising isn’t it? As said before pytest interpolates very well with unittest and we are going to illustrate it now. Here the code for this test: . import unittest import pytest import pandas as pd from summarize_dataframe.summarize_df import data_summary, display_summary class TestDataSummary(unittest.TestCase): def setUp(self): # initialize dataframe to test df_data = [[1, &#39;a&#39;], [2, &#39;b&#39;], [3, &#39;c&#39;]] df_cols = [&#39;numbers&#39;, &#39;letters&#39;] self.df = pd.DataFrame(data=df_data, columns=df_cols) # initialize expected dataframe exp_col = [&#39;Values&#39;] exp_idx = [&#39;Number of rows&#39;, &#39;Number of columns&#39;, &#39;int64&#39;, &#39;object&#39;] exp_data = [[3], [2], [1], [1]] self.exp_df = pd.DataFrame(data=exp_data, columns=exp_col, index=exp_idx) @pytest.fixture(autouse=True) def _pass_fixture(self, capsys): self.capsys = capsys def test_data_summary(self): expected_df = self.exp_df result_df = data_summary(self.df) self.assertTrue(expected_df.equals(result_df)) def test_display(self): print(&#39;- Data summary -&#39;, self.exp_df, sep=&#39; n&#39;) expected_stdout = self.capsys.readouterr() display_summary(self.df) result_stdout = self.capsys.readouterr() self.assertEqual(expected_stdout, result_stdout) if __name__ == &#39;__main__&#39;: unittest.main() . Notice the decorator @pytest.fixture(autouse=True) and the function it encapsulates (_pass_fixture). In the unit test terminology, this method is called a fixture. Fixtures are functions (or methods if you use an OOP approach), which will run before each test to which it is applied. Fixtures are used to feed some data to the tests. They fill the same objective as the setUp() method we used before. Here we are using a predefined fixture called capsys to capture the standard output (stdout) and reuse it in our test. We can then modify our code display_summary() accordingly: . import pandas as pd def data_summary(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to output details about the number of rows and columns and the column dtype frequency of the passed pandas DataFrame &quot;&quot;&quot; def _shape(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to return a dataframe with details about the number of row and columns &quot;&quot;&quot; row, col = df.shape return pd.DataFrame(data=[[row], [col]], columns=[&#39;Values&#39;], index=[&#39;Number of rows&#39;, &#39;Number of columns&#39;]) def _dtypes_freq(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to return a dataframe with details about the pandas dtypes frequency &quot;&quot;&quot; counter, types = {}, df.dtypes for dtype in types: tmp = str(dtype) if tmp in counter.keys(): counter[tmp] += 1 else: counter[tmp] = 1 values = [[value] for value in counter.values()] return pd.DataFrame(data=values, columns=[&#39;Values&#39;], index=list(counter.keys())) result_df = pd.concat([_shape(df), _dtypes_freq(df)]) return result_df def display_summary(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function define to print out the result of the data summary &quot;&quot;&quot; result_df = data_summary(df) message = &#39;- Data summary -&#39; print(message, result_df, sep=&#39; n&#39;) . Then run the tests again: . poetry run pytest -v =============================================== test session starts =============================================================== platform linux -- Python 3.8.7, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 -- /home/fbraza/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8/bin/python cachedir: .pytest_cache rootdir: /home/fbraza/Documents/python_project/summarize_dataframe collected 2 items tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary PASSED [ 50%] tests/test_summarize_dataframe.py::TestDataSummary::test_display PASSED [100%] =============================================== 2 passed in 0.29s ================================================================= . The tests now succeed. It is time to commit and share our work, for example by publishing it to GitHub. Before that, let’s take a close look at how to properly communicate about our work with Git commit messages while respecting and enforcing a common standard. . Enforce Git commit messages rules in your Python project . Writing optimal Git commit messages is not an easy task. Messages need to be clear, readable, and understandable in the long term. The Conventional Commits specification proposes a set of rules for creating explicit commit histories. . Using commitizen . In our series about JavaScript monorepos, we saw how to integrate these conventions to enforce good practices regarding commit messages. Applied to Python, we are going to use a package called commitizen to achieve this. Let’s add this package to our developer dependencies: . poetry add -D commitizen Using version ^2.17.0 for commitizen Updating dependencies Resolving dependencies... (3.1s) Writing lock file Package operations: 11 installs, 0 updates, 0 removals • Installing markupsafe (1.1.1) • Installing prompt-toolkit (3.0.18) • Installing argcomplete (1.12.2) • Installing colorama (0.4.4) • Installing decli (0.5.2) • Installing jinja2 (2.11.3) • Installing pyyaml (5.4.1) • Installing questionary (1.6.0) • Installing termcolor (1.1.0) • Installing tomlkit (0.7.0) • Installing commitizen (2.17.0) . To setup commitizen for your project, run the command cz init. It prompts us with a set of questions: . cz init ? Please choose a supported config file: (default: pyproject.toml) (Use arrow keys) » pyproject.toml .cz.toml .cz.json cz.json .cz.yaml cz.yaml ? Please choose a cz (commit rule): (default: cz_conventional_commits) (Use arrow keys) » cz_conventional_commits cz_jira cz_customize ? Please enter the correct version format: (default: &quot;$version&quot;) ? Do you want to install pre-commit hook? (Y/n) . Choose all default choices here as they fit perfectly with our actual situation. The last question asks us if we want to use pre-commit hook. We are going to come back to this later on. So just answer no for now. If we look at our pyproject.toml file we can see that a new entry named [tool.commitizen] has been added: . [...] [tool.commitizen] name = &quot;cz_conventional_commits&quot; # commit rule chosen version = &quot;0.0.1&quot; tag_format = &quot;$version&quot; . To check your commit message, you can use the following command: . cz check -m &quot;all summarize_data tests now succeed&quot; commit validation: failed! please enter a commit message in the commitizen format. commit &quot;&quot;: &quot;all summarize_data tests now succeed&quot; pattern: (build|ci|docs|feat|fix|perf|refactor|style|test|chore|revert|bump)!?( ( S+ ))?:( s.*) . Our message is rejected because it does not respect the commit rules. The last line suggests some patterns to use. Take some time to read the conventional commits documentation and run the command cz info to print a short documentation: . cz info The commit contains the following structural elements, to communicate intent to the consumers of your library: fix: a commit of the type fix patches a bug in your codebase (this correlates with PATCH in semantic versioning). feat: a commit of the type feat introduces a new feature to the codebase (this correlates with MINOR in semantic versioning). BREAKING CHANGE: a commit that has the text BREAKING CHANGE: at the beginning of its optional body or footer section introduces a breaking API change (correlating with MAJOR in semantic versioning). A BREAKING CHANGE can be part of commits of any type. Others: commit types other than fix: and feat: are allowed, like chore:, docs:, style:, refactor:, perf:, test:, and others. [...] . This command guides you on how to write your commit message. Here the format should be &quot;[pattern]: [MESSAGE]&quot;. For us, this leads to: . cz check -m &quot;test: all summarize_data tests now succeed&quot; Commit validation: successful! . Very good, our commit message is valid. But hold on. Checking our messages each time with commitizen might be cumbersome and doesn’t provide the garanty to be applied. It would be better to check automatically the message each time we use the git commit command. That is where the pre-commit hook takes action. . Automatically enforce Git message conventions with pre-commit . Git hooks are useful to automate and perform some actions at specific place during the Git lifecycle. The pre-commit hook permits to run scripts before a Git commit is issued. We can use the hook to validate the commit messages and prevent Git from using a message which doesn’t match our expectations. The hook is active from the command line as well as from any tools interacting with the Git repository where the hook is registered, including your favoride IDE. . pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. If you want to know more about the inner workings and the spectrum of possibilities opened by the pre-commit hook, you can read its usage documentation. . To install pre-commit just run: . peotry add -D pre-commit . To automate the Git commit verification we first need to create a configuration file .pre-commit-config.yaml as followed: . repos: - repo: https://github.com/commitizen-tools/commitizen rev: master hooks: - id: commitizen stages: [commit-msg] . Next we can install the hook with its source defined in the repo property: . pre-commit install --hook-type commit-msg . Now that everything is set, we can use our Git hook: . git commit -m &quot;test: all summarize_data tests now succeed&quot; [INFO] Initializing environment for https://github.com/commitizen-tools/commitizen. [INFO] Installing environment for https://github.com/commitizen-tools/commitizen. [INFO] Once installed this environment will be reused. [INFO] This may take a few minutes... commitizen check.........................................................Passed [INFO] Restored changes from /home/fbraza/.cache/pre-commit/patch1617970841. [master 1e64d0a] test: all summarize_data tests now succeed 2 files changed, 48 insertions(+), 5 deletions(-) rewrite tests/test_summarize_dataframe.py (98%) . pre-commit installs an environment to run its checks. As you can see here the commit message assessment passed. To finish we can commit and push the modifications made on the build files (poetry.lock, pyproject.toml) and our module: . git commit -m &quot;build: add developer dependencies&quot; -m &quot;commitizen and pre-commit added to our dev dependencies&quot; commitizen check.........................................................Passed [master 1c6457c] build: add developer dependencies 2 files changed, 585 insertions(+), 1 deletion(-) git commit -m &quot;feat: implementation of the summary function to summarize dataframe&quot; commitizen check.........................................................Passed [master 5c053ad] build: add developer dependencies 1 file changed, 94 insertions(+) . We can now push everything to our GitHub repository: . git push origin master . Conclusion . We covered a few topics: . On the first hand, we saw how to write unit tests for your code. You shall always start to write tests before coding. It helps you affinate your API and expectations before implementing them. You will definitively benefit from it. We used unittest which is already available in the Python standard library. I actually like its simple design and object-oriented approach but others prefer using the pytest library which is definitively worth checking. One very convenient aspect is that pytest supports the unittest.TestCase class from the beginning. You can then write your tests with either of the two libraries or even mix both depending on your needs and have one common command to run them all. | We saw how to enforce good practices when writing Git commit messages. Our proposed solution relies on the use of two distinct Python packages: commitizen and pre-commit. The first one provides with the tools to check if a message validate the conventions you have chosen. The second one automates the process using a Git hook. | . In our next and last article, we are going to go one step further. We automate testing using tox and integrate it inside a CI/CD pipeline. Once done we will show how to prepare our package and finally publish it on PyPi using poetry. . Cheat sheet . poetry . Add project dependencies: . poetry add [package_name] . | Add developer dependencies: . poetry add -D [package_name] . poetry add --dev [package_name] . | Run test: . poetry run pytest . | . commitizen . Initialize commitizen: . cz init . | Check your commit: . cz check -m &quot;YOUR MESSAGE&quot; . | . pre-commit . Generate a default configuration file: . pre-commit sample-config . | Install git hook: . pre-commit install --hook-type [hook_name] . | . Acknowledgments . This article was first published in Adaltas blog and kindly reviewed by the CEO David Worms and one consultant Barthelemy NGOM. .",
            "url": "https://fbraza.github.io/BrazLog/python/devops/2021/06/24/modern-python-part2.html",
            "relUrl": "/python/devops/2021/06/24/modern-python-part2.html",
            "date": " • Jun 24, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Modern Python part 1 - start a project with pyenv & poetry",
            "content": "When learning a programming language, the focus is essentially on understanding the syntax, the code style, and the underlying concepts. With time, you become sufficiently comfortable with the language and you start writing programs solving new exciting problems. . However, when you need to move towards this step, there is an aspect that one might have underestimated which is how to build the right environment. An environment that enforces good software engineering practices, improves productivity and facilitates collaboration. Packaging and tooling with Python is often described as cumbersome and challenging. In this regard, several open-source projects emerged in the last years and aim at facilitating the management of Python packages along your working projects. We are going to see here how to use two of them: Pyenv, to manage and install different Python versions, and Poetry, to manage your packages and virtual environments. Combined or used individually, they help you to establish a productive environment. . This article is the first one from a series of three in which I share some best practices. . Part 1: project initialization with pyenv and poetry | Part 2: unit testing and commit enforcement | Part 3: CI pipeline with GitHub Actions and publication on PiPy | . Pre-requisites . pyenv installation . To install pyenv you require some OS-specific dependencies. These are needed as pyenv installs Python by building from source. For Ubuntu/Debian be sure to have the following packages installed: . sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl . To know the required dependencies on your OS go read this documentation. Once the dependencies are installed you can now install pyenv. For this, I recommend using pyenv-installer that automates the process. . curl https://pyenv.run | bash . From there on, you can install on your system any versions of Python you wish. You can use the following command to all versions and flavors of Python available: . pyenv install --list . In our case we are going to install the classical CPython in versions 3.7.10 , 3.8.7 , 3.9.2: . pyenv install 3.7.10 Downloading Python-3.7.10.tar.xz... -&gt; https://www.python.org/ftp/python/3.7.10/Python-3.7.10.tar.xz Installing Python-3.7.10... Installed Python-3.7.10 to /home/fbraza/.pyenv/versions/3.7.10 . Once the versions are installed you can see them by running: . pyenv versions * system 3.7.10 3.8.7 3.9.2 . You can see that pyenv identified recently installed Python versions and also the one installed by default on your system. The * before system means that the global version used now is the system version. pyenv permits to manage Python versions at different levels: globally and locally. Let’s say we are going to set version 3.7.10 as our global version. . pyenv global 3.7.10 . Let’s list our version again: . pyenv versions system * 3.7.10 (set by /home/&lt;username&gt;/.pyenv/version) 3.8.7 3.9.2 . You can see that pyenv sets 3.7.10 as our global Python version. This will not alter the operations that require the use of the system version. The path you can read between parenthesis corresponds to the path that points to the required Python version. How does this work? Briefly, pyenv captures Python commands using executables injected into your PATH. Then it determines which Python version you need to use, and passes the commands to the correct Python installation. Feel free to read the complete documentation to better understand the functionalities and possibilities offered by pyenv. . Note: Don’t be confused by the semantic here. Change the global version will not affect your system version. The system version corresponds to the version used by your OS to accomplish specific tasks or run background processes that depend on this specific Python version. Do not switch the system version to another one or you may face several issues with your OS! This version is usually updated along with your OS. The global version is just the version that pyenv will use to execute your Python commands / programs globally. . poetry installation . Poetry allows you to efficiently manage dependencies and packages in Python. It has a similar role as setup.py or pipenv, but offers more flexibility and functionalities. You can declare the libraries your project depends on in a pyproject.toml file. poetry will then install or update them on demand. Additionally this tools allows you to encapsulate your working project into isolated environments. Finally, you can use poetry to directly publish your package on Pypi. . As a last pre-requisite we are going to install poetry by running the following command: . curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - . Project creation . We are going to see how to create a project and isolate it inside a Python environment using pyenv and poetry. . Setting the Python version with Pyenv . Let’s first create a directory named my_awesome_project and move inside: . mkdir my_awesome_project &amp;&amp; cd $_ . Once inside, set the local Python version we are going to use (we are going to use Python 3.8.7). This will prompt poetry to use the local version of Python defined by pyenv: . pyenv local 3.8.7 . This creates a .python-version file inside our project. This file will be read by pyenv and prompts it to set the defined local Python version. Consequently every directory or file created down this point will depend on the local Python version and not the global one. . Create your project with poetry . Poetry proposes a robust CLI allowing you to create, configure and update your Python project and dependencies. To create your Python project use the following command: . poetry new &lt;project_name&gt; . This command generates a default project scaffold. The content of our new project is the following: . . ├── &lt;project_name&gt; │   └── __init__.py ├── pyproject.toml ├── README.rst └── tests ├── __init__.py └── test_summarize_dataframe.py . Notice the pyproject.toml. This is where we define everything from our project’s metadata, dependencies, scripts, and more. If you’re familiar with Node.js, consider the pyproject.toml as an equivalent of the Node.js package.json. . [tool.poetry] name = &quot;your_project_name&quot; version = &quot;0.1.0&quot; description = &quot;&quot; authors = [&quot;&lt;username&gt; &lt;email address&gt;&quot;] [tool.poetry.dependencies] python = &quot;^3.8&quot; [tool.poetry.dev-dependencies] pytest = &quot;^5.2&quot; [build-system] requires = [&quot;poetry-core&gt;=1.0.0&quot;] build-backend = &quot;poetry.core.masonry.api&quot; . We can see several entries in our defaultpyproject.toml file. . [tool.poetry]: This section contains metadata about our package. You can put there the package name, a short description, author’s details, the version of your project, and so on. All details here are optional but will be required if you decided to publish the package on Pypi. | [tool.poetry.dependencies]: This section contains all required dependencies for our package. You can specify specific version numbers for these packages (packageX = &quot;1.0.0&quot;) or use symbols. The version of Python we want the project to use is defined here as well. In our case python = &quot;^3.8&quot; specifies the minimum version required to run our app. Here this is Python 3.8 and this has been based on the version of our local version defined with pyenv. | [tool.poetry.dev-dependencies]: This section contains all developer dependencies which are packages needed to work and iterate on this project. Nevertheless, these dependencies are not required to run the app and will not be downloaded when building the package. | [build-system]: Do not touch this section unless you updated the version of poetry. | . Note: you can see the full list of available entries for the pyproject.toml file here . Install and activate the virtual environment . Here you have two approaches: whether you know in advance all dependencies you need and you can directly alter the .toml file accordingly or you decide to add later on when needed. In our example, we are going to add progressively our dependencies while writing code. Consequently, we just need to initialize the project and create the virtual environment. To do this run the command: . poetry install Creating virtualenv summarize-dataframe-SO-g_7pj-py3.8 in ~/.cache/pypoetry/virtualenvs Updating dependencies Resolving dependencies... (6.4s) Writing lock file Package operations: 8 installs, 0 updates, 0 removals • Installing pyparsing (2.4.7) • Installing attrs (20.3.0) • Installing more-itertools (8.7.0) • Installing packaging (20.9) • Installing pluggy (0.13.1) • Installing py (1.10.0) • Installing wcwidth (0.2.5) • Installing pytest (5.4.3) Installing the current project: summarize_dataframe (0.1.0) . Firstly the virtual environment is created and stored outside of the project. A bit similar to what we have when using conda. Indeed, Instead of creating a folder containing your dependency libraries (as virtualenv does), poetry creates an environment on a global system path (.cache/ by default). This separation of concerns allows keeping your project away from dependency source code. . Note: You can create your virtual environment inside your project or in any other directories. For that you need to edit the configuration of poetry. Follow this documentation for more details. . Secondly, poetry is going to read the pyproject.toml and install all dependencies specified in this file. If not defined, poetry will download the last version of the packages. At the end of the operation, a poetry.lock file is created. It contains all packages and their exact versions. Keep in mind that if a poetry.lock file is already present, the version numbers defined in it take precedence over what is defined in the pyproject.toml. Finally, you should commit the poetry.lock file to your project repository so that all collaborators working on the project use the same versions of dependencies. . Now let’s activate the environment we just created with the following command: . peotry shell Spawning shell within ~/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8 . ~/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8/bin/activate . The command creates a child process that inherits from the parent Shell but will not alter its environment. It encapsulates and restrict any modifications you will perform to your project environment. . Create our git repository . For our last step here we are going to create a git repository, add README.md and .gitignore files and push everything to our remote repository. . git init git remote add origin https://github.com/fbraza/summarize_dataframe.git echo &quot;.* n!.gitignore&quot; &gt; .gitignore echo &quot;# Summarize dataframe&quot; &gt; README.md git add . git commit -m &quot;build: first commit. Environment built&quot; git push -u origin master . Conclusion . Herein we have seen how to install and manage different versions of Python on our machine using pyenv. We demonstrated how to leverage pyenv local to set a specific Python version in your project and then create a virtual environment using poetry. The use of poetry really smoothens the process of creation by proposing a simple and widely project scaffold. In addition, it includes the minimum build system requirements as defined by PEP 518. . In our next article, we are going to dive more into our project. We will write some code with their respective unit tests and see how we can use poetry to add the expected dependencies and run the tests. Finally, we are going to go a bit further and install all necessary dependencies with poetry to help us enforcing good practices with our git commits when using a Python project. . Cheat sheet . pyenv . Get all available and installable versions of Python . pyenv install --list . | Set the global Python version . pyenv global &lt;version_id&gt; . | Set a local Python version . pyenv local &lt;version_id&gt; . | . poetry . Create a project . poetry new &lt;project_name&gt; . | Install core dependencies and create environment . poetry install . | Activate environment . poetry shell . | . Acknowledgments . This article was first published in Adaltas blog and kindly reviewed by the CEO David Worms and one consultant Barthelemy NGOM. .",
            "url": "https://fbraza.github.io/BrazLog/python/devops/2021/06/09/modern-python-part1.html",
            "relUrl": "/python/devops/2021/06/09/modern-python-part1.html",
            "date": " • Jun 9, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Faouzi is French. He lived in France, Germany, Portugal and now in Belgium where he works full-time as a Data Engineer consultant at Dataroots . He previously worked during 8 years as a scientist in biology and got hooked by the Bioinformaticians’ black magic! He decided to dive more into the IT and start learning how to program for data 2 years and a half ago. In the process, he got graduated in Data engineering from the Data ScienceTech Institute. . He worked as an intern in Altran for a mission of 8 weeks focused on the development of a computer vision system for the Portuguese national electricity company. The he worked 5 months at Adaltas where he improved his skills in infrastructure with notably the deployment of Hadoop clusters. . He likes working in the Python ecosystem. He has a focus on Data Engineering but likes to touch Data Analysis and Machine Learning. He loves programming languages and is attempting to work his way through the JavaScript ecosystem. . Contact . Gmail: faouzi.brazza@gmail.com | Github: fbraza | LinkedIn: Faouzi Braza | .",
          "url": "https://fbraza.github.io/BrazLog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fbraza.github.io/BrazLog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}