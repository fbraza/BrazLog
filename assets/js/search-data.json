{
  
    
        "post0": {
            "title": "Use Dagster for Reproducible Single cell Analysis Pipelines",
            "content": "Dagster blablabla … Single cell blablabla … data pipeline Dagster would be extremely relevant for local development of analytic pipeline in the field here we are going to through an example blablabla . Load the data with python only . I am not going to contextualize the data here. Just know that the single cell sequencing data come from cancer cells that were subjected to different treatments. The sequencing data was mapped against the human genome and saved as h5ad files. It is a file format that stores AnnData object, the fundamental building block of the Scanpy library. We are going to use Scanpy to read this object. . If we had to do it using classical python code we could propose the following: . import scanpy as sc import pandas as pd def get_count_matrix_from_h5ad(path: str) -&gt; pd.DataFrame: &quot;&quot;&quot;Extract the count matrix from an anndata object&quot;&quot;&quot; anndata = sc.read_h5ad(path) return anndata.to_df() def get_metadata_from_h5ad(path: str) -&gt; pd.DataFrame: &quot;&quot;&quot;Extract the metadata from an anndata object present in `anndata.obs`&quot;&quot;&quot; anndata = sc.read_h5ad(path) return anndata.obs if __name__ == &quot;__main__&quot;: PATH = &quot;data/raw/h5ad/RGR/KJF_RGR.h5ad&quot; print( get_count_matrix_from_h5ad(PATH), get_metadata_from_h5ad(PATH) ) . Now we could just execute our code: . python path/to/load.py . to get the following output: . 0 1 2 3 4 5 6 7 8 9 10 11 ... 20009 20010 20011 20012 20013 20014 20015 20016 20017 20018 20019 20020 hn017_X068_NT_HN017 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 hn017_X068_NT_HN017.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 hn017_X068_NT_HN017.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 hn017_X068_NT_HN017.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 hn017_X068_NT_HN017.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... hn031_X158_PT_HN031.187 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 hn031_X158_PT_HN031.188 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 hn031_X158_PT_HN031.189 0.0 0.0 0.0 0.0 0.0 0.0 1.0 6.003666 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 hn031_X158_PT_HN031.190 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.000244 0.0 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 hn031_X158_PT_HN031.191 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 [3596 rows x 20021 columns] orig.ident nCount_RNA nFeature_RNA batch sample treatment OG hn017_X068_NT_HN017 X068 773.430000 477 68 HN017 SMCs X068_NT_HN017 hn017_X068_NT_HN017.1 X068 438.140000 314 68 HN017 SMCs X068_NT_HN017.1 hn017_X068_NT_HN017.2 X068 886.110000 498 68 HN017 SMCs X068_NT_HN017.2 hn017_X068_NT_HN017.3 X068 402.250000 220 68 HN017 SMCs X068_NT_HN017.3 hn017_X068_NT_HN017.4 X068 345.220000 217 68 HN017 SMCs X068_NT_HN017.4 ... ... ... ... ... ... ... ... hn031_X158_PT_HN031.187 X158 9969.478365 2797 158 HN031 FMCs X158_PT_HN031.187 hn031_X158_PT_HN031.188 X158 11465.470291 3289 158 HN031 FMCs X158_PT_HN031.188 hn031_X158_PT_HN031.189 X158 25741.699607 4917 158 HN031 FMCs X158_PT_HN031.189 hn031_X158_PT_HN031.190 X158 42022.885436 6369 158 HN031 FMCs X158_PT_HN031.190 hn031_X158_PT_HN031.191 X158 998.665076 469 158 HN031 FMCs X158_PT_HN031.191 [3596 rows x 7 columns] . The print statement was used as an example here. The idea behind a data engineering pipeline is to build a “graph of functions that ingest and produce data assets“1. So here the output generated by our two functions will be used as input data for subsequent processing and analytic tasks that remain to be defined. . We can refactor our code to make it more modular. . import scprep as sp import scanpy as sc import pandas as pd def load_anndata(path: str) -&gt; sc.AnnData: &quot;&quot;&quot;Load an AnnData object from a h5ad file&quot;&quot;&quot; return sc.read_h5ad(path) def get_counts(anndata: sc.AnnData) -&gt; pd.DataFrame: &quot;&quot;&quot;Return a count matrix from an AnnData object&quot;&quot;&quot; return anndata.to_df() def get_metadata(anndata: sc.AnnData) -&gt; pd.DataFrame: &quot;&quot;&quot;Return the metadata from an AnnData object&quot;&quot;&quot; return anndata.obs if __name__ == &quot;__main__&quot;: PATH = &quot;data/raw/h5ad/RGR/KJF_RGR.h5ad&quot; anndata = load_anndata(PATH) print( get_counts(anndata=anndata), get_metadata(anndata=anndata) ) . We extracted the logic implemented to load the AnnData object in its own function, load_anndata. Its returned value feeds the other two functions get_counts &amp; get_metadata that respectively return pandas DataFrame for the counts and metadata. If we had to depict the data flow as a graph we would have the following: . . A couple of remark at this step: . The path is hardcoded. | We could implement a CLI application that parse the argument and get the path from there. | We could save the path value in a yaml / json / .env and modify it based on our needs. | Both imply to write extra code. But this is doable. | As it is implemented now, we have no logs of our runs and no historicall records of the different runs if any. | . Load the data with python and dagster . Let’s see how to translate this code to Dagster. We are going to approach this Top-Down. First the code and next we will elaborate on some concepts. Show me the code ! . from dagster import Output, op, Output, Out, job, fs_io_manager import scanpy as sc import pandas as pd @op(config_schema={&quot;path_to_h5ad&quot;: str}, out=Out(pd.DataFrame)) def load_anndata(context) -&gt; sc.AnnData: return sc.read_h5ad(context.op_config[&quot;path_to_h5ad&quot;]) @op(out=Out(pd.DataFrame)) def get_count_matrix_from_h5ad(context, anndata: sc.AnnData) -&gt; pd.DataFrame: &quot;&quot;&quot;Extract the count matrix from an anndata object&quot;&quot;&quot; counts = anndata.to_df() yield Output( counts, metadata={ &quot;whoiam&quot;: &quot;Metadata from the RGR dataset&quot;, &quot;# of cells&quot;: counts.shape[0], &quot;# of genes&quot;: counts.shape[1] } ) @op(out=Out(pd.DataFrame)) def get_metadata_from_h5ad(context, anndata: sc.AnnData) -&gt; pd.DataFrame: &quot;&quot;&quot;Extract the metadata from an anndata object present in `anndata.obs`&quot;&quot;&quot; sc_metadata = anndata.obs yield Output( sc_metadata, metadata={ &quot;whoiam&quot;: &quot;Metadata from the RGR dataset&quot;, &quot;columns&quot;: str(sc_metadata.columns), &quot;# of cells&quot;: sc_metadata.shape[0], &quot;# of variables&quot;: sc_metadata.shape[1] } ) @job(resource_defs={&quot;io_manager&quot;: fs_io_manager}) def from_anndata_to_pandas_job(): &quot;&quot;&quot;Load anndata and extract counts matrix and metadata&quot;&quot;&quot; anndata = load_anndata() get_count_matrix_from_h5ad(anndata) get_metadata_from_h5ad(anndata) .",
            "url": "https://fbraza.github.io/BrazLog/python/single%20cell/data%20engineering/2022/04/12/dagster-single-cells.html",
            "relUrl": "/python/single%20cell/data%20engineering/2022/04/12/dagster-single-cells.html",
            "date": " • Apr 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "A Shiny App for Microbiota Analysis",
            "content": "Topics . Basics of shiny | Reactivity | using modules in shiny | helpers_functions and helpers_modules | bslib for UI | Update select input (observe({})) | switchButton() of absolute values | req() | save plotly as html documents / downloadHandler | read from two files in shiny | make a package devtools::documents or roxygen2::roxygenize() | Docker deployment | .",
            "url": "https://fbraza.github.io/BrazLog/r/shiny/docker/2022/02/25/shiny.html",
            "relUrl": "/r/shiny/docker/2022/02/25/shiny.html",
            "date": " • Feb 25, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Installation of Scanpy on a Mac M1",
            "content": "Here a short article to provide some reminders about how to install Scanpy on your Mac M1. For managing my environments I use poetry, conda or pipenv. Unfortunately I did not manage to install Scanpy via pip depedent package managers (both pipenv or poetry do not work). The only alternative that worked wasconda. But to be operational, you first need to install and configure some dependencies. . HDF5 . HDF5 is a high performance data software library and file format to manage, process, and store your heterogeneous data. HDF5 is built for fast I/O processing and storage. Scanpy can read and store AnnData object as h5ad files that are hdf5 files with some additional structure specifying how to store AnnData objects. To install it you will need homebrew on your Mac M1 machine. . brew install hdf5 export HDF5_DIR=/opt/homebrew/Cellar/hdf5/1.12.1 # use the version you have . LLVMLITE . Next install llvmlite with homebrew. llvmlite provides a Python binding to LLVM for use in Numba that translates a subset of Python and NumPy code into fast machine code. It is extensively used in Scanpy given its dependency to numpy and scikit-learn. . brew install llvm@11 . Make sure /opt/homebrew/opt/llvm@11/bin is in your path. For that edit the /etc/paths to add this path. Next install llvmlite in your python environment. . Conda . I recommend installing Miniconda. Miniconda is essentially an installer for an empty conda environment, containing only Conda, its dependencies, and Python. . conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda config --set offline false . Once done, you can install scanpy in your machine. . conda install scanpy . Conclusion . It was a short one, but it will be, for sure, a good reminder for my future scanpy projects when working in a Mac M1. I hope that in the future these issues will be solved. In the mean time, I still use my Linux computer when I need to deal with Scanpy. There, everything works out of the box with classical pip install. .",
            "url": "https://fbraza.github.io/BrazLog/python/devops/single-cell/2022/02/10/install-scanpy.html",
            "relUrl": "/python/devops/single-cell/2022/02/10/install-scanpy.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Own your metadata with metadata hubs",
            "content": "Let’s imagine you are a freshly recruited Data Scientist. During one of your stand-up, your product owner gives you a couple of tasks that implies interacting with several data sources. You quickly realize that your main challenge here, even before fulfilling your task will be first to know your data: . Where is the data located? Cloud? On-premise? | Where is it stored? In a data lake? A relational database? | What is the schema of the data assets you need to use? | Do you have permissions to access it? If not, who is the owner of the data? | Are there any sensitive data that you should be aware of? | How can you know the history of the data? Is the data a result of several processing steps? in other words what is its lineage? | . We could continue here the list of questions one might ask when dealing with data assets inside a company. To be able to answer these, you could ask your colleagues. While this could work in a small working environment dealing with small amount, this approach is challenging in bigger companies that manage data at big scale. . To solve this particular problem and give more transparency to the way data flows inside a company, several tools propose to collect data about your data assets and contextualize it. These tools are called metadata hubs and provide ways to implement: . A data catalog to organize data assets inside companies to facilitate the discovery, management, and tracking of the data. | A data lineage framework to trace and track how data flows around your system. | A data quality system to define rules that assess and evaluate the accuracy of your data. Often organizations inaccuracy in your data is identified too late and this might cause issues. | . Here we will present you the different metadata hubs solutions available in the open-source ecosystem. We will see the type of architecture they rely on and what specific problems they aim to solve by highlighting their strengths and weaknesses. . The metadata hubs ecosystem . Metadata hub is not a new concept. But it attracted all attention in the last decade. Some well-established companies propose paid solutions to collect and process your metadata. For example, Alation, Collibra, and Data.World proposed software as a service solutions with advanced data catalog and lineage features. Cloud providers started to include hubs that process metadata from their services in their offer. Azure has Azure Purview, Google Cloud has Data Catalog and, Amazon has the AWS Glue data catalog. The open-source community witnessed the emergence of promising tools: Amundsen, Marquez, Apache Atlas, and LinkedIn’s DataHub. . Alternatives are not lacking. But which one should you choose? Which one should you implement to fit your needs and expectations in terms of data governance? There is one fundamental technical criterion to guide you in your decision: the technical architecture the solution is built upon. An article published by LinkedIn described three types of metadata hubs architectures. Here, we will go through each of them and discuss their advantages and trade-offs. For each architecture, we will report our own experience with some of the open-source tools available. . The type 1 architecture: a monolithic approach . . Figure 1: Type 1 architecture: reproduced and modified from here . The first-generation metadata hub consists of a monolithic application connected to a primary metadata store. Sometimes the metadata stored in the primary store is duplicated to be used as a search index. In practice, metadata is imported from your different data sources or ETL logs using an orchestrator such as Airflow. . This type of metadata hub is easy to set up and maintain. However, it provides limited features (mainly data cataloging) without programmatic access. Additionally, extra care is required to avoid any resource overflow of your own data infrastructure (loading data in batch cost resources and can seriously slow down other processes). Finally, periodic batch loads of your metadata cannot guarantee an always consistent metadata. . Amundsen . The open-source data catalog Amundsen is a typical example of a type-1 architecture. It uses the databuilder library to ingest, transform metadata from several sources and load it into a metadata store. Different choices can be made here for the storage solution: Neo4j, Apache Atlas, AWS Neptune, or Mysql. The main functionality of Amundsen is data cataloging coupled with an ergonomic UI enabling the user to search for descriptions of tables and columns, table usage frequency, or table statistics. No functionality related to data lineage of quality is available in this solution. . Despite its limited features, Admunsen remains a solid choice for data cataloging your data assets. Indeed, if your company remains small and you want to provide an easy interface for your Data Analyst and Scientist to search and track specific data assets, go for it. To see a more practical demonstration of this tool, you can watch our related Tour de Tools. . The type 2 architecture: a programmatic approach . . Figure 2: Type 2 architecture: reproduced and modified from here . One limitation with the type-1 architecture is the impossibility to enable programmatic use cases (no access to the RestAPI). Type-2 metadata hubs solve this specific problem. The monolithic application is composed of three main pieces: . the metadata store | the metadata service, a Rest-API that process HTTP requests | the front-end application that serves as a graphical interface to send requests to the metadata service | . The Rest-API enables the data engineer to own his metadata. Indeed, by capitalizing on the API, one can build several connectors and integrators libraries that would automatically scrape database and orchestrator metadata during data processing. By doing so, such a solution can leverage the metadata to build an advanced lineage graph that traces the history of your data assets. . One limitation of this architecture is the lack of a changelog repository that captures all metadata changes and updates your search indexes accordingly. Some solutions do propose an approach to version your jobs but, it does not store the full history of metadata changes and has no update process to keep it always fresh. . Finally, keep in mind that such a system might not scale well, especially if you are dealing with real (and we insist on this) big data. By not being distributed you might face some scalability and reliability issues with such solutions. . Marquez . . Figure 3: Marquez architecture and API . The open-source solution Marquez is a metadata hub built following the type-2 design. The metadata service is a Java server that process API HTTP requests coming from three different endpoints: . Core: for reading and writing processes. | Search: for (and as its name states) searching data. | Lineage: for extracting lineage data from several data sources and ETL tools. | . Integrators are libraries coded to extract metadata from different sources, including Airflow, Spark, and dbt. API calls are formatted using the OpenLineage standard. With Airflow, you can collect job and datasets metadata from PostgreSQL, Google BigQuery, AWS Redshift, Snowflake, and Great Expectations. . The graphical interface of Marquez fetches metadata and outputs it in the form of tables and lineage graphs. However, the UI capability is pretty limited compared to a tool like Collibra, another type-2 metadata hub. Marquez does not provide a graphical interface to write or update metadata. Instead, Marquez is designed for “coding people” and might refrain business stakeholders. . To conclude, we think that Marquez is an interesting tool to start mastering and owning your metadata to implement a data governance strategy. Indeed, Marquez provides a data catalog, advanced data lineage, and security features in one place. Noteworthy, Marquez revealed to be simple to set up and use during our investigations. The documentation is thorough. The community behind it is active and responsive on their slack channel. To learn more about Marquez, we recommend you to watch our RootsConf presentation about metadata hubs. . The type 3 architecture: a distributed approach . . Figure 4: Type 3 architecture: reproduced and modified from here . The third-generation tool relies on a distributed architecture. It provides several ways to lift metadata from services and data sources: . Using a pull approach with a metadata ingestion library (not shown in the schema above) | Using a push approach with Kafka streams or a Rest-API | . One major advantage of theses systems is their ability to capture and process metadata changes. Any changes made to your data sources or jobs generate some metadata change logs. These are pushed/committed into the appropriate metadata services to be processed. Changes are then applied to the appropriate metadata stores or indexes. The ability to process and update changes is the main evolution provided by this type of metadata hub. . Do not forget that complexity comes as the main trade-off here. Such solutions require a lot of engineering and maintenance power to make them work in your company. . Apache Atlas . . Figure 5: Apache Atlas architecture: reproduced and modified from here . Apache Atlas is a data Governance and metadata framework for Hadoop. It is tightly coupled with the Hadoop ecosystem. Noteworthy, Purview is built on top of Apache Atlas and provides metadata governance and management capabilities with Azure data services. . Metadata is sent from different sources using Kafka messaging or REST-API calls. The ingest / export component consumes metadata pushed by Kafka and captures change events. These are then exported and committed to the right store or service. . To manage your metadata with Apache Atlas, you need to comply with its type system. This system is flexible but complex to handle. By default, Hadoop components are modeled accordingly but, you can define your types to add custom resources if needed. However, understanding the concepts of the type system component is not an easy task and, the learning curve might be stiff. Consequently, it may take a long time to harness the power of Apache Atlas. . The metadata objects (which are Java objects) are stored internally using Janus Graph. It enables efficient management of relationships between the metadata objects and works as an interface to translate between types and entities of the Atlas type system. The graph component permits holding lineage metadata information. . Two main applications consume metadata processed by Atlas: . Atlas admin UI to discover and annotate metadata with business knowledge. It provides a search interface and, SQL is used to query the metadata types and objects. The UI is quite complete but can be overwhelming for new users. ING Bank recently discussed how they modernized their Apache Atlas setup by combining it with Amundsen. It notably abstracts away Atlas’ complexity to facilitate data discovery and management by their Data Analysts and Scientists. | Apache Ranger to define metadata-driven security and access permissions to protect your data assets. | . Our investigations revealed that Apache Atlas was quite complex to apprehend. You will need a subsequent amount of time to handle the type system properly. Moreover, although you can spin up all Atlas components using a docker image, you cannot do much with it unless you have a Hadoop cluster around. Some custom connectors or integrators for other data sources are available in the open-source system nut remains scare. As such, you will need to invest a lot of time to integrate your non-Hadoop data sources. . In conclusion, we think that Apache Atlas is worth investing in if your company heavily relies upon Hadoop services and applications and under the condition you have the engineering power to manage and maintain its infrastructure. To learn more about Apache and its type system, we recommend you to watch our RootsConf presentation about metadata hubs. . LinkedIn DataHub . . Figure 6: DataHub architecture, reproduced and modified from here . DataHub is LinkedIn’s answer to provide a scalable and always consistent metadata hub. It harbors a flexible architecture to ingest metadata following two main approaches: . A pull approach with an ingestion system that collects metadata from several sources. The metadata is pushed via Kafka or HTTP calls to the appropriate service or storage. You can use Airflow to orchestrate the ingestion pipelines. | A push approach using direct API HTTP calls or Kafka streams. Metadata pushed by Kafka are processed by a consumer (mce-consumer-job). Its role is to convert new metadata or metadata changes into the expected Pegasus type to make it ingestable and usable by the different DataHub services (Pegasus is a typed Data Schema language developed by LinkedIn). | . Several modulable DataHub services consume the formatted metadata: . The datahub-gms service exposes a REST API for performing read/write operations on the metadata store. Different solutions are possible to store metadata. You can use document, traditional relational, or key-value databases to store your metadata. Once new metadata or metadata changes are successfully stored, the service emits a commit stream using Kafka. Next, a change stream processor consumes the commit stream and applies changes to the search and graph indexes. This design provides a near-real-time indexes update. | The DataHub front-end service provides a complete set of features to interact with your metadata. You have access to advanced search capabilities using full-text search, logical operators, or regex. Each data asset has its profile page with all its metadata. Finally, you can also edit or update metadata through the UI. | . The whole architecture is pretty complex and full of technical details. If you want to know every technicality, read the documentation and this article published by LinkedIn. . Jumping immediately into such architecture might be counterproductive. DataHub is complex to apprehend as it provides an architecture with several layers and modules. You have several options to deploy it with Docker &amp; Kubernetes and, the documentation describes a step-by-step guide to deploy to Google Cloud or Amazon Web Services. Nevertheless, not every company is LinkedIn and, we can see that DataHub was developed with their use case in mind. . If you decide to migrate to DataHub, we advise you to spend a substantial amount of time prototyping and testing. It will allow you to apprehend and master DataHub’s complexity. And for that, you need time, money, and advanced engineering skills. . Conclusion and perspectives . Data governance is a hot topic. Companies need ways to govern and own their metadata and, metadata hubs are the technical answer to fill this goal. We saw that different solutions exist and, each supports different use-cases. If your data architecture is not at a big scale, and you want to use open-source tools, make the transition progressively. Type-1 and type-2 tools are pretty easy to use and implement and should be a good starting point. If you don’t want to implement it yourself, you should look at the paid alternatives. The caveats with paid solutions are the potent lack of connectors or integrators for your preferred data sources or data processing tool. . The metadata hub ecosystem is dynamic but remains fragmented. Each solution comes with its way of processing and modeling metadata. The ING bank use case illustrates this aspect very well. To integrate Amundsen with Apache Atlas, they needed to convert Altas’ typed metadata objects into Amundsen metadata models. Imagine now that you want to use Marquez for its integrated data lineage and quality features in combination with the data cataloging features of Amundsen. Again, the metadata extracted with Marquez needs to comply with the Amundsen model. Such proprietary formats significantly reduce the value of data and its flexibility. In response to this issue, two open-source initiatives have emerged: OpenLineage and OpenMetadata. Both solutions promote a standard format for metadata so that metadata remains solution-neutral and harmonized. With these standards, engineers will be able to implement and deploy very customizable metadata hubs using already available tools making the technical transition easier. . References . Amundsen | Overview Marquez | DataHub: A generalized metadata search and discovery tool, LinkedIn Engineering | DataHub: Popular metadata architectures explained, LinkedIn Engineering | Apache Atlas – Data Governance and Metadata framework for Hadoop | Apache Atlas and Amundsen at ING Bank | Data Governance: why and how ? - YouTube | Data Governance: metadata-hubs to the rescue! - YouTube | Tour de Tools #2 - guild.ai and Amundsen by Lyft - YouTube | . Acknowledgments . I originally published this at dataroots in collaboration with Xuyao Zhang, Bram Vandendriessche. .",
            "url": "https://fbraza.github.io/BrazLog/bigdata/ops/2021/12/05/Own-your-metadata-with-metadata-hubs.html",
            "relUrl": "/bigdata/ops/2021/12/05/Own-your-metadata-with-metadata-hubs.html",
            "date": " • Dec 5, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Some notes about React",
            "content": "With JavaScript you can manupulate the DOM (Document Object Model). The DOM is how data is represented in a webpage. It is represented as a hierarchical tree with leaves / nodes that are elements of the web document. . JavaScipt sees the DOM as an object that holds all the data. . Manipulating the DOM with JavaScript . We can use JavaScript code to manipulate the DOM namely create or modify specific elements. Read this snippet of HTML: . &lt;html&gt; &lt;header&gt; &lt;/header&gt; &lt;body&gt; &lt;script type=&quot;module&quot;&gt; const rootElement = document.createElement(&#39;div&#39;) rootElement.setAttribute(&#39;id&#39;, &#39;root&#39;) document.body.append(rootElement) const divElement = document.createElement(&#39;div&#39;) divElement.textContent = &#39;Hello World&#39; divElement.className = &#39;container&#39; rootElement.append(divElement) &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; . Focus on what is happening in the script element: . const rootElement = document.createElement(&#39;div&#39;) // 1 rootElement.setAttribute(&#39;id&#39;, &#39;root&#39;) // 2 document.body.append(rootElement) // 3 const divElement = document.createElement(&#39;div&#39;) // 4 divElement.textContent = &#39;Hello World&#39; // 5 divElement.className = &#39;container&#39; // 6 rootElement.append(divElement) // 7 . Let’s break down what is happening here: . We create an div element | We add it an attribute id with value root | We append this div element to the &lt;body&gt;&lt;/body&gt;. Notice how we access the body sarting from document | Next, we create a new div element | We add it some text content | We define its class name | We append the the &lt;div id=root&gt; with the second div | Manipulating the DOM with React . React is the most widely used frontend framework in the world and it’s using the same APIs that we’re using when it creates DOM nodes. React abstracts away the imperative browser API from you to give you a much more declarative API to work with. With that in mind, you need two JavaScript files to write React applications for the web: . React: responsible for creating React elements (kinda like document.createElement()) | ReactDOM: responsible for rendering React elements to the DOM (kinda like rootElement.append()) | . const divElement = &#39;div&#39; const divContainerProps = {className: &#39;container&#39; children: &#39;Hello World&#39;} const reactDivContainer = React.createElement(divElement, divContainerProps) ReactDOM.render(reactDivContainer, document.getElementById(&#39;root&#39;)) . The difference is quite srtiking here. We are using raw React and no JSX. Let’s break down the code: . divElement is a variable that holds a string describing the element we want | divContainerProps is a variable that holds an object with properties as field and their respective values | React.createElement() create the element using both variables | Then we append our new div to the div id=&#39;root&#39; element | Note that if you want to make quick test with an npm library you can use UNPKG. This is a tool to use everything that is on npm by just putting an URL in a script tag. Once your page is loaded with these scripts you have access in the web console to some global variables. For react these are React and ReactDOM. . Run the following command in the browser console: . React . This should return: . Object { Fragment: Symbol(&quot;react.fragment&quot;), StrictMode: Symbol(&quot;react.strict_mode&quot;), Profiler: Symbol(&quot;react.profiler&quot;), Suspense: Symbol(&quot;react.suspense&quot;), Children: {…}, Component: Component(props, context, updater), PureComponent: PureComponent(props, context, updater), __SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED: {…}, cloneElement: cloneElementWithValidation(element, props, children), createContext: createContext(defaultValue, calculateChangedBits) , … }​ Children: Object { map: mapChildren(children, func, context), forEach: forEachChildren(children, forEachFunc, forEachContext), count: countChildren(children) , … } Component: function Component(props, context, updater) Fragment: Symbol(&quot;react.fragment&quot;) Profiler: Symbol(&quot;react.profiler&quot;) PureComponent: function PureComponent(props, context, updater) StrictMode: Symbol(&quot;react.strict_mode&quot;) Suspense: Symbol(&quot;react.suspense&quot;) __SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED: Object { ReactCurrentDispatcher: {…}, ReactCurrentOwner: {…}, IsSomeRendererActing: {…}, … } cloneElement: function cloneElementWithValidation(element, props, children) createContext: function createContext(defaultValue, calculateChangedBits) createElement: function createElementWithValidation(type, props, children) createFactory: function createFactoryWithValidation(type) createRef: function createRef() forwardRef: function forwardRef(render) isValidElement: function isValidElement(object) lazy: function lazy(ctor) memo: function memo(type, compare) useCallback: function useCallback(callback, deps) useContext: function useContext(Context, unstable_observedBits) useDebugValue: function useDebugValue(value, formatterFn) useEffect: function useEffect(create, deps) useImperativeHandle: function useImperativeHandle(ref, create, deps) useLayoutEffect: function useLayoutEffect(create, deps) useMemo: function useMemo(create, deps) useReducer: function useReducer(reducer, initialArg, init) useRef: function useRef(initialValue) useState: function useState(initialState) version: &quot;17.0.0&quot; &lt;prototype&gt;: Object { … } . Manipulating the DOM with JSX . JSX is more intuitive than the raw React API and is easier to understand when reading the code. It’s fairly simple HTML-like syntactic sugar on top of the raw React APIs: . const ui = &lt;h1 id=&quot;greeting&quot;&gt;Hey there&lt;/h1&gt; // ↓ ↓ ↓ ↓ compiles to ↓ ↓ ↓ ↓ const ui = React.createElement(&#39;h1&#39;, {id: &#39;greeting&#39;, children: &#39;Hey there&#39;}) . Babel compiles JSX down to React.createElement() calls. . These two examples are identical: . const element = ( &lt;h1 className=&quot;greeting&quot;&gt; Hello, world! &lt;/h1&gt; ); . const element = React.createElement( &#39;h1&#39;, {className: &#39;greeting&#39;}, &#39;Hello, world!&#39; ); . React.createElement() creates an javascript object that holds the “identity” of your element. . // Note: this structure is simplified const element = { type: &#39;h1&#39;, props: { className: &#39;greeting&#39;, children: &#39;Hello, world!&#39; } }; . These objects are called “React elements”. You can think of them as descriptions of what you want to see on the screen. React reads these objects and uses them to construct the DOM and keep it up to date. . Interpolation with JSX . Let’s have a look to this code . &lt;body&gt; &lt;div id=&quot;root&quot;&gt;&lt;/div&gt; &lt;script src=&quot;https://unpkg.com/react@17.0.0/umd/react.development.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://unpkg.com/react-dom@17.0.0/umd/react-dom.development.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://unpkg.com/@babel/standalone@7.12.4/babel.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/babel&quot;&gt; const className = &#39;container&#39; const children = &#39;Hello World&#39; const element = &lt;div className={className}&gt;{children}&lt;/div&gt; ReactDOM.render(element, document.getElementById(&#39;root&#39;)) &lt;/script&gt; &lt;/body&gt; . Notice that we type=&quot;text/babel&quot; to transpile the JSX syntax to javascript. . We use interpolation with the {} to move from JSX to the JavaScript world. Using interpolation tells the compiler that code inside {} should not be changed and considered as pure JavaScript code. .",
            "url": "https://fbraza.github.io/BrazLog/frontend/react/javascript/2021/09/14/raw-react.html",
            "relUrl": "/frontend/react/javascript/2021/09/14/raw-react.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Some notes about BASH programming",
            "content": "This page will serve as a reference for my perigrination with BASH. A good place to keep track of BASH constructs that I met during my personal and professional projects. . A basic command line interface . To setup a command line interface for your bash scripts, you can follow this code snippet: . THISSCRIPT=$(basename $0) VAR=&quot;&quot; usage() { echo &quot;&quot; echo &quot;Usage: $THISSCRIPT [-h|--help]&quot; echo &quot;&quot; echo &quot; Describe here what this script will do&quot; echo &quot;&quot; echo &quot;Params:&quot; echo &quot; -a1|--argument1: description ... &quot; echo &quot; -a2|--argument2: description ... &quot; echo &quot; -an|--argumentn: description ... &quot; echo &quot; -h|--help: description ... &quot; echo &quot;&quot; echo &quot;Examples:&quot; echo &quot;&quot; echo &quot; ./$THISSCRIPT&quot; } # Parsing arguments while [[ $# -gt 0 ]] do case $1 in -a1|--argument1) shift echo &quot;Hello a1&quot; # put here any logic or code execution you want ;; -a2|--argument2) shift VAR=$1 # you can assign a value to a variable initialized before ;; -an|--argumentn) shift echo &quot;Hello an&quot; # put here any logic or code execution you want ;; -h|--help) usage exit 0 # use the exit command if you want the script to exit after an action ;; *) echo &gt;$2 &quot;$THISSCRIPT Invalid argument: $1&quot; usage exit 1 ;; esac shift done # you need to validate VAR [[ -z &quot;$VAR&quot; ]] &amp;&amp; { echo &gt;$2 &quot;$THISSCRIPT ERROR: please specify a value for VAR&quot;; exit 1; } . Code explanation . To get the name of the script file . THISSCRIPT=$(basename $0) . | while loop . while [[ condition ]] do ... done . | case statement: . case $1 in -a1|--argument1) shift # do something ;; esac . esac is the keyword used to end a case statement . | The [[ -z &quot;$string&quot; ]] condition return True if string is empty . | Redirect STDOUT to STDERROR . echo &gt;$2 &quot;your text&quot; . | .",
            "url": "https://fbraza.github.io/BrazLog/bash/devops/linux/2021/09/14/bash-notes.html",
            "relUrl": "/bash/devops/linux/2021/09/14/bash-notes.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Guide for optimizing performance with Spark",
            "content": "Apache Spark is an in-memory processing and analytics engine designed to be particularly efficient to process Big Data. You can use Spark on an on-premise or a cloud-deployed Hadoop cluster or through the Databricks platform. In any of these setups, using Spark efficiently is critical if you want to control and reduce costs. For that you should be able to diagnose and resolve some common performance issues. These usually fall into the five following categories: spill, skew, shuffle, storage and serialization. Here we are going to go over each of them to understand what they are, see how to identify them and mitigate their impact on your workflow. . Skew . Definition and root causes . In Spark, data is distributed across a cluster as partitions that are processed by different worker nodes. Usually partitions are 128 MB sized and evenly distributed. In reality however, some of your partitions can have significantly more records than others. Typically you face data skewness when using join or groupBy operations using a key that is not evenly distributed. This is not a Spark specific problem but keep in mind that the distribution of the data dramatically impacts on the performance of distributed systems. Let’s imagine that in your workflow your data ends up being partitioned as shown below: . . As partition one (P1) is around four times bigger than the others, it takes four time as much time and requires four time as much RAM to process P1. Consequently, the entire Spark job is slower. More specifically, the stage, including these tasks, takes as much time as the P1 processing task. Finally, when P1 does not fit in memory, Spark raises out-of-memory (OOM) errors or undergo some spill on disk, another issue described later. . Monitoring skew . To monitor if your data is skewed, on the Spark UI go on the Stages tab and read the timeline. If the tasks execution time is not evenly distributed and some task takes a dramatic amount of time compared to others, you data is skewed. . Mitigating skew issues . Remember skew is a data problem. Several approaches exist to solve and mitigate it. Here three of them that you should consider when using Spark: . Use the well-known “salting-key” strategy which briefly consists on concatenating the key with a random number. This randomizes the data and redistribute it more evenly. . | Use query hints to annotate and help the optimizer engine to improve logical execution plans. . | Use the Adaptive Query Execution framework that shipped with Apache Spark 3.0 by enabling its features: . // Enable AQE and the adaptive skew join spark.conf.set(&quot;spark.sql.adaptive.enabled&quot;, true) spark.conf.set(&quot;spark.sql.adaptive.skewedJoin.enabled&quot;, true) . | . Spill . Definition and root causes . When the partitions are to big and cannot fit in memory, Spark moves the data on disk and gets it back later in memory. This phenomenon, called spill, is made possible thank to the ExternalAppendOnlyMap collection class. This prevents OOM errors when partitioned data is too big to fit in memory. Potentially expensive disk I/O operations arise as a trade-off from this operation. There are different scenarios where spill happens: . Ingesting too large partitions. . | Aggregating tables on a skewed column. . | Using join(), crossjoin() or the explode() operations may create very large partitions. . | Using the union() operation. This operation takes two DataFrames and combine them into one and always use the same number of partitions that it started with. As depicted below we start with two DataFrame (DF1 &amp; DF2) with a certain number of partitions and end up with the same number of concatenated partitions that are bigger. . . | Setting an inappropriate value (too big usually) to the spark.sql.files.maxPartitionBytes parameter (set to 128 MB by default). Our advice is to keep it at default and only alter it after some testing. . | . Monitoring spill . To assess whether spill happened during your jobs, the easiest way is to go to the Stages tab from the Spark U and read the summary metrics table for all completed tasks. There, spill is represented by two values: . Spill (Memory): this is the size of the data as it existed in memory | Spill (Disk): this is the size of the data as it existed in disk | . Please note two things: first, the Spill value in disks will be always lower than Spill value in memory due to compression. Second, if no spill occurred you won’t find these values in the summary metrics table. . To know whether spill is occurring during the execution of your jobs, and not wait its end, use the Spark.Listener class in your code. A SparkListener object captures events from the Spark scheduler over the course of a Spark application execution (it is used to output the logs and metrics in the Spark UI). You can implement your own custom SpillListener to track spill. A very nice example can be found in the spark Github repository: . // from https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/TestUtils.scala /** * A `SparkListener` that detects whether spills have occurred in Spark jobs. */ private class SpillListener extends SparkListener { private val stageIdToTaskMetrics = new mutable.HashMap[Int, ArrayBuffer[TaskMetrics]] private val spilledStageIds = new mutable.HashSet[Int] def numSpilledStages: Int = synchronized { spilledStageIds.size } override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = synchronized { stageIdToTaskMetrics.getOrElseUpdate( taskEnd.stageId, new ArrayBuffer[TaskMetrics]) += taskEnd.taskMetrics } override def onStageCompleted(stageComplete: SparkListenerStageCompleted): Unit = synchronized { val stageId = stageComplete.stageInfo.stageId val metrics = stageIdToTaskMetrics.remove(stageId).toSeq.flatten val spilled = metrics.map(_.memoryBytesSpilled).sum &gt; 0 if (spilled) { spilledStageIds += stageId } } } . Mitigating spill issues . A quick answer would be to add more memory to your cluster’s workers. If not possible, decrease the size of each partition by increasing the number of partitions generated during data processing. In Spark you can: . configure the default number of partitions. . spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, number_of_partitions) . | repartition the data with the repartition() method (be careful this is an expensive operation). . | configure the size of each partition with the spark.sql.files.maxPartitionBytes Spark setting. . | solve any issues related to skewed data first. . | . Shuffle . Definition and root causes . Shuffle occurs when Spark needs to regroup data from different partitions to compute a final result. It is a side effect observed with wide transformations. These include for example the groupBy(), distinct() or join() operations. Let’s explain shuffle by going through a quick and simple example involving a groupBy() combined with a count() operation. . . First the data is read from source (i.e., HDFS, cloud storage, previous stage) (1). At stage 1, Spark performs a “mapping” operation to identify which record belongs to which group (2). Then data is prepared for the next partitions and written on disk in shuffle files (3). For the next stage, data is read from the shuffle files and transferred through the network to the next executors (4) where a “reduce” operation is performed. Our final result is computed (5) and data written on disk (6). . Shuffle is a potentially very expensive operation that involves a lot of disk and network I/O which impact on Spark performance. Additionally, keep in mind that whatever type of wide transformations you are executing, the mapping and the reduce operations are performed in-memory and remain susceptible to some spill on disk which will add to the overhead of disk I/O. . Monitoring shuffle . The Spark UI is useful to get some statistics about the shuffled data in the Stages tab. In the summary metrics table, have a look to the following entries: . Shuffle Read Size / Records: this the total of shuffle bytes read locally or from remote executors. | Shuffle Remote Reads: this is the total of shuffle bytes read only from remote executors. | Shuffle Read Blocked Time: this the time spent awaiting for shuffle data to be read from remote executors. | . While these numbers are interesting, there are no conventional threshold. It depends on your data and the way you process it. Reducing the amount of shuffle is something you should target but keep in mind that shuffle is a necessary evil. With the Spark UI, you can identify the most expensive tasks and try to reduce shuffle in these cases. Check also if data is not skewed (watch the timeline) or if spill occurred before focusing on shuffle. Finally knowing how to mitigate it properly should permit you to keep these metrics below acceptable thresholds. . Mitigating shuffle issues . There are different approaches against shuffle: . to reduce the network I/O, design your cluster by favoring fewer but larger workers. This limits the number of machines where the data is shuffled across. . | to reduce the amount of data being shuffled, filter out columns or unnecessary records for your processing and analysis. . | to optimize join queries, when the size of one DataFrame is small (below 10 MB), Spark uses Broadcast joins (see the spark.sql.autoBroadcastJoinThreshold, in the documentation). But be careful with it and be sure to have enough memory on the driver as smallest partitions are processed there. . | to optimize joins, use bucketing (spark.sql.sources.bucketing.enabled is set to true by default). Bucketing is an optimization technique to pull down data into distinct manageable parts named “buckets”. Use the bucketBy() method to bucket your data based on specific columns. Notably bucketing your data by sorted keys permits to avoid expensive shuffle operations. . | to reduce the shuffle behavior you can edit several spark configuration properties. You should not blindly play with these but rather test your changes before going into production. We advise the reader to focus on the following properties to limit shuffle and expensive I/O activity: . Configuration Description and recommendation . spark.driver.memory | The default value is 1GB. This is the amount of memory allocated to the Spark driver to receive data from executors. You can change while submitting a spark job with the spark-submit command. Increase the value if you expect the driver to process more a great amount of data notably in the context of a broadcast join. | . spark.shuffle.file.buffer | The default value is 32 KB. If your workload increased, set it to larger values (1 MB). The more you have the more Spark will buffer your data before writing mapping result on disk. | . spark.file.transferTo | Set to true by default. Set it to false if you want Spark to use the file buffer before writing on disk. This decrease the I/O activity. | . spark.io.compression.lz4.block.Size | The default value is 32 KB. By increasing it you can decrease the size f the shuffle file (don’t go over 1 MB). By default Spark uses lz4 compression but you can change the compression codec by altering the property spark.io.compression.codec | . | . Storage . Definition and root causesStages . When talking about the impact of storage on performance, we talk about the overhead I/O cost of data ingestion. The most common example relate to: . reading tiny files . The “tiny files problem” has been pinpointed and described since the existence of distributed system like Hadoop. Things are similar with Spark. Before executing any query on your data, Spark will assess how many tasks are required to read the input data and determine on which worker it should schedule these tasks. Moreover, some files contain metadata (i.e., ORC, Parquet…) to be read and parsed. Then with a huge number of small files, you increase the workload on the Spark Scheduler, number of read / close file operations and metadata to parse. Collectively these operations greatly impact on Spark performance. . | scanning repositories . Directory scanning adds overhead to the tiny files problem. But it also exists for terabytes files especially in the context of highly partitioned datasets on disks. For each partition you have one directory. If we consider some data partitioned by year, month day and hour we will have 8640 directories to scan! If you let your data scale for 10 years you will end with 86400 directories. Keep in mind that the Spark driver scan the repository one at the time. . | dealing with dataset schemas . inferring schema with Spark for csv and json files also impairs performance in Spark. Indeed it requires to do a full scan of the data to assess all types. In contrast, Spark only reads one file when dealing with the Parquet format. This is under the assumption that all Parquet files under the same partition have the same schema. But be careful if you wish to support Parquet schema evolution. For each new schema evolution, you have a new partition with new files. If you alter the schema a lot you progressively fall into the scanning issue as described before. By default, schema evolution is disabled in Spark 2 &amp; 3 but if you need it use the spark.sql.parquet.mergeSchema property. . | . Monitoring storage . On the Spark UI you have access to some interesting metrics to monitor file scanning and count: . In the Stages tab when looking at the stage details, have a look to the Input Size / Records metrics which gives you an idea about the total amount of data that is ingested versus the number of records. | In the SQL tab, select your job and stage to have access to more details as the total number of files read, scan time total and filesystem read time (sampled) total (min, med, max). | . Mitigating storage issues . Measures to mitigate these issues are pretty simple: . avoid using tiny files if possible, or merge them into bigger files before performing any operations on your data. | keep in mind that the reading / scanning problem cannot be solved by adding more resources to your workers. Everything is handled by the driver. | partition your data according to your needs. Avoid over-partitioning, if not necessary, although this will depend on the data problem you are tackling. | . Serialization . Definition and root causes . Serialization improves performance on distributed applications by converting code objects and data into a stream of bytes and vice-versa. For distributed systems like Spark, the majority of the compute time is spent on data serialization. . When writing and executing code, the Spark driver serializes the code, send it to the executors that deserialized the code to execute it. By default, Spark uses Java serialization with the ObjectOutputStream framework that works with any types and classes that implement java.io.Serializable. The Kryo serialization permits to serialize data faster but is not compatible with every classes and types. You also need to do extra-work before to register the classes you want to be serialized. . At the beginning of Spark, users were mostly dealing with resilient distributed datasets (RDDs) by writing empiric code which describes how you want to do things. For RDDs, Spark uses Java serialization to serialize individual Scala and Java objects. This process is expensive especially because the Java objects model is highly memory consumptive. This is even more expensive with Pyspark where code and data are serialized / deserialized twice: first to Java/Scala and then to Python. . Note: For PySpark all data that come to and from a Python executor has to be passed through a socket and a Java Virtual Machine (JVM) worker. Briefly, with PySpark, the SparkContext uses Py4J to launch a JVM to create a JavaSparkContext. That is the communication between Py4J and the JVM that orchestrate the data flow. It is worth noting that Py4J calls have pretty high latency. That is why all operations on RDDs takes much more time on PySpark than on Spark. . The project Tungsten in 2004 and the design of the DataFrame API were critical steps towards performances improvement of the Spark engine. The first altered and improved the Java objects model allowing Spark to manage data expressed as DataFrame much more efficiently. The API permits us to write more declarative code that will be processed as instructions for the transformation chain. Both minimize the amount of work required by the Spark JVMs. And if you work with PySpark note that in this context nothing will be done in Python then excluding the double serialization needed with RDDs. . All these theoretical details are not easy to grasp but shed light on a very important aspect of Spark: each time you will get away from the DataFrame API in your code, you will lose all these optimizations and encounter some performance hits. This is notably the case when you: . process manually RDDs (writing map() and / or lambda functions). | use user-defined functions (UDFs) that are useful and easy-to-use to extend Spark SQL functionalities. However, UDFs are “black-box” and prevent several Spark optimization processes including the way Spark deals with Java objects. For example, using UDFs in PySpark will bring you back to the double serialization issue. | . Mitigating storage issues . Here the rules are simple: . USE THE DATAFRAME API. Dig into the API to know exactly the possibilities offered to you. | Use UDFs only when strictly necessary. By design Scala will be faster than Python when using UDFs. With Python however you can still give a try to the pandas UDFS (also called vectorized UDFs) that use apache Arrow to give a performance boost to PySpark in this context. | Give a try to the Kryo serialization framework despite its limitations. You may benefit from its performance boost for your current data problem. It is actually the default framework used by Spark in Apache Hudi. | . Conclusion . Using Spark efficiently requires a good knowledge of its inner parts and an ability to identify technical and performance issues. Here we framed and articulated the five main problems that you may encounter and have discussed a number of techniques to bypass them and optimize your Spark applications. Noteworthy, do not take this article as a step-wise guide that will solve all your problems for all your situations. Instead, it should give you an idea of what is happening and what are the solutions you can implement based on your project. This lets a lot of rooms for experimentations and testing that are necessary to find the right and optimal balance for your Spark applications. .",
            "url": "https://fbraza.github.io/BrazLog/spark/python/scala/bigdata/2021/07/08/spark-optimization.html",
            "relUrl": "/spark/python/scala/bigdata/2021/07/08/spark-optimization.html",
            "date": " • Jul 8, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Modern Python part 3 - run a CI pipeline & publish your package to PiPy",
            "content": "To propose a well-maintained and usable Python package to the open-source community or even inside your company, you are expected to accomplish a set of critical steps. First ensure that your code is unit tested. Second respect the common writing and format styles. Automate these steps and integrate them in a continuous integration pipeline to avoid any regression that stems from modifications applied to your source code. Finally, provide enough documentation for future users. Once done it is common to publish your Python package on the Python Package Index (PyPI). Here we are going to see how to accomplish each of these steps using Poetry, Tox and GitHub Actions. The code used for our use case can be found on our repository. . This article is the last one from a series of three in which I share some best practices. . Part 1: project initialization with pyenv and poetry | Part 2: unit testing and commit enforcement | Part 3: CI pipeline with GitHub Actions and publication on PiPy | . Automate linter checks and tests with tox . If not done, activate your virtual environment. . poetry shell . To check the conformity of our code, we use a couple of packages that are going to evaluate if the code respects the common Python writing guidelines. Then, to automate their execution as well as our unit tests, we use tox. To install them run: . poetry add black flake8 pylint tox --dev . tox and poetry don’t work well together by default. They are somewhat redundant. To use them together, we need to implement a few tricks (see issues 1941 and 1745). tox install its own environment and dependencies to run its tasks. But to install dependencies, you have to declare the command poetry install in our tox configuration. This brings redundancy and can lead to some issues. Moreover, this does not allow to install developers dependencies needed to execute our tests. It is more productive to let tox use the poetry.lock file to install necessary dependencies. For this, I advise you to use the tox-poetry-installer package developed to solve these problems: . poetry add tox-poetry-installer[poetry] --dev . Now we declare our tox configuration in a tox.ini file whose content is: . [tox] envlist = py38 isolated_build = true [testenv] description = Linting, checking syntax and running tests require_locked_deps = true install_dev_deps = true commands = poetry run black summarize_dataframe/summarize_df.py poetry run flake8 summarize_dataframe/summarize_df.py poetry run pylint summarize_dataframe/summarize_df.py poetry run pytest -v . You can see two sections here: . [tox]: Define the global settings for your tox automation pipeline including the Python version of the test environments. | [testenv]: Define the test environments. In our case we have some extra-variables require_locked_deps and install_dev_deps that are brought by the tox-poetry-installer package. require_locked_deps is to choose whether or not you want tox to harness the poetry.lock file to install dependencies. install_dev_deps is to choose if tox installs the developer dependencies. | . Refer to the tox documentation to learn more about the configuration as well as the tox-poetry-installer documentation to learn more about it extra configuration. . Run the tox pipeline: . tox py38 run-test: commands[0] | poetry run black summarize_dataframe/summarize_df.py All done! ✨ 🍰 ✨ 1 file left unchanged. py38 run-test: commands[1] | poetry run flake8 summarize_dataframe/summarize_df.py py38 run-test: commands[2] | poetry run pylint summarize_dataframe/summarize_df.py ************* Module summarize_dataframe.summarize_df summarize_dataframe/summarize_df.py:1:0: C0114: Missing module docstring (missing-module-docstring) summarize_dataframe/summarize_df.py:4:0: C0103: Argument name &quot;df&quot; doesn&#39;t conform to snake_case naming style (invalid-name) summarize_dataframe/summarize_df.py:11:4: C0103: Argument name &quot;df&quot; doesn&#39;t conform to snake_case naming style (invalid-name) summarize_dataframe/summarize_df.py:23:4: C0103: Argument name &quot;df&quot; doesn&#39;t conform to snake_case naming style (invalid-name) summarize_dataframe/summarize_df.py:43:0: C0103: Argument name &quot;df&quot; doesn&#39;t conform to snake_case naming style (invalid-name) Your code has been rated at 7.62/10 (previous run: 7.62/10, +0.00) ERROR: InvocationError for command /home/fbraza/Documents/python_project/summarize_dataframe/.tox/py38/bin/poetry run pylint summarize_dataframe/summarize_df.py (exited with code 16) ________________________________________________________ summary ________________________________________________________________ ERROR: py38: commands failed . An error is raised because pylint shed light on some style inconsistencies. By default, tox quits if any warnings or errors occurred during the execution of the commands. The errors are by themselves quite explicit. After correcting them, run again the pipeline: . tox # shorten for brevety [...] py38 run-test: commands[0] | poetry run black summarize_dataframe/summarize_df.py All done! ✨ 🍰 ✨ 1 file left unchanged. py38 run-test: commands[1] | poetry run flake8 summarize_dataframe/summarize_df.py py38 run-test: commands[2] | poetry run pylint summarize_dataframe/summarize_df.py -- Your code has been rated at 10.00/10 (previous run: 10.00/10, +0.00) py38 run-test: commands[3] | poetry run pytest -v ================================================= test session starts ============================================================= platform linux -- Python 3.8.7, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 -- /home/fbraza/Documents/python_project/summarize_dataframe/.tox/py38/bin/python cachedir: .tox/py38/.pytest_cache rootdir: /home/fbraza/Documents/python_project/summarize_dataframe collected 2 items tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary PASSED [ 50%] tests/test_summarize_dataframe.py::TestDataSummary::test_display PASSED [100%] ================================================= 2 passed in 0.30s =============================================================== ______________________________________________________ summary ____________________________________________________________________ py38: commands succeeded congratulations :) . Perfect. The tox automation pipeline succeed locally. The next step start implements the CI pipeline with GitHub Actions. . Continuous Integration with GitHub Actions . GitHub Actions make it easy to automate all your software workflows. This service is event-driven meaning that a set of commands is triggered when a specific event occurs. Such events could be a commit pushed to the branch or a pull request. GitHub Actions are pretty convenient to run all needed tests against your code. . Most importantly, GitHub Actions provide the ability to test your Python package using several Python versions and on different operating systems (Linux, macOS and Windows). The only thing you need is an existing repository and a .github/workflows/&lt;file_name&gt;.yaml file: . mkdir -p .github/workflows touch .github/workflows/ci.yml . The content of the .github/workflows/ci.yml file is: . name: CI Pipeline for summarize_df on: - push - pull_request jobs: build: runs-on: $ strategy: matrix: platform: [ubuntu-latest, macos-latest, windows-latest] python-version: [3.7, 3.8, 3.9] steps: - uses: actions/checkout@v1 - name: Set up Python $ uses: actions/setup-python@v2 with: python-version: $ - name: Install dependencies run: | python -m pip install poetry poetry install - name: Test with tox run: poetry run tox . A few words about the different fields: . on: this field defines the type of event that is going to trigger the pipeline. | jobs: this field defines the multiple steps of your pipeline. They run in an instance of a virtual environment. | build: this is where all the magic happens: The strategy.matrix.platform field defines the different OS you want to use to test your package. Use templating to pass these values to the build.runs-on field ($). | The strategy.matrix.python-version field defines the different versions of Python you want to use to test your package. | The steps field permits you to specify which actions you use (steps.uses) and which command you want to run (steps.run) | . | . Before finishing, alter the tox.ini and pyporject.toml files accordingly. Initially we chose the 3.8 Python version for tox. But we want it to be compatible with 3.7 and 3.9. For the pyproject.toml file, choose a version expected to be compatible with your package. Here we choose to make our package compatible from 3.7.1 and above. Below are the changes added to our files: . # content of: tox.ini [tox] envlist = py37,py38,py39 isolated_build = true skip_missing_interpreters = true [...] . Having several python version defined in your tox.ini file causes issue with your local testing. Running the tox raises an error because of lacking python versions. If you still want to test you module locally just use the tox -e py command. . # content of: pyproject.toml [...] [tool.poetry.dependencies] python = &quot;^3.7.1&quot; [...] . When you modify the pyproject.toml file, always run the poetry update command that can check some unexpected incompatibilities between your dependencies and the version of Python you wish to use. . To finish, we are going to install a package, called tox-gh-actions, to run tox in parallel on GitHub while using several versions of Python: . poetry add tox-gh-actions --dev . The pipeline is ready. Add, commit and push your changes to see the pipeline running: . echo &quot;!.github/&quot; &gt;&gt; .gitignore git add .gitignore git commit -m &quot;build: update .gitignore to unmask .github/ folder&quot; git add pyproject.toml tox.ini poetry.lock `.github/workflows/ci.yml` git commit -m &quot;build: tox pipeline + github actions CI pipeline&quot; . Go to your GitHub repository and click on the Actions tab: You see all the previous and ongoing pipelines: Let’s click on the ongoing pipeline. The pipeline runs on each OS and for each Python version. Wait a couple of minutes to see the results: All the pipelines succeed! We are ready to publish our package on the PyPi registry. . Publish packages on PyPi with poetry . To make your package publishable, add some details in the [tool.poetry] section of your pyproject.toml file: . [tool.poetry] name = &quot;summarize_dataframe&quot; version = &quot;0.1.0&quot; description = &quot;A package to provide summary data about pandas DataFrame&quot; license = &quot;MIT&quot; authors = [&quot;fbraza &lt;fbraza@tutanota.com&gt;&quot;] keywords = [&quot;pandas&quot;, &quot;dataframe&quot;] readme = &quot;README.md&quot; homepage = &quot;https://github.com/fbraza/summarize_dataframe&quot; repository = &quot;https://github.com/fbraza/summarize_dataframe&quot; include = [&quot;CHANGELOG.md&quot;] [...] . All the variables here are quite explicit. These are metadata needed for the publication of the package. The include variable is interesting to add any files you want. In our case we are going to add a CHANGELOG.md file. Do you remember commitizen? If not please take the time to read our article on commitizen and conventional commits. Use the following command: . cz bump . It prints the semantic version from your pyproject.toml file and ask you to create a Git tag. The version will be updated based on your Git commit. Next we create the CHANGELOG.md: . cz changelog cat CHANGELOG.md ## Unreleased ## 0.1.0 (2021-04-28) ### Refactor - correct pylint warnings - split the function into two: one returning df other for output ### Feat - implementation of the summary function to summarize dataframe . Your CHANGELOG.md has been created based on the Git history you generated thanks to commitizen. Pretty neat isn’t it?! Once done let’s focus on publishing our package: . poetry build Building summarize_dataframe (0.1.0) - Building sdist - Built summarize_dataframe-0.1.0.tar.gz - Building wheel - Built summarize_dataframe-0.1.0-py3-none-any.whl . This creates a folder called dist where the built package is located. To test if everything works you can use pip: . Do this outside of your virtual environment to not pollute it. . pip install path/to/your/package/summarize_dataframe-0.1.0-py3-none-any.whl . Now we need to create an account on PyPi. Just enter the expected details, validate your email and execute: . poetry publish Username: *********** Password: *********** Publishing summarize_dataframe (0.1.0) to PyPI - Uploading summarize_dataframe-0.1.0-py3-none-any.whl 100% - Uploading summarize_dataframe-0.1.0.tar.gz 100% . The package is now online and shared with the community. . . Conclusion . tox provides a nice interface to automate all your unit tests and validation checks. The ecosystem around poetry is getting more mature and provides solutions to work with tox without too much hassle. Collectively, these two solutions permit to establish a very efficient and coherent CI pipeline. To run the pipeline and test your packages against different OS or versions of Python, you can leverage GitHub Actions as described above. . poetry was at the center of our approach. From the project initialization to its publication and going through the management of its packages and dependencies. poetry demonstrated its ease of use and efficacy that will definitely facilitate the life of developers, Data Scientists or Data Engineers who develop projects in Python. . Our articles describe a full setup that you can leverage to build your own Python project to respect good software engineering practices. . Cheat Sheet . tox . Run your tox pipeline . tox . | . poetry . Build your package . poetry build . | Publish your package . poetry publish . | . Acknowledgments . This article was first published in Adaltas blog and kindly reviewed by the CEO David Worms and one consultant Barthelemy NGOM. .",
            "url": "https://fbraza.github.io/BrazLog/python/devops/2021/06/29/modern-python-part3.html",
            "relUrl": "/python/devops/2021/06/29/modern-python-part3.html",
            "date": " • Jun 29, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Modern Python part 2 - write unit tests & enforce Git commit conventions",
            "content": "Good software engineering practices always bring a lot of long-term benefits. For example, writing unit tests permits you to maintain large codebases and ensures that a specific piece of your code behaves as expected. Writing consistent Git commits also enhance the collaboration between the project stakeholders. Well-crafted Git commit messages open the door to automatic versioning and generated change log files. Consequently, a lot of attempts are currently ongoing and applied to normalize the messages written in our Git commits. . In the first part of this serie, we setup, our project by installing different Python versions with pyenv, setting a local version of Python with pyenv, encapsulating it into a virtual environment with poetry. Here we show more precisely how to unit test your Python application and how to enforce and validate your Git commit messages. The source code associated with this article is published on GitHub. . This article is the second one from a series of three in which I share our best practices. . Part 1: project initialization with pyenv and poetry | Part 2: unit testing and commit enforcement | Part 3: CI pipeline with GitHub Actions and publication on PiPy | . Testing our code . The project is a simple python function that summarizes data present in a pandas DataFrame. The function outputs the number of rows and columns and the frequency of each data types present in the pandas DataFrame: . - Data Summary Values Number of rows 230 Number of columns 9 float64 3 int64 4 object 2 . Go to your project root directory and activate your virtual environment: . poetry shell . We add a couple of dependencies using poetry: . poetry add -D pynvim numpy pandas Using version ^0.4.3 for pynvim Using version ^1.20.2 for numpy Using version ^1.2.3 for pandas Updating dependencies Resolving dependencies... (1.4s) Writing lock file Package operations: 8 installs, 0 updates, 0 removals • Installing six (1.15.0) • Installing greenlet (1.0.0) • Installing msgpack (1.0.2) • Installing numpy (1.20.2) • Installing python-dateutil (2.8.1) • Installing pytz (2021.1) • Installing pandas (1.2.3) • Installing pynvim (0.4.3) . The -D flag indicates that the dependency only apply to development environments. . Note: I personally use NeoVim for coding that is why I need the pynvim package to support NeoVim python plugins. . Based on the expected output defined above, our program is made of three steps: . Getting the shape of the pandas DataFrame. | Getting the pandas dtypes frequency. | Concatenating the two results into a unified DataFrame that we will use to output the final result. | Once the final DataFrame is obtained we output the result as depicted above. In this regard our code scaffold could look as the following: . import pandas as pd def data_summary(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function defined to return a DataFrame containing details about the number of rows and columns and the column dtype frequency of the passed pandas DataFrame &quot;&quot;&quot; def _shape(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function defined to return a dataframe with details about the number of row and columns &quot;&quot;&quot; return None def _dtypes_freq(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function defined to return a dataframe with details about the pandas dtypes frequency &quot;&quot;&quot; return None return None def display_summary(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function define to print out the result of the data summary &quot;&quot;&quot; result_df = True message = &#39;- Data summary -&#39; print(message, result_df, sep=&#39; n&#39;) . Let’s now start writing our unit tests. We are going to use the unittest tool available with the Python standard library. You may remember in the previous article that pytest was defined as a developer dependency for testing. It is not an issue with pytest because it natively runs tests written with the unittest library. . Unit tests are single methods that unittest expects you to write inside Python classes. Choose a descriptive name for your test classes and methods. The name of your test methods should start with test_. Additionally, unittest uses a series of special assertion methods inherited from the unittest.TestCase class. In practice, a test should precisely cover one feature, be autonomous without requiring external cues, and should recreate the conditions of their success. . To recreate the necessary environment, setup code must be written. If this code happens to be redundant, implements a setUp() method, that will be executed before every single test. This is pretty convenient to re-use and re-organize your code. Depending on your use case you may have to perform systematic operations after the tests ran. For that, you may use the tearDown() method. . First you can read below the unit test we implemented for the data_summary() function: . import unittest import pandas as pd from summarize_dataframe.summarize_df import data_summary class TestDataSummary(unittest.TestCase): def setUp(self): # initialize dataframe to test df_data = [[1, &#39;a&#39;], [2, &#39;b&#39;], [3, &#39;c&#39;]] df_cols = [&#39;numbers&#39;, &#39;letters&#39;] self.df = pd.DataFrame(data=df_data, columns=df_cols) # initialize expected dataframe exp_col = [&#39;Values&#39;] exp_idx = [&#39;Number of rows&#39;, &#39;Number of columns&#39;, &#39;int64&#39;, &#39;object&#39;] exp_data = [[3], [2], [1], [1]] self.exp_df = pd.DataFrame(data=exp_data, columns=exp_col, index=exp_idx) def test_data_summary(self): expected_df = self.exp_df result_df = data_summary(self.df) self.assertTrue(expected_df.equals(result_df)) if __name__ == &#39;__main__&#39;: unittest.main() . The setUp() method initializes two distinct pandas DataFrame. self.exp_df is the resulting DataFrame we expect to get after calling the data_summary() function and self.df is the one used to test our functions. At the moment, tests are expected to fail. The logic has not been implemented. To test with poetry use the command: . poetry run pytest -v ============================================== test session starts ============================== platform linux -- Python 3.8.7, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 -- /home/fbraza/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8/bin/python cachedir: .pytest_cache rootdir: /home/fbraza/Documents/python_project/summarize_dataframe collected 1 item tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary FAILED [100%] =============================================== FAILURES ========================================= ___________________________________TestDataSummary.test_data_summary _____________________________ self = &lt;tests.test_summarize_dataframe.TestDataSummary testMethod=test_data_summary&gt; def test_data_summary(self): expected_df = self.exp_df result_df = data_summary(self.df) &gt; self.assertTrue(expected_df.equals(result_df)) E AssertionError: False is not true tests/test_summarize_dataframe.py:26: AssertionError ============================================== short test summary info ============================= FAILED tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary - AssertionError: False is not true ============================================== 1 failed in 0.32s =================================== . Using the -v flag returns a more verbose output for your test results. You can see that your tests are labeled according to the classes and functions names you gave (i.e., &lt;test_module.py&gt;::&lt;class&gt;::&lt;test_method&gt;). . The code is updated to conform with the unit tests: . import pandas as pd def data_summary(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to output details about the number of rows and columns and the column dtype frequency of the passed pandas DataFrame &quot;&quot;&quot; def _shape(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to return a dataframe with details about the number of row and columns &quot;&quot;&quot; row, col = df.shape return pd.DataFrame(data=[[row], [col]], columns=[&#39;Values&#39;], index=[&#39;Number of rows&#39;, &#39;Number of columns&#39;]) def _dtypes_freq(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to return a dataframe with details about the pandas dtypes frequency &quot;&quot;&quot; counter, types = {}, df.dtypes for dtype in types: tmp = str(dtype) if tmp in counter.keys(): counter[tmp] += 1 else: counter[tmp] = 1 values = [[value] for value in counter.values()] return pd.DataFrame(data=values, columns=[&#39;Values&#39;], index=list(counter.keys())) result_df = pd.concat([_shape(df), _dtypes_freq(df)]) return result_df def display_summary(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function define to print out the result of the data summary &quot;&quot;&quot; result_df = True message = &#39;- Data summary -&#39; print(message, result_df, sep=&#39; n&#39;) . Run our test again: . poetry run pytest -v =============================================== test session starts =============================================================== platform linux -- Python 3.8.7, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 -- /home/fbraza/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8/bin/python cachedir: .pytest_cache rootdir: /home/fbraza/Documents/python_project/summarize_dataframe collected 1 item tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary PASSED [100%] =============================================== 1 passed in 0.28s ================================================================= . One last thing here. In our tests, we did not test the actual output. Our module is designed to output a string representation of our DataFrame summary. There are solutions to achieve this goal with unittest. However we are going to use pytest for this test. Surprising isn’t it? As said before pytest interpolates very well with unittest and we are going to illustrate it now. Here the code for this test: . import unittest import pytest import pandas as pd from summarize_dataframe.summarize_df import data_summary, display_summary class TestDataSummary(unittest.TestCase): def setUp(self): # initialize dataframe to test df_data = [[1, &#39;a&#39;], [2, &#39;b&#39;], [3, &#39;c&#39;]] df_cols = [&#39;numbers&#39;, &#39;letters&#39;] self.df = pd.DataFrame(data=df_data, columns=df_cols) # initialize expected dataframe exp_col = [&#39;Values&#39;] exp_idx = [&#39;Number of rows&#39;, &#39;Number of columns&#39;, &#39;int64&#39;, &#39;object&#39;] exp_data = [[3], [2], [1], [1]] self.exp_df = pd.DataFrame(data=exp_data, columns=exp_col, index=exp_idx) @pytest.fixture(autouse=True) def _pass_fixture(self, capsys): self.capsys = capsys def test_data_summary(self): expected_df = self.exp_df result_df = data_summary(self.df) self.assertTrue(expected_df.equals(result_df)) def test_display(self): print(&#39;- Data summary -&#39;, self.exp_df, sep=&#39; n&#39;) expected_stdout = self.capsys.readouterr() display_summary(self.df) result_stdout = self.capsys.readouterr() self.assertEqual(expected_stdout, result_stdout) if __name__ == &#39;__main__&#39;: unittest.main() . Notice the decorator @pytest.fixture(autouse=True) and the function it encapsulates (_pass_fixture). In the unit test terminology, this method is called a fixture. Fixtures are functions (or methods if you use an OOP approach), which will run before each test to which it is applied. Fixtures are used to feed some data to the tests. They fill the same objective as the setUp() method we used before. Here we are using a predefined fixture called capsys to capture the standard output (stdout) and reuse it in our test. We can then modify our code display_summary() accordingly: . import pandas as pd def data_summary(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to output details about the number of rows and columns and the column dtype frequency of the passed pandas DataFrame &quot;&quot;&quot; def _shape(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to return a dataframe with details about the number of row and columns &quot;&quot;&quot; row, col = df.shape return pd.DataFrame(data=[[row], [col]], columns=[&#39;Values&#39;], index=[&#39;Number of rows&#39;, &#39;Number of columns&#39;]) def _dtypes_freq(df: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Function defined to return a dataframe with details about the pandas dtypes frequency &quot;&quot;&quot; counter, types = {}, df.dtypes for dtype in types: tmp = str(dtype) if tmp in counter.keys(): counter[tmp] += 1 else: counter[tmp] = 1 values = [[value] for value in counter.values()] return pd.DataFrame(data=values, columns=[&#39;Values&#39;], index=list(counter.keys())) result_df = pd.concat([_shape(df), _dtypes_freq(df)]) return result_df def display_summary(df: pd.DataFrame) -&gt; None: &quot;&quot;&quot; Function define to print out the result of the data summary &quot;&quot;&quot; result_df = data_summary(df) message = &#39;- Data summary -&#39; print(message, result_df, sep=&#39; n&#39;) . Then run the tests again: . poetry run pytest -v =============================================== test session starts =============================================================== platform linux -- Python 3.8.7, pytest-5.4.3, py-1.10.0, pluggy-0.13.1 -- /home/fbraza/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8/bin/python cachedir: .pytest_cache rootdir: /home/fbraza/Documents/python_project/summarize_dataframe collected 2 items tests/test_summarize_dataframe.py::TestDataSummary::test_data_summary PASSED [ 50%] tests/test_summarize_dataframe.py::TestDataSummary::test_display PASSED [100%] =============================================== 2 passed in 0.29s ================================================================= . The tests now succeed. It is time to commit and share our work, for example by publishing it to GitHub. Before that, let’s take a close look at how to properly communicate about our work with Git commit messages while respecting and enforcing a common standard. . Enforce Git commit messages rules in your Python project . Writing optimal Git commit messages is not an easy task. Messages need to be clear, readable, and understandable in the long term. The Conventional Commits specification proposes a set of rules for creating explicit commit histories. . Using commitizen . In our series about JavaScript monorepos, we saw how to integrate these conventions to enforce good practices regarding commit messages. Applied to Python, we are going to use a package called commitizen to achieve this. Let’s add this package to our developer dependencies: . poetry add -D commitizen Using version ^2.17.0 for commitizen Updating dependencies Resolving dependencies... (3.1s) Writing lock file Package operations: 11 installs, 0 updates, 0 removals • Installing markupsafe (1.1.1) • Installing prompt-toolkit (3.0.18) • Installing argcomplete (1.12.2) • Installing colorama (0.4.4) • Installing decli (0.5.2) • Installing jinja2 (2.11.3) • Installing pyyaml (5.4.1) • Installing questionary (1.6.0) • Installing termcolor (1.1.0) • Installing tomlkit (0.7.0) • Installing commitizen (2.17.0) . To setup commitizen for your project, run the command cz init. It prompts us with a set of questions: . cz init ? Please choose a supported config file: (default: pyproject.toml) (Use arrow keys) » pyproject.toml .cz.toml .cz.json cz.json .cz.yaml cz.yaml ? Please choose a cz (commit rule): (default: cz_conventional_commits) (Use arrow keys) » cz_conventional_commits cz_jira cz_customize ? Please enter the correct version format: (default: &quot;$version&quot;) ? Do you want to install pre-commit hook? (Y/n) . Choose all default choices here as they fit perfectly with our actual situation. The last question asks us if we want to use pre-commit hook. We are going to come back to this later on. So just answer no for now. If we look at our pyproject.toml file we can see that a new entry named [tool.commitizen] has been added: . [...] [tool.commitizen] name = &quot;cz_conventional_commits&quot; # commit rule chosen version = &quot;0.0.1&quot; tag_format = &quot;$version&quot; . To check your commit message, you can use the following command: . cz check -m &quot;all summarize_data tests now succeed&quot; commit validation: failed! please enter a commit message in the commitizen format. commit &quot;&quot;: &quot;all summarize_data tests now succeed&quot; pattern: (build|ci|docs|feat|fix|perf|refactor|style|test|chore|revert|bump)!?( ( S+ ))?:( s.*) . Our message is rejected because it does not respect the commit rules. The last line suggests some patterns to use. Take some time to read the conventional commits documentation and run the command cz info to print a short documentation: . cz info The commit contains the following structural elements, to communicate intent to the consumers of your library: fix: a commit of the type fix patches a bug in your codebase (this correlates with PATCH in semantic versioning). feat: a commit of the type feat introduces a new feature to the codebase (this correlates with MINOR in semantic versioning). BREAKING CHANGE: a commit that has the text BREAKING CHANGE: at the beginning of its optional body or footer section introduces a breaking API change (correlating with MAJOR in semantic versioning). A BREAKING CHANGE can be part of commits of any type. Others: commit types other than fix: and feat: are allowed, like chore:, docs:, style:, refactor:, perf:, test:, and others. [...] . This command guides you on how to write your commit message. Here the format should be &quot;[pattern]: [MESSAGE]&quot;. For us, this leads to: . cz check -m &quot;test: all summarize_data tests now succeed&quot; Commit validation: successful! . Very good, our commit message is valid. But hold on. Checking our messages each time with commitizen might be cumbersome and doesn’t provide the garanty to be applied. It would be better to check automatically the message each time we use the git commit command. That is where the pre-commit hook takes action. . Automatically enforce Git message conventions with pre-commit . Git hooks are useful to automate and perform some actions at specific place during the Git lifecycle. The pre-commit hook permits to run scripts before a Git commit is issued. We can use the hook to validate the commit messages and prevent Git from using a message which doesn’t match our expectations. The hook is active from the command line as well as from any tools interacting with the Git repository where the hook is registered, including your favoride IDE. . pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. If you want to know more about the inner workings and the spectrum of possibilities opened by the pre-commit hook, you can read its usage documentation. . To install pre-commit just run: . peotry add -D pre-commit . To automate the Git commit verification we first need to create a configuration file .pre-commit-config.yaml as followed: . repos: - repo: https://github.com/commitizen-tools/commitizen rev: master hooks: - id: commitizen stages: [commit-msg] . Next we can install the hook with its source defined in the repo property: . pre-commit install --hook-type commit-msg . Now that everything is set, we can use our Git hook: . git commit -m &quot;test: all summarize_data tests now succeed&quot; [INFO] Initializing environment for https://github.com/commitizen-tools/commitizen. [INFO] Installing environment for https://github.com/commitizen-tools/commitizen. [INFO] Once installed this environment will be reused. [INFO] This may take a few minutes... commitizen check.........................................................Passed [INFO] Restored changes from /home/fbraza/.cache/pre-commit/patch1617970841. [master 1e64d0a] test: all summarize_data tests now succeed 2 files changed, 48 insertions(+), 5 deletions(-) rewrite tests/test_summarize_dataframe.py (98%) . pre-commit installs an environment to run its checks. As you can see here the commit message assessment passed. To finish we can commit and push the modifications made on the build files (poetry.lock, pyproject.toml) and our module: . git commit -m &quot;build: add developer dependencies&quot; -m &quot;commitizen and pre-commit added to our dev dependencies&quot; commitizen check.........................................................Passed [master 1c6457c] build: add developer dependencies 2 files changed, 585 insertions(+), 1 deletion(-) git commit -m &quot;feat: implementation of the summary function to summarize dataframe&quot; commitizen check.........................................................Passed [master 5c053ad] build: add developer dependencies 1 file changed, 94 insertions(+) . We can now push everything to our GitHub repository: . git push origin master . Conclusion . We covered a few topics: . On the first hand, we saw how to write unit tests for your code. You shall always start to write tests before coding. It helps you affinate your API and expectations before implementing them. You will definitively benefit from it. We used unittest which is already available in the Python standard library. I actually like its simple design and object-oriented approach but others prefer using the pytest library which is definitively worth checking. One very convenient aspect is that pytest supports the unittest.TestCase class from the beginning. You can then write your tests with either of the two libraries or even mix both depending on your needs and have one common command to run them all. | We saw how to enforce good practices when writing Git commit messages. Our proposed solution relies on the use of two distinct Python packages: commitizen and pre-commit. The first one provides with the tools to check if a message validate the conventions you have chosen. The second one automates the process using a Git hook. | . In our next and last article, we are going to go one step further. We automate testing using tox and integrate it inside a CI/CD pipeline. Once done we will show how to prepare our package and finally publish it on PyPi using poetry. . Cheat sheet . poetry . Add project dependencies: . poetry add [package_name] . | Add developer dependencies: . poetry add -D [package_name] . poetry add --dev [package_name] . | Run test: . poetry run pytest . | . commitizen . Initialize commitizen: . cz init . | Check your commit: . cz check -m &quot;YOUR MESSAGE&quot; . | . pre-commit . Generate a default configuration file: . pre-commit sample-config . | Install git hook: . pre-commit install --hook-type [hook_name] . | . Acknowledgments . This article was first published in Adaltas blog and kindly reviewed by the CEO David Worms and one consultant Barthelemy NGOM. .",
            "url": "https://fbraza.github.io/BrazLog/python/devops/2021/06/24/modern-python-part2.html",
            "relUrl": "/python/devops/2021/06/24/modern-python-part2.html",
            "date": " • Jun 24, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Modern Python part 1 - start a project with pyenv & poetry",
            "content": "When learning a programming language, the focus is essentially on understanding the syntax, the code style, and the underlying concepts. With time, you become sufficiently comfortable with the language and you start writing programs solving new exciting problems. . However, when you need to move towards this step, there is an aspect that one might have underestimated which is how to build the right environment. An environment that enforces good software engineering practices, improves productivity and facilitates collaboration. Packaging and tooling with Python is often described as cumbersome and challenging. In this regard, several open-source projects emerged in the last years and aim at facilitating the management of Python packages along your working projects. We are going to see here how to use two of them: Pyenv, to manage and install different Python versions, and Poetry, to manage your packages and virtual environments. Combined or used individually, they help you to establish a productive environment. . This article is the first one from a series of three in which I share some best practices. . Part 1: project initialization with pyenv and poetry | Part 2: unit testing and commit enforcement | Part 3: CI pipeline with GitHub Actions and publication on PiPy | . Pre-requisites . pyenv installation . To install pyenv you require some OS-specific dependencies. These are needed as pyenv installs Python by building from source. For Ubuntu/Debian be sure to have the following packages installed: . sudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl . To know the required dependencies on your OS go read this documentation. Once the dependencies are installed you can now install pyenv. For this, I recommend using pyenv-installer that automates the process. . curl https://pyenv.run | bash . From there on, you can install on your system any versions of Python you wish. You can use the following command to all versions and flavors of Python available: . pyenv install --list . In our case we are going to install the classical CPython in versions 3.7.10 , 3.8.7 , 3.9.2: . pyenv install 3.7.10 Downloading Python-3.7.10.tar.xz... -&gt; https://www.python.org/ftp/python/3.7.10/Python-3.7.10.tar.xz Installing Python-3.7.10... Installed Python-3.7.10 to /home/fbraza/.pyenv/versions/3.7.10 . Once the versions are installed you can see them by running: . pyenv versions * system 3.7.10 3.8.7 3.9.2 . You can see that pyenv identified recently installed Python versions and also the one installed by default on your system. The * before system means that the global version used now is the system version. pyenv permits to manage Python versions at different levels: globally and locally. Let’s say we are going to set version 3.7.10 as our global version. . pyenv global 3.7.10 . Let’s list our version again: . pyenv versions system * 3.7.10 (set by /home/&lt;username&gt;/.pyenv/version) 3.8.7 3.9.2 . You can see that pyenv sets 3.7.10 as our global Python version. This will not alter the operations that require the use of the system version. The path you can read between parenthesis corresponds to the path that points to the required Python version. How does this work? Briefly, pyenv captures Python commands using executables injected into your PATH. Then it determines which Python version you need to use, and passes the commands to the correct Python installation. Feel free to read the complete documentation to better understand the functionalities and possibilities offered by pyenv. . Note: Don’t be confused by the semantic here. Change the global version will not affect your system version. The system version corresponds to the version used by your OS to accomplish specific tasks or run background processes that depend on this specific Python version. Do not switch the system version to another one or you may face several issues with your OS! This version is usually updated along with your OS. The global version is just the version that pyenv will use to execute your Python commands / programs globally. . poetry installation . Poetry allows you to efficiently manage dependencies and packages in Python. It has a similar role as setup.py or pipenv, but offers more flexibility and functionalities. You can declare the libraries your project depends on in a pyproject.toml file. poetry will then install or update them on demand. Additionally this tools allows you to encapsulate your working project into isolated environments. Finally, you can use poetry to directly publish your package on Pypi. . As a last pre-requisite we are going to install poetry by running the following command: . curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - . Project creation . We are going to see how to create a project and isolate it inside a Python environment using pyenv and poetry. . Setting the Python version with Pyenv . Let’s first create a directory named my_awesome_project and move inside: . mkdir my_awesome_project &amp;&amp; cd $_ . Once inside, set the local Python version we are going to use (we are going to use Python 3.8.7). This will prompt poetry to use the local version of Python defined by pyenv: . pyenv local 3.8.7 . This creates a .python-version file inside our project. This file will be read by pyenv and prompts it to set the defined local Python version. Consequently every directory or file created down this point will depend on the local Python version and not the global one. . Create your project with poetry . Poetry proposes a robust CLI allowing you to create, configure and update your Python project and dependencies. To create your Python project use the following command: . poetry new &lt;project_name&gt; . This command generates a default project scaffold. The content of our new project is the following: . . ├── &lt;project_name&gt; │   └── __init__.py ├── pyproject.toml ├── README.rst └── tests ├── __init__.py └── test_summarize_dataframe.py . Notice the pyproject.toml. This is where we define everything from our project’s metadata, dependencies, scripts, and more. If you’re familiar with Node.js, consider the pyproject.toml as an equivalent of the Node.js package.json. . [tool.poetry] name = &quot;your_project_name&quot; version = &quot;0.1.0&quot; description = &quot;&quot; authors = [&quot;&lt;username&gt; &lt;email address&gt;&quot;] [tool.poetry.dependencies] python = &quot;^3.8&quot; [tool.poetry.dev-dependencies] pytest = &quot;^5.2&quot; [build-system] requires = [&quot;poetry-core&gt;=1.0.0&quot;] build-backend = &quot;poetry.core.masonry.api&quot; . We can see several entries in our defaultpyproject.toml file. . [tool.poetry]: This section contains metadata about our package. You can put there the package name, a short description, author’s details, the version of your project, and so on. All details here are optional but will be required if you decided to publish the package on Pypi. | [tool.poetry.dependencies]: This section contains all required dependencies for our package. You can specify specific version numbers for these packages (packageX = &quot;1.0.0&quot;) or use symbols. The version of Python we want the project to use is defined here as well. In our case python = &quot;^3.8&quot; specifies the minimum version required to run our app. Here this is Python 3.8 and this has been based on the version of our local version defined with pyenv. | [tool.poetry.dev-dependencies]: This section contains all developer dependencies which are packages needed to work and iterate on this project. Nevertheless, these dependencies are not required to run the app and will not be downloaded when building the package. | [build-system]: Do not touch this section unless you updated the version of poetry. | . Note: you can see the full list of available entries for the pyproject.toml file here . Install and activate the virtual environment . Here you have two approaches: whether you know in advance all dependencies you need and you can directly alter the .toml file accordingly or you decide to add later on when needed. In our example, we are going to add progressively our dependencies while writing code. Consequently, we just need to initialize the project and create the virtual environment. To do this run the command: . poetry install Creating virtualenv summarize-dataframe-SO-g_7pj-py3.8 in ~/.cache/pypoetry/virtualenvs Updating dependencies Resolving dependencies... (6.4s) Writing lock file Package operations: 8 installs, 0 updates, 0 removals • Installing pyparsing (2.4.7) • Installing attrs (20.3.0) • Installing more-itertools (8.7.0) • Installing packaging (20.9) • Installing pluggy (0.13.1) • Installing py (1.10.0) • Installing wcwidth (0.2.5) • Installing pytest (5.4.3) Installing the current project: summarize_dataframe (0.1.0) . Firstly the virtual environment is created and stored outside of the project. A bit similar to what we have when using conda. Indeed, Instead of creating a folder containing your dependency libraries (as virtualenv does), poetry creates an environment on a global system path (.cache/ by default). This separation of concerns allows keeping your project away from dependency source code. . Note: You can create your virtual environment inside your project or in any other directories. For that you need to edit the configuration of poetry. Follow this documentation for more details. . Secondly, poetry is going to read the pyproject.toml and install all dependencies specified in this file. If not defined, poetry will download the last version of the packages. At the end of the operation, a poetry.lock file is created. It contains all packages and their exact versions. Keep in mind that if a poetry.lock file is already present, the version numbers defined in it take precedence over what is defined in the pyproject.toml. Finally, you should commit the poetry.lock file to your project repository so that all collaborators working on the project use the same versions of dependencies. . Now let’s activate the environment we just created with the following command: . peotry shell Spawning shell within ~/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8 . ~/.cache/pypoetry/virtualenvs/summarize-dataframe-SO-g_7pj-py3.8/bin/activate . The command creates a child process that inherits from the parent Shell but will not alter its environment. It encapsulates and restrict any modifications you will perform to your project environment. . Create our git repository . For our last step here we are going to create a git repository, add README.md and .gitignore files and push everything to our remote repository. . git init git remote add origin https://github.com/fbraza/summarize_dataframe.git echo &quot;.* n!.gitignore&quot; &gt; .gitignore echo &quot;# Summarize dataframe&quot; &gt; README.md git add . git commit -m &quot;build: first commit. Environment built&quot; git push -u origin master . Conclusion . Herein we have seen how to install and manage different versions of Python on our machine using pyenv. We demonstrated how to leverage pyenv local to set a specific Python version in your project and then create a virtual environment using poetry. The use of poetry really smoothens the process of creation by proposing a simple and widely project scaffold. In addition, it includes the minimum build system requirements as defined by PEP 518. . In our next article, we are going to dive more into our project. We will write some code with their respective unit tests and see how we can use poetry to add the expected dependencies and run the tests. Finally, we are going to go a bit further and install all necessary dependencies with poetry to help us enforcing good practices with our git commits when using a Python project. . Cheat sheet . pyenv . Get all available and installable versions of Python . pyenv install --list . | Set the global Python version . pyenv global &lt;version_id&gt; . | Set a local Python version . pyenv local &lt;version_id&gt; . | . poetry . Create a project . poetry new &lt;project_name&gt; . | Install core dependencies and create environment . poetry install . | Activate environment . poetry shell . | . Acknowledgments . This article was first published in Adaltas blog and kindly reviewed by the CEO David Worms and one consultant Barthelemy NGOM. .",
            "url": "https://fbraza.github.io/BrazLog/python/devops/2021/06/09/modern-python-part1.html",
            "relUrl": "/python/devops/2021/06/09/modern-python-part1.html",
            "date": " • Jun 9, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Faouzi is French. He lived in France, Germany, Portugal and live now in Belgium where he works full-time as a Bioinformatic &amp; Data Engineer at Biolizard . He previously worked during 8 years as a scientist in Immunology and got hooked by the Bioinformaticians’ black magic! He decided to dive more into the IT and start learning how to program for data 2 years and a half ago. In the process, he got graduated in Data engineering from the Data ScienceTech Institute. . He worked (i) as an intern in Altran for a mission of 8 weeks focused on the development of a computer vision system for the Portuguese national electricity company; (ii) at Adaltas where he improved his skills in infrastructure with notably the deployment of Hadoop clusters; and (iii) at dataroots where he deployed and maintained data pipelines in Spark for a banking institution. . He likes working in the Python ecosystem. He has a focus on Data Engineering but works his way through Data Analysis and Machine Learning. . Contact . Gmail: faouzi.brazza@gmail.com | Github: fbraza | LinkedIn: Faouzi Braza | .",
          "url": "https://fbraza.github.io/BrazLog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fbraza.github.io/BrazLog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}